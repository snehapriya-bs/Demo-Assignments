{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tdtrlAhvIHY"
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "At the end of the experiment, you will be able to:\n",
    "\n",
    "* understand the big picture of transformers\n",
    "* explore masking of transformers\n",
    "* implement transformer decoder and understand its architecture\n",
    "* apply learning on a machine translation problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tv8TQrtfwaNd"
   },
   "source": [
    "### The Big Picture of Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ni6YCBwE7d_D"
   },
   "source": [
    "<center>\n",
    "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST%205%20Big%20Picture.png\" width=700px/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xCXKCLBobfj"
   },
   "source": [
    "Transformer architecture follows an encoder-decoder structure:\n",
    "\n",
    "- the ***encoder***, on the left-hand side, is tasked with mapping an input sequence to a sequence of continuous representations;\n",
    "- the ***decoder***, on the right-hand side, receives the output of the encoder together with the decoder output at the previous time step to generate an output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqM7uCmtEt5c"
   },
   "source": [
    "The Transformer decoder generates sequences autoregressively by attending to previously generated positions using masked self-attention, attending to the encoder's output using encoder-decoder attention, applying feed-forward networks, and utilizing positional encodings. This architecture allows the decoder to produce coherent and contextually accurate sequences in various natural language processing tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKQ0Fvl_jNqU"
   },
   "source": [
    "### Setup Steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "WBPPuGmBlDIN"
   },
   "outputs": [],
   "source": [
    "#@title Run this cell to complete the setup for this Notebook\n",
    "from IPython import get_ipython\n",
    "ipython.magic(\"sx unzip -q Demo_spa-eng.zip\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RH8Ecq9sbYU"
   },
   "source": [
    "### Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ms8SJA8jELck"
   },
   "outputs": [],
   "source": [
    "# Importing the NumPy library, which provides support for mathematical operations on large arrays and matrices.\n",
    "import numpy as np\n",
    "\n",
    "# Importing the re module for working with regular expressions, useful for pattern matching in strings.\n",
    "import re\n",
    "\n",
    "# Importing the random module, which provides functionalities for generating random numbers and choices.\n",
    "import random\n",
    "\n",
    "# Importing the string module to access string constants (e.g., ascii_letters, digits) and utilities for string manipulation.\n",
    "import string\n",
    "\n",
    "# Importing TensorFlow, a popular library for machine learning and deep learning.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Importing Keras, the high-level neural network API included in TensorFlow, to build and train models.\n",
    "from tensorflow import keras\n",
    "\n",
    "# Importing specific modules from Keras for building deep learning models:\n",
    "# - `layers` provides various building blocks like Dense, Conv2D, LSTM, etc., for constructing neural network layers.\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDk0xc-_gbV3"
   },
   "source": [
    "# **Part A** : Building Encoder Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuUJwJFNE8jc"
   },
   "source": [
    "The concepts for Transformer encoder have been discussed in Assignment 4 and the same steps are implemented here for creating a decoder network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtZo2HU5IpgJ"
   },
   "source": [
    "### Define TransformerEncoder class to be used in model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h31IDF-uFt3M"
   },
   "outputs": [],
   "source": [
    "# Defining a custom Transformer Encoder layer by subclassing the `layers.Layer` class in Keras.\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        # Initialize the parent Layer class with any additional keyword arguments.\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Store the embedding dimension, which defines the size of input embeddings (e.g., 4 in a dummy example).\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Define the size of the dense (fully connected) layer in the feedforward network within the encoder.\n",
    "        self.dense_dim = dense_dim\n",
    "\n",
    "        # Define the number of attention heads in the Multi-Head Attention mechanism.\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Create a Multi-Head Attention layer for self-attention, with specified number of heads and embedding dimension.\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "\n",
    "        # Build a feedforward neural network (FFN) with two dense layers.\n",
    "        # The first layer uses ReLU activation, and the second projects back to the embedding dimension.\n",
    "        self.dense_proj = keras.Sequential([\n",
    "            layers.Dense(dense_dim, activation=\"relu\"),  # Expands to `dense_dim`.\n",
    "            layers.Dense(embed_dim)                      # Projects back to `embed_dim` to match input shape.\n",
    "        ])\n",
    "\n",
    "        # Layer normalization for stabilizing training and improving convergence.\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    # Define the forward pass logic for the Transformer Encoder layer.\n",
    "    def call(self, inputs, mask=None):\n",
    "        # Apply masking if a mask is provided, adding a new axis to the mask tensor.\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "            print(f\"**test: mask in not None. mask = {mask}\")\n",
    "\n",
    "        # Perform self-attention with inputs as query, key, and value.\n",
    "        # This makes it a \"self-attention\" mechanism as all arguments come from the same source (inputs).\n",
    "        attention_output = self.attention(\n",
    "            query=inputs,             # The query tensor.\n",
    "            value=inputs,             # The value tensor.\n",
    "            key=inputs,               # The key tensor.\n",
    "            attention_mask=mask       # Optional attention mask.\n",
    "        )\n",
    "\n",
    "        # Apply residual connection and normalization after the attention step.\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "\n",
    "        # Pass the normalized result through the feedforward network.\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "\n",
    "        # Apply another residual connection and normalization after the feedforward network.\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    # Define a method to return the configuration of the layer (for serialization purposes).\n",
    "    def get_config(self):\n",
    "        # Retrieve the configuration from the parent class and update it with custom attributes.\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,  # Embedding dimension.\n",
    "            \"num_heads\": self.num_heads,  # Number of attention heads.\n",
    "            \"dense_dim\": self.dense_dim,  # Size of the dense layer in the FFN.\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcVnGSnpSFGB"
   },
   "source": [
    "### Positional Embedding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*   Learn position- embedding vectors the same way we learn to embed word indices.\n",
    "*   Proceed to **add** our position embeddings to the corresponding word embeddings, to obtain a position-aware word embedding.\n",
    "*   This technique is called “positional embedding.”\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0n04kGhBbQCe"
   },
   "source": [
    "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST6%20Positional%20Embedding.png\" width=700px/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3VrrCylc6q4"
   },
   "source": [
    "<center>\n",
    "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST6%20Encoder%20Embedding.png\" width=650px/>\n",
    "</center>\n",
    "\n",
    "![]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMOrn4pQeefD"
   },
   "source": [
    "**Q:** In the picture above:\n",
    "\n",
    "\n",
    "*   What is the embedding dimension for both the layers? - 3\n",
    "*   How many rows would the token embedding layer have?  - 20000 (vocab size)\n",
    "*   How many rows would the postional embedding layer have? - 600 (seq length)\n",
    "*   Where do we get the indices in token embedding layer? - from TextVectorization\n",
    "*   Where do we get the indices in token embedding layer? - We explicitly define a range\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSibMhQ0I3T_"
   },
   "source": [
    "### Define PositionalEmbedding class to be used in model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QjxKe_ZTGxfx"
   },
   "outputs": [],
   "source": [
    "# Using positional encoding to re-inject order information\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        # input_dim = (token) vocabulary size, output_dim = embedding size\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Embedding layer for token embeddings:\n",
    "        # Converts tokens into dense vector representations of size `output_dim`.\n",
    "        self.token_embeddings = layers.Embedding(input_dim=input_dim, output_dim=output_dim)\n",
    "        # Q: what is input_dim and output_dim? A: vocab size, embedding dim\n",
    "\n",
    "        # Embedding layer for positional embeddings:\n",
    "        # Assigns a unique embedding to each position in the sequence (0 to sequence_length-1).\n",
    "        self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=output_dim)\n",
    "        # Q: Why input_dim = seq_length?  A: there are seq_len; no. of possible positions\n",
    "        # Q: What is the vocab for this Embedding layer? A: seq_length\n",
    "\n",
    "        # Store the sequence length, input dimension, and output dimension.\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):  # Inputs will be a batch of sequences (batch_size, seq_len)\n",
    "        # Extract the sequence length dynamically from the input tensor.\n",
    "        length = tf.shape(inputs)[-1]  # `length` will just be sequence length.\n",
    "\n",
    "        # Generate position indices (0 to sequence_length-1) for the input sequence.\n",
    "        positions = tf.range(start=0, limit=length, delta=1)  # Indices for input to positional embedding.\n",
    "\n",
    "        # Convert token IDs in `inputs` to dense embeddings using `token_embeddings`.\n",
    "        embedded_tokens = tf.reshape(self.token_embeddings(inputs), (-1, length, self.output_dim))\n",
    "\n",
    "        # Convert position indices to dense embeddings using `position_embeddings`.\n",
    "        embedded_positions = tf.reshape(self.position_embeddings(positions), (-1, length, self.output_dim))\n",
    "\n",
    "        # Add token embeddings and positional embeddings element-wise.\n",
    "        return layers.Add()([embedded_tokens, embedded_positions])  # ADD the embeddings.\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):  # Makes this layer a mask-generating layer.\n",
    "        if mask is None:\n",
    "            return None  # If no mask is provided, return None.\n",
    "        # Generate a boolean mask where tokens equal to 0 (padding tokens) are marked as False.\n",
    "        return tf.math.not_equal(inputs, 0)  # Mask will get propagated to the next layer.\n",
    "\n",
    "    # When using custom layers, this enables the layer to be reinstantiated from its config dict,\n",
    "    # which is useful during model saving and loading.\n",
    "    def get_config(self):\n",
    "        # Get the configuration of the parent class and update it with custom attributes.\n",
    "        config = super(PositionalEmbedding, self).get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,        # Embedding dimension size.\n",
    "            \"sequence_length\": self.sequence_length,  # Maximum sequence length.\n",
    "            \"input_dim\": self.input_dim,         # Vocabulary size.\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwDciZc2g3za"
   },
   "outputs": [],
   "source": [
    "a = tf.constant([1, 0, 2, 0, 3])  # Define a TensorFlow constant tensor with the specified values.\n",
    "print(a)  # Print the tensor `a`. Output will display the tensor's values.\n",
    "\n",
    "print(tf.math.not_equal(a, 0))\n",
    "# Use TensorFlow's `tf.math.not_equal` function to create a boolean tensor.\n",
    "# This tensor will have `True` where the elements of `a` are not equal to `0`, and `False` where they are `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVD5IzTuJD29"
   },
   "source": [
    "### TransformerEncoder model definition with Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7uaobuC7WZ0w"
   },
   "outputs": [],
   "source": [
    "# Combining the Transformer encoder with positional embedding\n",
    "# The values below are for the classification problem. We will change them for the translation example.\n",
    "\n",
    "# Define key parameters for the model.\n",
    "vocab_size = 15000           # The size of the vocabulary (number of unique tokens).\n",
    "sequence_length = 20         # The maximum length of input sequences.\n",
    "embed_dim = 256              # The embedding size (dimensionality of token embeddings).\n",
    "num_heads = 2                # Number of attention heads in the Transformer encoder.\n",
    "dense_dim = 32               # Number of neurons in the feedforward network of the encoder.\n",
    "\n",
    "# Define the model input.\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "# Input is expected to be integer-encoded (e.g., token indices from a TextVectorization layer).\n",
    "\n",
    "# Add a PositionalEmbedding layer to combine token and positional embeddings.\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "\n",
    "# Add a TransformerEncoder layer for self-attention and feedforward transformations.\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "# Apply global max pooling to reduce the sequence dimension, retaining only the most significant features.\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# Add a dropout layer to prevent overfitting during training.\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Add a dense output layer with a sigmoid activation function for binary classification.\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Create the Keras model by specifying the input and output layers.\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model with the following configurations:\n",
    "# - Optimizer: \"rmsprop\" (RMSProp optimization algorithm).\n",
    "# - Loss: \"binary_crossentropy\" (loss function for binary classification tasks).\n",
    "# - Metrics: Track \"accuracy\" during training and evaluation.\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Print a summary of the model, including the number of trainable parameters and layer details.\n",
    "model.summary()\n",
    "\n",
    "# Compute and print the number of trainable weights for embeddings.\n",
    "print(f\"Token embedding weights: {256*15000}\")  # Number of weights in the token embedding layer.\n",
    "print(f\"Position embedding weights: {256*20}\")  # Number of weights in the positional embedding layer.\n",
    "print(f\"Total no. of weights: {256*15000 + 256*20}\")  # Sum of token and positional embedding weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orBRzfHAebIJ"
   },
   "source": [
    "# **Part B** : Building Decoder Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2Z0_zLTsYPz"
   },
   "source": [
    "## Encoder - Decoder Overview\n",
    "\n",
    "Encoder - Encodes the input as some representation\n",
    "\n",
    "Decoder - Uses the encoded representation (and targets) to decode these representation as per the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzlzttwC2-I1"
   },
   "source": [
    "Transformer Architecture:\n",
    "\n",
    "<center>\n",
    "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST6%20Transformer%20Network.png\" width=350px/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3LdyPKhsYWs"
   },
   "source": [
    "During training,\n",
    "* An encoder model turns the source sequence into an intermediate representation.\n",
    "* **A decoder is trained to predict the next token i** in the target sequence by looking at both\n",
    "    - previous tokens (0 to i - 1) and\n",
    "    - the encoded source sequence\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wq5yUiVhudHe"
   },
   "source": [
    "During inference, we don’t have access to the target sequence—we’re trying to predict it from scratch. We’ll have to generate it one token at a time:\n",
    "1. We obtain the encoded source sequence from the encoder.\n",
    "2. The decoder starts by looking at the encoded source sequence as well as an initial “seed” token (such as the string \"[start]\"), and uses them to predict the\n",
    "first real token in the sequence.\n",
    "3. The predicted sequence so far is fed back into the decoder, which generates the next token, and so on, until it generates a stop token (such as the string \"[end]\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Do0zZWCuiJF"
   },
   "source": [
    "<center>\n",
    "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST6%20Transformer%20gif.gif\" width=750px/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQO73b-r4y0a"
   },
   "source": [
    "### Masking\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Dkb0qlNSsAM"
   },
   "source": [
    "Masking is needed to prevent the attention mechanism of a transformer from “cheating” in the decoder when training (on a translating task for instance). This kind of “ cheating-proof masking” is not present in the encoder side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oR2VCAGWTWHZ"
   },
   "source": [
    "Consider the sequence: “I love it”, then the expected prediction for the token at position one (“I”) is the token at the next position (“love”). Similarly the expected prediction for the tokens “I love” is “it”.\n",
    "\n",
    "We do not want the attention mechanism to share any information regarding the token at the next positions, when giving a prediction using all the previous tokens.\n",
    "\n",
    "To ensure that this is done, we mask future positions (setting them to -inf) before the softmax step in the self-attention calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLhi72ah-9na"
   },
   "source": [
    "### Padding mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zg2lNXdplnb1"
   },
   "source": [
    "Padding is a special form of masking where the masked steps are at the start or the end of a sequence. Padding comes from the need to encode sequence data into contiguous batches: in order to make all sequences in a batch fit a given standard length, it is necessary to pad or truncate some sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fsvrm8dRX4M"
   },
   "source": [
    "* The Embedding layer is capable of generating a “mask” that corresponds to its input data.\n",
    "\n",
    "* By default, this option isn’t active—you can turn it on by passing mask_zero=True to your Embedding layer.\n",
    "\n",
    "* You can retrieve the mask with the compute_mask() method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvl9mkDal6zP"
   },
   "source": [
    "### An example to understand Padding Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Md-LHZtQ7Zp"
   },
   "outputs": [],
   "source": [
    "# Padding mask\n",
    "# Define an Embedding layer with:\n",
    "# - `input_dim=10`: The vocabulary size (number of unique tokens).\n",
    "# - `output_dim=256`: The size of the embedding vectors.\n",
    "# - `mask_zero=True`: Enables masking for padding tokens (value 0).\n",
    "\n",
    "embedding_layer_ = layers.Embedding(input_dim=10, output_dim=256, mask_zero=True)\n",
    "\n",
    "# Define some input sequences with padding tokens (value 0 at the end).\n",
    "some_input = [\n",
    "  [4, 3, 2, 1, 0, 0, 0],  # Sequence with padding (0) at the end.\n",
    "  [5, 4, 3, 2, 1, 0, 0],  # Sequence with padding (0) at the end.\n",
    "  [2, 1, 0, 0, 0, 0, 0]   # Sequence with more padding (0) at the end.\n",
    "]\n",
    "\n",
    "# Compute the mask for the input sequences using the embedding layer.\n",
    "d_mask = embedding_layer_.compute_mask(some_input)\n",
    "\n",
    "# Print the mask. It is a boolean tensor where `True` represents valid tokens,\n",
    "# and `False` represents padding tokens (value 0).\n",
    "print(d_mask)\n",
    "\n",
    "# Convert the boolean mask to integers for clearer representation:\n",
    "# `True` becomes 1 and `False` becomes 0.\n",
    "print(tf.cast(d_mask, dtype=\"int32\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iuyzd5P7Q5Q4"
   },
   "source": [
    "### Causal Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TNaWfPasli3"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "*   The TransformerDecoder is order-agnostic: it looks at the entire target sequence at once.\n",
    "*   If it were allowed to use its entire input, it would simply learn to copy input step N+1 to location N in the output.\n",
    "*  Solution: mask the upper half of the pairwise attention matrix to prevent the model from paying any attention to information from the future\n",
    "*  We'll see this in the method get_causal_attention_mask(self, inputs) inside the decoder class\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-U2aT2c58-9"
   },
   "source": [
    "<center>\n",
    "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST5%20Self%20Attention%20Scores.png\" width=600px/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFAiWsE3kK7S"
   },
   "source": [
    "<center>\n",
    "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST5%20Multihead%20Attention.png\" width=600px/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DIIseZQWPsF"
   },
   "outputs": [],
   "source": [
    "# Assume sequence length is 5\n",
    "j = normal_range = tf.range(5)\n",
    "# `tf.range(5)` generates a 1D tensor with values [0, 1, 2, 3, 4].\n",
    "# `normal_range` and `j` are just aliases for the same tensor.\n",
    "\n",
    "i = with_new_axis = tf.range(5)[:, tf.newaxis]\n",
    "# `tf.range(5)` generates [0, 1, 2, 3, 4].\n",
    "# `[:, tf.newaxis]` adds a new dimension to make the tensor 2D:\n",
    "# The resulting shape is (5, 1), producing a column vector:\n",
    "# [[0],\n",
    "#  [1],\n",
    "#  [2],\n",
    "#  [3],\n",
    "#  [4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cv-vP3TrWWaP"
   },
   "outputs": [],
   "source": [
    "print(normal_range)\n",
    "print(with_new_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7nqm6dAHaF4G"
   },
   "outputs": [],
   "source": [
    "# `j` is a 1D tensor: [0, 1, 2, 3, 4]\n",
    "# `i` is a 2D tensor: [[0], [1], [2], [3], [4]]\n",
    "# Broadcasting will align the dimensions of `j` and `i` for comparison.\n",
    "# The comparison `i >= j` is performed element-wise.\n",
    "\n",
    "d_mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "# `i >= j` produces a boolean tensor, where:\n",
    "# - `True` indicates that the element in `i` is greater than or equal to the corresponding element in `j`.\n",
    "# - `False` indicates otherwise.\n",
    "# `tf.cast(..., dtype=\"int32\")` converts the boolean tensor into an integer tensor:\n",
    "# - `True` becomes `1`.\n",
    "# - `False` becomes `0`.\n",
    "\n",
    "# Print the resulting mask.\n",
    "print(d_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dK_XMJCfap91"
   },
   "outputs": [],
   "source": [
    "# Reshape the tensor `d_mask` to have a shape of (1, 5, 5).\n",
    "d_mask = tf.reshape(d_mask, (1, 5, 5))\n",
    "\n",
    "# Print the reshaped tensor.\n",
    "print(d_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSbwK1f3bIuU"
   },
   "outputs": [],
   "source": [
    "# Define tile multiplier for tiling\n",
    "\n",
    "# Define the batch size value (a scalar integer)\n",
    "batch_size = 2\n",
    "\n",
    "# Expand the dimension of batch_size to create a tensor with shape (1,)\n",
    "# and value [batch_size]. This makes batch_size a 1D tensor for compatibility in operations like concat.\n",
    "mult = tf.concat(\n",
    "    [tf.expand_dims(batch_size, -1),  # Expands batch_size to shape (1,)\n",
    "     tf.constant([1, 1], dtype=tf.int32)],  # Defines a tensor with values [1, 1] and shape (2,)\n",
    "    axis=0)  # Concatenates both tensors along the 0th axis, resulting in a tensor of shape (3,)\n",
    "\n",
    "# Print the result of expanding batch_size to shape (1,)\n",
    "print(tf.expand_dims(batch_size, -1))  # Output: tf.Tensor([2], shape=(1,), dtype=int32)\n",
    "\n",
    "# Print the tensor created with values [1, 1] and shape (2,)\n",
    "print(tf.constant([1, 1], dtype=tf.int32))  # Output: tf.Tensor([1 1], shape=(2,), dtype=int32)\n",
    "\n",
    "# Print the concatenated result of expanding batch_size and the constant tensor [1, 1]\n",
    "print(mult)  # Output: tf.Tensor([2 1 1], shape=(3,), dtype=int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_YgbB3zeu8x"
   },
   "outputs": [],
   "source": [
    "# Tile the mask to replicate across batchsize\n",
    "\n",
    "# Use the previously created `d_mask` and replicate it across the batch size.\n",
    "# `mult` is a tensor that indicates how many times to replicate the `d_mask` tensor across each axis.\n",
    "causal_mask_ = tf.tile(d_mask, mult)\n",
    "\n",
    "# Print the tiled mask result\n",
    "print(causal_mask_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTB56QC6gwPW"
   },
   "source": [
    "In the example above:\n",
    "\n",
    "sequence length = 5\n",
    "\n",
    "batch size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDJ5XdcUU6B4"
   },
   "source": [
    "To know more about masking, refer [here](https://www.tensorflow.org/guide/keras/masking_and_padding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jccpLKpUEwCg"
   },
   "source": [
    "### Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T92X85GUFCLn"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        # Define the layers. Let's point them out in the diagram\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim  # Dimension of embedding (e.g., 256)\n",
    "        self.dense_dim = dense_dim  # Number of neurons in dense layer (e.g., 32)\n",
    "        self.num_heads = num_heads  # Number of heads for MultiHead Attention layer\n",
    "\n",
    "        # Now we have 2 MultiHead Attention layers - one for self attention and one for generalized attention\n",
    "        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)  # Self-attention\n",
    "        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)  # Cross-attention\n",
    "        self.dense_proj = keras.Sequential([layers.Dense(dense_dim, activation=\"relu\"),  # Fully connected layers\n",
    "                                            layers.Dense(embed_dim),]\n",
    "                                           )\n",
    "        # Layer normalization for stabilizing training and improving performance\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "\n",
    "        self.supports_masking = True  # Ensures that the layer will propagate its input mask to its outputs\n",
    "\n",
    "    def get_config(self):\n",
    "        # Provides a configuration dictionary for the custom layer, for model serialization\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        # Generates a causal attention mask to prevent the decoder from attending to future tokens\n",
    "        input_shape = tf.shape(inputs)  # Get the shape of the input tensor\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]  # Extract batch size and sequence length\n",
    "\n",
    "        # Generate a causal mask by comparing indices i and j (where i >= j means attention is allowed)\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]  # Create a column vector of sequence indices\n",
    "        j = tf.range(sequence_length)  # Create a row vector of sequence indices\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")  # True (1) for valid positions, False (0) for invalid positions\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))  # Reshape to (1, seq_len, seq_len)\n",
    "\n",
    "        # Concatenate batch size to create a multiplier for tiling\n",
    "        mult = tf.concat([tf.expand_dims(batch_size, -1),  # Expand batch size to match shape\n",
    "                          tf.constant([1, 1], dtype=tf.int32)],  # Keep the other dimensions unchanged\n",
    "                         axis=0\n",
    "                         )\n",
    "\n",
    "        # Tile the mask according to the batch size\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        # `inputs`: decoder input sequence\n",
    "        # `encoder_outputs`: output of the encoder (key-value pairs for cross-attention)\n",
    "        # `mask`: optional mask for padding (e.g., for handling padded tokens)\n",
    "\n",
    "        # Generate a causal mask to prevent attending to future tokens\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "\n",
    "        # Padding mask: if provided, it prevents attending to padding tokens in the encoder output\n",
    "        padding_mask = None\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")  # Expand mask for attention compatibility\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)  # Union of padding and causal mask (0s prevent attention)\n",
    "\n",
    "        # First attention layer (self-attention)\n",
    "        attention_output_1 = self.attention_1(query=inputs,  # Query: decoder inputs\n",
    "                                              value=inputs,  # Value: decoder inputs\n",
    "                                              key=inputs,    # Key: decoder inputs\n",
    "                                              attention_mask=causal_mask  # Causal mask prevents future token attention\n",
    "                                              )\n",
    "\n",
    "        # Apply layer normalization after adding the attention output to the input (residual connection)\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        # Second attention layer (cross-attention) using encoder outputs as key and value\n",
    "        attention_output_2 = self.attention_2(query=attention_output_1,  # Query: output from first attention layer\n",
    "                                              value=encoder_outputs,  # Value: encoder outputs (key-value pairs)\n",
    "                                              key=encoder_outputs,    # Key: encoder outputs\n",
    "                                              attention_mask=padding_mask,  # Padding mask if provided\n",
    "                                              )\n",
    "\n",
    "        # Apply layer normalization after adding the attention output to the input (residual connection)\n",
    "        attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n",
    "\n",
    "        # Apply a dense projection after the second attention layer\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "\n",
    "        # Apply final layer normalization with residual connection\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Mix-Vvd9Byv"
   },
   "source": [
    "<center>\n",
    "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST6%20Transformer%20Network.png\" width=350px/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0C1QP9ubAXx4"
   },
   "outputs": [],
   "source": [
    "# English to Spanish translation using a Transformer model\n",
    "\n",
    "# Define model parameters\n",
    "vocab_size = 15000  # Vocabulary size for both English and Spanish\n",
    "sequence_length = 20  # Maximum sequence length (length of the input and output sentences)\n",
    "embed_dim = 256  # Dimension of token embeddings\n",
    "dense_dim = 2048  # Number of neurons in the dense layers after the attention layers\n",
    "num_heads = 8  # Number of heads in the multi-head attention mechanism\n",
    "\n",
    "# Define encoder input for English sentences\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")  # Input sequence of English words\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)  # Apply positional embedding\n",
    "# Q: First arg acts like a 'vocabulary' for pos embedding layer. A: The first argument corresponds to sequence length, which determines how many unique positions need to be embedded.\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)  # Pass through the Transformer encoder\n",
    "# Q: What are these arguments? A: embedding dimension, number of neurons in the dense layer, number of heads in the multi-head attention layer.\n",
    "\n",
    "# Define decoder input for Spanish sentences\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")  # Input sequence of Spanish words\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)  # Apply positional embedding\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)  # Pass through the Transformer decoder, taking encoder output as context\n",
    "# Q: What are the call arguments in the picture? A: The decoder receives the embedded inputs and the encoder's output to perform cross-attention.\n",
    "\n",
    "x = layers.Dropout(0.5)(x)  # Apply dropout to prevent overfitting\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)  # Output layer with softmax activation for predicting the next word in Spanish\n",
    "\n",
    "# Create the complete Transformer model with encoder and decoder inputs and decoder outputs\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)  # Note that there are two input layers: one for the encoder and one for the decoder\n",
    "\n",
    "# Display model summary\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsFTQP9S6s-a"
   },
   "source": [
    "# A Machine Translation Example\n",
    "\n",
    "English to Spanish translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0Y2NmDV7lF6"
   },
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxaC6lh95Ox2"
   },
   "outputs": [],
   "source": [
    "# Rows of the dataset\n",
    "!tail spa-eng/spa.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nxxtnbv9D2-u"
   },
   "outputs": [],
   "source": [
    "# Pre-processing: Separating input and output sequences\n",
    "text_file = \"spa-eng/spa.txt\"  # Define the file path of the dataset\n",
    "\n",
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]  # Open the file and read the contents, splitting by newlines and removing the last empty line\n",
    "\n",
    "text_pairs = []  # Initialize an empty list to store English-Spanish pairs\n",
    "\n",
    "for line in lines:  # Iterate through each line in the dataset\n",
    "    english, spanish = line.split(\"\\t\")  # Split each line into English and Spanish sentences based on the tab character\n",
    "    spanish = \"[start] \" + spanish + \" [end]\"  # Add start and end tokens to the Spanish sentence for the translation model\n",
    "    text_pairs.append((english, spanish))  # Append the pair (English, Spanish) to the list\n",
    "\n",
    "print(random.choice(text_pairs))  # Print a random English-Spanish pair from the list to check the format\n",
    "print(f\"no. of pairs: {len(text_pairs)}\")  # Print the total number of sentence pairs processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcybUiQTD3Az"
   },
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "\n",
    "random.shuffle(text_pairs)  # Shuffle the list of sentence pairs to ensure randomness before splitting\n",
    "\n",
    "# Calculate the number of validation samples (15% of total data)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "\n",
    "# Calculate the number of training samples (remaining after allocating for validation and test sets)\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_pairs = text_pairs[:num_train_samples]  # The first `num_train_samples` pairs for training\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]  # The next `num_val_samples` pairs for validation\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]  # The remaining pairs for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hj9HjcGE7h4N"
   },
   "outputs": [],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBc6EItWD3C3"
   },
   "outputs": [],
   "source": [
    "# Vectorizing the English and Spanish text pairs\n",
    "\n",
    "# Define which characters to strip out for Spanish data- [, ], ¿\n",
    "strip_chars = string.punctuation + \"¿\"  # Combine standard punctuation and the Spanish specific character (¿) to strip\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")  # Remove the \"[\" character from strip_chars\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")  # Remove the \"]\" character from strip_chars\n",
    "\n",
    "# Custom standardization function for Spanish\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)  # Convert input string to lowercase\n",
    "    return tf.strings.regex_replace(  # Replace elements of input matching regex pattern with an empty string\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")  # Remove characters defined in `strip_chars`\n",
    "\n",
    "vocab_size = 15000  # Define the vocabulary size\n",
    "sequence_length = 20  # Set the sequence length for padding/truncation\n",
    "\n",
    "# Create a TextVectorization layer for the source (English) text\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,  # Maximum number of tokens in the vocabulary\n",
    "    output_mode=\"int\",  # Convert text into integer sequences\n",
    "    output_sequence_length=sequence_length,  # Pad/truncate sequences to a fixed length\n",
    ")\n",
    "\n",
    "# Create a TextVectorization layer for the target (Spanish) text with custom standardization\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,  # Maximum number of tokens in the vocabulary\n",
    "    output_mode=\"int\",  # Convert text into integer sequences\n",
    "    output_sequence_length=sequence_length + 1,  # Add 1 for the [end] token at the end of each sequence\n",
    "    standardize=custom_standardization,  # Apply the custom standardization function\n",
    ")\n",
    "\n",
    "# Prepare the training data: separate English and Spanish text pairs\n",
    "train_english_texts = [pair[0] for pair in train_pairs]  # Extract English sentences\n",
    "train_spanish_texts = [pair[1] for pair in train_pairs]  # Extract Spanish sentences\n",
    "\n",
    "# Adapt the vectorization layers to the training data (build the vocabulary)\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_spanish_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7HndSHekBRwZ"
   },
   "outputs": [],
   "source": [
    "seq = tf.range(10)  # Create a tensor with values from 0 to 9 (10 elements)\n",
    "dec_in = seq[:-1]  # Slice the sequence to get all elements except the last one (input for decoder)\n",
    "dec_out = seq[1:]  # Slice the sequence to get all elements except the first one (output for decoder)\n",
    "\n",
    "# Print the original sequence, the decoder input, and the decoder output\n",
    "print(f\"original seq:  {seq}\")  # Prints the original sequence: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "print(f\"dec_in:   {dec_in}\")    # Prints the decoder input sequence: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "print(f\"dec_out:  {dec_out}\")   # Prints the decoder output sequence: [1, 2, 3, 4, 5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMicSLvGD3E_"
   },
   "outputs": [],
   "source": [
    "# Preparing datasets for the translation task\n",
    "\n",
    "batch_size = 64  # Set the batch size for training and validation datasets\n",
    "\n",
    "# IMPORTANT- returns nested tuple- ( (eng_encod_input, spa_ decod_input), spa_decod_output)\n",
    "def format_dataset(eng, spa):\n",
    "    # Q: What are eng and spa pre and post re-assignment? A: raw text (eng, spa) and indices after vectorization.\n",
    "    eng = source_vectorization(eng)  # Convert English sentences into integer sequences using source vectorizer\n",
    "    spa = target_vectorization(spa)  # Convert Spanish sentences into integer sequences using target vectorizer\n",
    "    return ({\n",
    "        \"english\": eng,            # Encoder input: English sequence\n",
    "        \"spanish\": spa[:, :-1],    # Decoder input: Spanish sequence without the last token (used for prediction)\n",
    "    }, spa[:, 1:])                 # Decoder output: Spanish sequence without the first token (target for prediction)\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    # Unzip the pairs (english, spanish) into separate lists\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)  # Convert English texts into a list\n",
    "    spa_texts = list(spa_texts)  # Convert Spanish texts into a list\n",
    "    # Create a tf.data.Dataset object from the text pairs\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)  # Batch the dataset to the specified batch size\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)  # Apply the formatting function in parallel\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()  # Shuffle data, prefetch for performance, and cache in memory\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_pairs)  # Create the training dataset\n",
    "val_ds = make_dataset(val_pairs)     # Create the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4A2wVFs8Epoy"
   },
   "outputs": [],
   "source": [
    "# Iterate over the dataset `train_ds` and take one batch\n",
    "for inputs, targets in train_ds.take(1):\n",
    "    # Print the shape of the 'english' input sequence in the batch (encoder input)\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
    "\n",
    "    # Print the shape of the 'spanish' input sequence in the batch (decoder input)\n",
    "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
    "\n",
    "    # Print the shape of the 'targets' (decoder output) in the batch\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54BdC0gg7pQS"
   },
   "source": [
    "#### Train and evaluate the model *(Switch to GPU runtime if needed)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-16ZBlBkm9Ez"
   },
   "outputs": [],
   "source": [
    "# Compiling the Transformer model with the following configurations:\n",
    "transformer.compile(optimizer=\"rmsprop\",                     # Use RMSprop optimizer for training\n",
    "                    loss=\"sparse_categorical_crossentropy\",  # Use sparse categorical crossentropy loss function for multi-class classification\n",
    "                    metrics=[\"accuracy\"]                   # Track accuracy as a metric during training and evaluation\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XumS29R6PkRr"
   },
   "outputs": [],
   "source": [
    "# Training the Transformer model with the provided datasets\n",
    "transformer.fit(train_ds,              # Train the model using the training dataset 'train_ds'\n",
    "                validation_data=val_ds, # Validate the model during training using the validation dataset 'val_ds'\n",
    "                epochs=20)              # Set the number of epochs to 20, indicating how many times the entire dataset will be passed through the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqvG6xs42X3X"
   },
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aucv-0CS1qcz"
   },
   "outputs": [],
   "source": [
    "# Save the trained Transformer model to a file for later use\n",
    "transformer.save(\"trained-transformer-model.keras\")  # Save the entire model (architecture, weights, optimizer, etc.) to a file named \"trained-transformer-model.keras\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trviWIKk1XkP"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdjEkvP2oC6U"
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "\n",
    "# Retrieve the vocabulary for the Spanish language from the target vectorizer\n",
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "\n",
    "# Create a dictionary that maps token indices to words in the Spanish vocabulary\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "\n",
    "# Define a maximum length for the decoded Spanish sentence\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "# Function to decode a sequence (translate English to Spanish)\n",
    "def decode_sequence(input_sentence):\n",
    "    # Tokenize the input sentence using the English vectorizer\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "\n",
    "    # Initialize the decoded sentence with the start token\n",
    "    decoded_sentence = \"[start]\"\n",
    "\n",
    "    # Loop to generate each word in the translated sentence, up to the max length\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        # Tokenize the partial decoded sentence (without the last token) using the Spanish vectorizer\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n",
    "\n",
    "        # Get the model's predictions for the next token in the sequence\n",
    "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "\n",
    "        # Select the token with the highest probability from the prediction\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "\n",
    "        # Look up the word corresponding to the sampled token index\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "\n",
    "        # Add the word to the decoded sentence\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        # If the end token is predicted, stop decoding\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "# Sample a few test sentences and generate translations\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(4):\n",
    "    input_sentence = random.choice(test_eng_texts)  # Select a random English sentence\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Input (eng):  {input_sentence}\")  # Print the original English sentence\n",
    "    print(f\"Output(spa): {decode_sequence(input_sentence)}\")  # Print the translated Spanish sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ije_bIxsnGc"
   },
   "source": [
    "Note that both the TransformerEncoder and the TransformerDecoder are shape-invariant, so you could be stacking many of them to create a more powerful encoder or decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHJf19zldfi1"
   },
   "source": [
    "<center>\n",
    "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST%206%20last%20image.png\" width=600px/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xce42Pfp-9vM"
   },
   "source": [
    "### Technical Ungraded Questions:\n",
    "\n",
    "1. Connection between encoder outputs and decoder inputs when there are multiple stacks of them?\n",
    "\n",
    "    **Answer:** The output from the last encoder block acts as input to all decoder blocks.\n",
    "\n",
    "\\\\\n",
    "\n",
    "2. During training, are the decoder inputs obtained from decoder predictions or are they obtained directly from the target data?\n",
    "\n",
    "    **Answer:** During training, the decoder input is obtained directly from the target data. The only differnce between the decoder input and decoder target is an offset of 1 index. For example, consider a hindi to english translation problem with a an english sample \"[start] I like to learn [end]\".  The input to the decoder for this sample will be sample[:-1], i.e. \"[start] I like to learn\" and the target will be sample[1:], i.e. \"I like to learn [end]\". The prediction during training will be a probabilitly distribution over the vocabulary for each element in the sequence. So if the sequence length is 8 and the vocabulary size is 100, then the output shape of the prediction for the given sample will be (6,100). The actual predicted sequence can be computed by taking the argmax, i.e. the token with the maximum probability, for each token in the sequence. An exemplary prediction based on our example can be \"I love to study\". The loss will be computed based on the sum of cross-entropy losses for each token. Here 'like'/'love' and 'learn'/'study' will contribute to the loss.\n",
    "\n",
    "  (Notes:\n",
    "  1. The sample will actually have integer data. Here its written text for the sake of clarity\n",
    "  2. The above explanation is for 1 sample. If the batch size of 64, i.e. 64 samples in a mini-batch , then the decoder output shape is (64,6,100). In general, it is (batch_size, seq_length, vocab_size).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oX_gVNGnjbI"
   },
   "source": [
    "Other important points:\n",
    "- An advatage of Transformers is that they allow parallizable computations. Note that the computation of given token does not depend on the computations of the previous token, and can be done in parallel during training.\n",
    "\n",
    "- Note what kind of data structure the the function \"format_dataset(eng, spa)\" returns. It is a nested tuple- ( (eng_encod_input, spa_decod_input), spa_decod_output), where '(eng_encod_input, spa_decod_input)' form the input of the Transformer Model and 'spa_decod_output' is the target output of the Transformer Model."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
