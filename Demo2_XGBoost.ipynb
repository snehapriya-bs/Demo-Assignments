{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Mxmt2UTIlPy"
   },
   "source": [
    "## Learning Objectives:\n",
    "\n",
    "At the end of the experiment, you will be able to:\n",
    "\n",
    "*  perform preprocessing for different types of features\n",
    "*  build pipeline for preprocessing of features\n",
    "*  implement feature selection manually and automatically\n",
    "*  build an XG-Boost regressor model and check its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auIFJoV_C0-M"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Predicting house prices is helpful to identify profitable investments or to determine whether the price advertised is over or under-estimated. Here, we will build an ML model to predict the sale price of homes based on different explanatory variables describing the aspects of residential houses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LK-C5VcAcPG"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset chosen is a [Housing dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) containing 79 features, one target feature (`SalePrice`), and 1460 samples. Visit the data source to understand each feature/column. Download the 'data_description.txt' file, which gives a full description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVF-O0dYjUZu"
   },
   "source": [
    "### Setup Steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-xx2h3lEA5G0"
   },
   "outputs": [],
   "source": [
    "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
    "Id = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhPEIYA4A5G2"
   },
   "outputs": [],
   "source": [
    "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
    "password = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1lCya9WKEmv"
   },
   "source": [
    "### Install XG-Boost and feature_engine library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wj8HDV2hVT-a"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip -q install xgboost\n",
    "!pip -q install feature_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "At96tBpLXC2y"
   },
   "source": [
    "Ignore the above warning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwDH0TpbKn70"
   },
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5dmv4mQRi5X"
   },
   "outputs": [],
   "source": [
    "# Mathematical functions (e.g., square root)\n",
    "from math import sqrt\n",
    "\n",
    "# Data manipulation and analysis library, often used for working with DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "# Numerical operations, especially arrays and matrix computations\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library for creating static, interactive, and animated visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Statistical data visualization library based on Matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "# Implementation of gradient boosting algorithms (used for regression and classification tasks)\n",
    "import xgboost as xgb\n",
    "\n",
    "# Utility for splitting datasets into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Pipeline for chaining multiple steps (like data preprocessing and modeling) together\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Handles missing data by filling with specified values or statistical measures\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Encodes categorical features with an ordinal encoding (labels are assigned as ordered integers)\n",
    "from sklearn.preprocessing import OrdinalEncoder as OrdinalEncoder_Sk\n",
    "\n",
    "# Selects features based on importance scores from a trained model\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Evaluation metrics for regression models (Mean Squared Error and RÂ² score)\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Imputes missing values with arbitrary numbers or based on specific rules (Feature-engine library)\n",
    "from feature_engine.imputation import ArbitraryNumberImputer, CategoricalImputer\n",
    "\n",
    "# Encodes rare labels (those with few occurrences) and performs ordinal encoding (Feature-engine library)\n",
    "from feature_engine.encoding import RareLabelEncoder, OrdinalEncoder\n",
    "\n",
    "# Drops specified features from the dataset (Feature-engine library)\n",
    "from feature_engine.selection import DropFeatures\n",
    "\n",
    "# to visualise all the columns and upto 100 rows in the dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "# for supressing warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32IuzBpNLfYc"
   },
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JaylWUE2SwIw"
   },
   "outputs": [],
   "source": [
    "# Read the 'housing_dataset.csv' file into a DataFrame using pandas.\n",
    "# This loads the dataset into a tabular format, making it easy to analyze and manipulate.\n",
    "data = pd.read_csv('Demo2_housing_dataset.csv')\n",
    "\n",
    "# Print the dimensions of the dataset (rows, columns).\n",
    "# Useful to check the size of the dataset and ensure it was loaded correctly.\n",
    "print(data.shape)\n",
    "\n",
    "# Display the first 5 rows of the dataset to get an overview of the data.\n",
    "# Helps to quickly inspect the structure and contents (e.g., column names and sample values).\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUXr9yL2CZwt"
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hn4l_WHP7Lss"
   },
   "source": [
    "Check the information of the dataframe regarding number of rows and columns, any null values, data types, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8FIqK5f54wN"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIZp5LNlSJN4"
   },
   "source": [
    "### Summarising the Data\n",
    "\n",
    "The following cell displays the types of features, the number of unique entries, and the percentage of Null entries in each feature column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKhAkQCeSIpB"
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame that summarizes the data types of each column in the dataset.\n",
    "summary = pd.DataFrame(data.dtypes, columns=['dtype'])\n",
    "\n",
    "# Reset the index to turn the column names into a regular column called 'Name'.\n",
    "summary = summary.reset_index()\n",
    "\n",
    "# Rename the 'index' column to 'Name' for better readability.\n",
    "summary = summary.rename(columns={'index': 'Name'})\n",
    "\n",
    "# Add a new column 'Null_Counts' that shows the total number of missing (null) values in each column.\n",
    "summary['Null_Counts'] = data.isnull().sum().values\n",
    "\n",
    "# Add a column 'Uniques' that counts the number of unique values for each column.\n",
    "summary['Uniques'] = data.nunique().values\n",
    "\n",
    "# Calculate the percentage of missing values for each column and store it in 'Null_Percent'.\n",
    "summary['Null_Percent'] = (summary['Null_Counts'] * 100) / len(data)\n",
    "\n",
    "# Sort the summary DataFrame by 'Null_Percent' in descending order to prioritize columns with the most missing data.\n",
    "summary.sort_values(by='Null_Percent', ascending=False, inplace=True)\n",
    "\n",
    "# Display the summary DataFrame to review the structure, null values, and uniqueness of each column.\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyc_ZOd_Ri5c"
   },
   "source": [
    "### Split dataset into train and test\n",
    "\n",
    "Separating the data into training and testing set before engineering. This is to avoid over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0ULUOl-Ri5g"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets using train_test_split.\n",
    "# The target variable 'SalePrice' is separated from the feature variables.\n",
    "\n",
    "# 'data.drop('SalePrice', axis=1)' removes the target column, keeping only feature columns for X (input).\n",
    "# 'data.SalePrice' extracts the target variable (output) as y.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop('SalePrice', axis=1),  # Feature set (X)\n",
    "    data.SalePrice,                  # Target variable (y)\n",
    "    test_size=0.1,                   # 10% of the data used for testing, 90% for training.\n",
    "    random_state=0                   # Ensures reproducibility by setting a random seed.\n",
    ")\n",
    "\n",
    "# Display the shape (dimensions) of the training and testing sets.\n",
    "# Useful to confirm correct data splitting and understand the number of samples and features.\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3tbXUovRi5h"
   },
   "source": [
    "### Missing values\n",
    "* Separating Date, Numerical, and Categorical variables\n",
    "* Checking missing entries in each data type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6IeyolqRi5i",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a list of columns that represent dates or years.\n",
    "# These variables often need special treatment (e.g., extracting age).\n",
    "vars_dates = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']\n",
    "\n",
    "# Create a list of categorical variables (those with data type 'object').\n",
    "# These typically represent categories or text data (e.g., 'Neighborhood', 'HouseStyle').\n",
    "vars_cat = [var for var in X_train.columns if X_train[var].dtypes == 'O']\n",
    "\n",
    "# Create a list of numerical variables (those that are not of data type 'object').\n",
    "# Exclude the 'Id' column, as it is an identifier and not a feature to be used for modeling.\n",
    "vars_num = [var for var in X_train.columns if X_train[var].dtypes != 'O' and var not in ['Id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJS1h8wBRi5j"
   },
   "outputs": [],
   "source": [
    "# Calculate the proportion of missing values for each date-related variable in the training set.\n",
    "# 'X_train[vars_dates]' selects only the date-related columns.\n",
    "# 'isnull().mean()' calculates the fraction of missing values for each column.\n",
    "# 'sort_values(ascending=False)' sorts the columns in descending order,\n",
    "# prioritizing those with the most missing values at the top.\n",
    "Date_V = X_train[vars_dates].isnull().mean().sort_values(ascending=False)\n",
    "\n",
    "# Display the sorted proportions of missing values.\n",
    "Date_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xs4nBHFu3CZO"
   },
   "outputs": [],
   "source": [
    "# Visualize missing values in our date variables\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.barplot(x=Date_V.index, y=Date_V.values, hue=Date_V.index)\n",
    "plt.xlabel(\"Date features\")\n",
    "plt.ylabel(\"Missing values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPFr7n-DRi5k",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Missing values in our numerical variables\n",
    "Num_V = X_train[vars_num].isnull().mean().sort_values(ascending=False)\n",
    "print(Num_V)\n",
    "len(Num_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3vs9Xxr3i77"
   },
   "outputs": [],
   "source": [
    "# Visualize missing values in our numerical variables\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# 'x=Num_V.index' sets the x-axis as the variable names (column names).\n",
    "# 'y=Num_V.values' sets the y-axis as the corresponding values (e.g., mean, missing value proportion, etc.).\n",
    "# 'hue=Num_V.index' colors each bar differently based on the variable name, adding a legend.\n",
    "sns.barplot(x=Num_V.index, y=Num_V.values, hue=Num_V.index)\n",
    "plt.xlabel(\"Numerical features\")\n",
    "plt.ylabel(\"Missing values\")\n",
    "plt.xticks(rotation=80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_s9a5yrRi5l",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Missing values in our categorical variables\n",
    "Cat_V = X_train[vars_cat].isnull().mean().sort_values(ascending=False)\n",
    "print(Cat_V)\n",
    "len(Cat_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKED-yaA34Dt"
   },
   "outputs": [],
   "source": [
    "# Visualize missing values in our categorical variables\n",
    "plt.figure(figsize=(20, 5))\n",
    "sns.barplot(x=Cat_V.index, y=Cat_V.values, hue=Cat_V.index)\n",
    "plt.xlabel(\"Categorical features\")\n",
    "plt.ylabel(\"Missing values\")\n",
    "plt.xticks(rotation=80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSvrywUWL4Uw"
   },
   "source": [
    "### Handling missing data through imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5Wgh4eJRi5m",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imputate numerical variables\n",
    "# Create a SimpleImputer to fill missing values in 'LotFrontage' with a constant value of -1.\n",
    "# This ensures missing values are handled consistently across both training and testing sets.\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=-1)\n",
    "\n",
    "# Apply the imputer to the 'LotFrontage' column in the training set.\n",
    "# 'to_frame()' converts the column into a DataFrame to match the input format expected by the imputer.\n",
    "X_train['LotFrontage'] = imputer.fit_transform(X_train['LotFrontage'].to_frame())\n",
    "\n",
    "# Use the same imputer (already fit on the training data) to transform the 'LotFrontage' column in the test set.\n",
    "X_test['LotFrontage'] = imputer.transform(X_test['LotFrontage'].to_frame())\n",
    "\n",
    "# Create another SimpleImputer to fill missing values in numerical variables with the most frequent value (mode).\n",
    "# This method is useful for variables with repeated values where mode is a meaningful replacement.\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Fit the imputer on the numerical variables in the training set and transform them.\n",
    "X_train[vars_num] = imputer.fit_transform(X_train[vars_num])\n",
    "\n",
    "# Apply the trained imputer to the test set to ensure consistent transformations.\n",
    "X_test[vars_num] = imputer.transform(X_test[vars_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jShoycGlRi5n"
   },
   "outputs": [],
   "source": [
    "# Imputate categorical variables\n",
    "imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "X_train[vars_cat] = imputer.fit_transform(X_train[vars_cat])\n",
    "X_test[vars_cat] = imputer.transform(X_test[vars_cat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIGSllEsRi5n"
   },
   "source": [
    "### Temporal features\n",
    "\n",
    "Extracting information from the data to capture the difference in years between the year in which the house was built, and the year in which the house was sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SbdK27xRi5o"
   },
   "outputs": [],
   "source": [
    "# Create new temporal features from date variables\n",
    "def elapsed_years(df, var):\n",
    "    # capture difference between year variable and year the house was sold\n",
    "    df[var] = df['YrSold'] - df[var]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3avmd931Ri5o"
   },
   "outputs": [],
   "source": [
    "# Apply it to both train and test set\n",
    "for var in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:\n",
    "    X_train = elapsed_years(X_train, var)\n",
    "    X_test = elapsed_years(X_test, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jvMOkp1Ri5o"
   },
   "outputs": [],
   "source": [
    "# Check that test set does not contain null values in the engineered variables\n",
    "[var for var in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt'] if X_test[var].isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wow9d8gqRi5p"
   },
   "source": [
    "### Checking for any Null still exists either in a train or test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9eEOXswRi5p"
   },
   "outputs": [],
   "source": [
    "# Train set\n",
    "[var for var in X_train.columns if X_train[var].isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5H_bToZ2Ri5q"
   },
   "outputs": [],
   "source": [
    "# Test set\n",
    "[var for var in X_train.columns if X_test[var].isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ha_nkHBm7XxE"
   },
   "source": [
    "### Replacing  all rarely appearing categories with 'Rare':\n",
    "\n",
    "The `RareLabelEncoder()` groups rare or infrequent categories in a new category called â`Rare`â, or any other name entered by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1hWazlzRi5q"
   },
   "outputs": [],
   "source": [
    "# Encode rare categories\n",
    "# Create a RareLabelEncoder to group infrequent categories in categorical variables.\n",
    "# 'tol=0.01' sets the tolerance, meaning categories that appear in less than 1% of the data will be grouped as 'Rare'.\n",
    "# 'n_categories=5' ensures that only variables with more than 5 unique categories will undergo rare label encoding.\n",
    "# 'variables=vars_cat' specifies the categorical variables to apply this transformation.\n",
    "\n",
    "rare_enc = RareLabelEncoder(tol=0.01, n_categories=5, variables=vars_cat)\n",
    "\n",
    "# Fit the encoder on the training data to learn the rare categories.\n",
    "rare_enc.fit(X_train)\n",
    "\n",
    "# Transform the training set by grouping rare categories under a common label ('Rare').\n",
    "X_train = rare_enc.transform(X_train)\n",
    "\n",
    "# Apply the same transformation to the test set to ensure consistency.\n",
    "X_test = rare_enc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwaGHtEpOkRi"
   },
   "source": [
    "### Checking for rare categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_eMQeIM_Tfu"
   },
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store the frequency distribution of each categorical variable.\n",
    "cat_dic = {}\n",
    "\n",
    "# Loop through each categorical variable in 'vars_cat'.\n",
    "for i in vars_cat:\n",
    "    # Calculate the frequency (proportion) of each category in the current variable.\n",
    "    # 'value_counts()' counts the occurrences of each unique category.\n",
    "    # Dividing by 1460 (number of rows in X_train) gives the proportion of each category.\n",
    "    freq_df = pd.DataFrame(X_train[vars_cat][i].value_counts() / 1460)\n",
    "\n",
    "    # Print the frequency distribution of the current categorical variable.\n",
    "    print(freq_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPfdKZgSO3o6"
   },
   "source": [
    "### Encoding of Categorical variables\n",
    "\n",
    "Transform the string values of categorical variables into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwIt5DR1Ri5r"
   },
   "outputs": [],
   "source": [
    "# Encode with labels\n",
    "# Create an instance of OrdinalEncoder from scikit-learn to encode categorical variables as ordinal integers.\n",
    "ordinal_enc = OrdinalEncoder_Sk()\n",
    "\n",
    "# Fit the ordinal encoder on the categorical variables of the training set and transform them.\n",
    "# This assigns each category an integer based on the order of appearance in the data.\n",
    "X_train[vars_cat] = ordinal_enc.fit_transform(X_train[vars_cat])\n",
    "\n",
    "# Transform the categorical variables in the test set using the same encoder.\n",
    "# This ensures that the same encoding scheme is applied to both training and testing datasets.\n",
    "X_test[vars_cat] = ordinal_enc.transform(X_test[vars_cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dJmVFYdRi5r"
   },
   "outputs": [],
   "source": [
    "# Check any null values in test set\n",
    "[var for var in X_train.columns if X_test[var].isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ3Mi87Ew2Va"
   },
   "source": [
    "### Building Pipeline for Pre-Processing\n",
    "\n",
    "All the pre-processing steps above can be implemented inside Pre-Processing Pipeline. Building a pipeline removes the dual task of hard coding for the same operation on the train and test set separately. Apart from this, it helps in the automation of testing and deployment without much human intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bOXf5UQj_jb"
   },
   "source": [
    "#### Creating Class for temporal transformation that is compatible with SK_learn pipeline:\n",
    "\n",
    "In the pre-processing steps above, a function was created to calculate the year elapsed. Now we are converting that function into a class suitable for inserting inside the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkxLIP5nkLFN"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TemporalVariableTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Transformer for calculating the elapsed time between a reference variable and specified temporal variables.\n",
    "\n",
    "    def __init__(self, variables, reference_variable):\n",
    "        # Initialize the transformer with the list of temporal variables and a reference variable.\n",
    "\n",
    "        # Check that 'variables' is a list; if not, raise a ValueError.\n",
    "        if not isinstance(variables, list):\n",
    "            raise ValueError('variables should be a list')\n",
    "\n",
    "        self.variables = variables  # Store the list of temporal variables.\n",
    "        self.reference_variable = reference_variable  # Store the reference variable.\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Fit method required for scikit-learn pipeline compatibility.\n",
    "        # This method does not need to perform any operations, so it just returns self.\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Transform the input DataFrame to calculate elapsed time.\n",
    "\n",
    "        # Create a copy of the DataFrame to avoid modifying the original data.\n",
    "        X = X.copy()\n",
    "\n",
    "        # Loop through each temporal variable and calculate the difference between the reference variable and the temporal variable.\n",
    "        for feature in self.variables:\n",
    "            X[feature] = X[self.reference_variable] - X[feature]  # Calculate elapsed time.\n",
    "\n",
    "        return X  # Return the modified DataFrame with updated temporal variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ak80o0g81mnI"
   },
   "source": [
    "#### Building the Pre-Processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GImUVVjhZaj3"
   },
   "outputs": [],
   "source": [
    "price_pipe = Pipeline([\n",
    "\n",
    "    # ===== IMPUTATION =====\n",
    "    # impute numerical variables with the ArbitraryNumberImputer\n",
    "    ('ArbitraryNumber_imputation', ArbitraryNumberImputer( arbitrary_number=-1, variables='LotFrontage' )),\n",
    "\n",
    "     # impute numerical variables with the mostfrequent\n",
    "    ('frequentNumber_imputation', CategoricalImputer(imputation_method='frequent', variables=vars_num, ignore_format=True)),\n",
    "\n",
    "    # impute categorical variables with string missing\n",
    "    ('missing_imputation', CategoricalImputer(imputation_method='missing', variables=vars_cat)),\n",
    "\n",
    "    # == TEMPORAL VARIABLES ====\n",
    "    ('elapsed_time', TemporalVariableTransformer(\n",
    "        variables=['YearBuilt', 'YearRemodAdd', 'GarageYrBlt'], reference_variable='YrSold')),\n",
    "\n",
    "    ('drop_features', DropFeatures(features_to_drop=['YearBuilt', 'YearRemodAdd', 'GarageYrBlt'])),\n",
    "\n",
    "      # == CATEGORICAL ENCODING\n",
    "    ('rare_label_encoder', RareLabelEncoder(tol=0.01, n_categories=5, variables=vars_cat)),\n",
    "\n",
    "    # encode categorical and discrete variables using the target mean\n",
    "    ('categorical_encoder', OrdinalEncoder(encoding_method='ordered', variables=vars_cat)), #\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLRLcrnqDCj8"
   },
   "source": [
    "Since we have already done pre-processing before the pipeline, we can't apply the pipeline to pre-preprocessed data. To apply the pipeline, copy the same train-test split cell again here so that we can get un-processed data as a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZXrqNkFDLDc"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('SalePrice', axis=1), # predictors\n",
    "                                                    data.SalePrice, # target\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=0)  # for reproducibility\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7XELxoNeU11"
   },
   "source": [
    "### Finally applying pipeline in train & test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IvRKC1sunUcG"
   },
   "outputs": [],
   "source": [
    "price_pipe.fit(X_train,y_train) # Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBj1tLiPr_Q5"
   },
   "outputs": [],
   "source": [
    "X_train_tfr = price_pipe.transform(X_train)        # Transformation for train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dimU04TXu1Y6"
   },
   "outputs": [],
   "source": [
    "X_test_tfr = price_pipe.transform(X_test)          # Transformation for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gy0F_FcFn1XS"
   },
   "outputs": [],
   "source": [
    "X_train_tfr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4ijnrccRi5s"
   },
   "source": [
    "## XG-Boost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQBQbsHVV54x"
   },
   "outputs": [],
   "source": [
    "# Create an xgboost regression model\n",
    "# Create an instance of the XGBoost regressor for a regression task.\n",
    "# This model will be used for predicting continuous target variables.\n",
    "\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=100,            # Number of boosting rounds (trees) to be created. More trees can lead to better performance.\n",
    "    max_depth=7,                 # Maximum depth of each tree. Controls the complexity of the model; deeper trees can capture more information but may overfit.\n",
    "    eta=0.1,                     # Learning rate (also known as 'alpha'). A smaller value makes the model more robust but requires more boosting rounds.\n",
    "    subsample=0.7,               # Fraction of samples used for fitting individual trees. Reduces overfitting by randomly sampling training data.\n",
    "    colsample_bytree=0.8,        # Fraction of features used for each tree. Helps prevent overfitting by introducing randomness in feature selection.\n",
    "    objective='reg:squarederror',# The learning task objective. Here, it indicates a regression task using squared error loss.\n",
    "    random_state=0               # Random seed for reproducibility. Ensures that the results can be replicated across runs.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGL8DRTo6vsy"
   },
   "source": [
    "**Note :**  Good hyperparameter values can be found by trial and error for a given dataset, or systematic experimentation such as using a grid search across a range of values.\n",
    "\n",
    "**The most commonly configured hyperparameters are the following:**\n",
    "\n",
    "**n_estimators:** The number of trees in the ensemble, often increased until no further improvements are seen.\n",
    "\n",
    "**max_depth:** The maximum depth of each tree, often values are between 1 and 10.\n",
    "\n",
    "**eta:** The learning rate used to weight each model, often set to small values such as 0.3, 0.1, 0.01, or smaller.\n",
    "\n",
    "**subsample:** The number of samples (rows) used in each tree, set to a value between 0 and 1, often 1.0 to use all samples.\n",
    "\n",
    "**colsample_bytree:** Number of features (columns) used in each tree, set to a value between 0 and 1, often 1.0 to use all features.\n",
    "\n",
    "**XGBoost Parameters Detail** - [Ref.](https://xgboost.readthedocs.io/en/stable/parameter.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UE2HaP5bRi5t"
   },
   "outputs": [],
   "source": [
    "# Train on training set\n",
    "model.fit(X_train_tfr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1tgIJV_Ri5u"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model:\n",
    "# Evaluate performance using the mean squared error and the root of the mean squared error\n",
    "pred = model.predict(X_train_tfr)\n",
    "print('linear train mse: {}'.format(mean_squared_error(y_train, pred)))\n",
    "print('linear train rmse: {}'.format(sqrt(mean_squared_error(y_train, pred))))\n",
    "print()\n",
    "\n",
    "pred = model.predict(X_test_tfr)\n",
    "print('linear test mse: {}'.format(mean_squared_error(y_test, pred)))\n",
    "print('linear test rmse: {}'.format(sqrt(mean_squared_error(y_test, pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FyzYPzpeRi5w"
   },
   "outputs": [],
   "source": [
    "# Evaluating predictions with respect to the original price\n",
    "plt.scatter(y_test, model.predict(X_test_tfr))\n",
    "plt.xlabel('True House Price')\n",
    "plt.ylabel('Predicted House Price')\n",
    "plt.title('Evaluation of XGBoost Predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF2hhXufQaib"
   },
   "source": [
    "### Displaying the feature importance value given by the XG-Boost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybgswOPrwqrc"
   },
   "outputs": [],
   "source": [
    "# List features\n",
    "print(X_train_tfr.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jp0I-BaWwZjb"
   },
   "outputs": [],
   "source": [
    "# Feature importance given by XGB\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LWko7t2w1ck"
   },
   "outputs": [],
   "source": [
    "# Feature Importance in dataframe\n",
    "# Create a DataFrame to store the feature names and their corresponding importance scores.\n",
    "dfeature = pd.DataFrame({\n",
    "    'Var': X_train_tfr.columns.to_list(),  # Extract feature names from the transformed training set.\n",
    "    'Importance': model.feature_importances_  # Get the importance scores from the trained XGBoost model.\n",
    "}).sort_values(by='Importance', ascending=False)  # Sort the DataFrame by importance scores in descending order.\n",
    "\n",
    "# Display the DataFrame containing features and their importance scores.\n",
    "dfeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4ePqHsHwWm4"
   },
   "outputs": [],
   "source": [
    "# Plot bar plot showing feature importances\n",
    "plt.figure(figsize=(24, 6))\n",
    "sns.barplot(x=dfeature['Var'], y=dfeature['Importance'], hue=dfeature['Var'])\n",
    "plt.xticks(rotation=80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDNTKoGiRUrE"
   },
   "source": [
    "We can pick features having the highest feature importance values,  for example choosing the top 15:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YoDi3vLgR_5k"
   },
   "outputs": [],
   "source": [
    "# Top 15 features\n",
    "dfeature[:16]['Var'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQzwhzZURi5x"
   },
   "source": [
    "## Feature Selection\n",
    "\n",
    "Above manual selection of best features can be done automatically using Scikit-Learn's `SelectFromModel` class. Here, we need to specify the model which has `feature_importances_` or `coef_` attribute after fitting, then train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FK2o6MIbRi5x"
   },
   "outputs": [],
   "source": [
    "# Feature selection using SelectFromModel, with XGBoost Regressor\n",
    "\n",
    "sel_ = SelectFromModel(xgb.XGBRegressor(n_estimators=150, objective='reg:squarederror', random_state=0))\n",
    "sel_.fit(X_train_tfr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6CYlp3PRi5y"
   },
   "outputs": [],
   "source": [
    "# Show the number of total features and selected features\n",
    "selected_feat = X_train_tfr.columns[(sel_.get_support())]\n",
    "print('total features: {}'.format((X_train_tfr.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ssLdy2nRi5y"
   },
   "outputs": [],
   "source": [
    "selected_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yu_HHEGhRi5z"
   },
   "source": [
    "## Re-build model with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6qaw4BfXkRK"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_tfr[selected_feat], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qx0NGU2V1swN"
   },
   "outputs": [],
   "source": [
    "# Evaluate performance using the mean squared error and the root of the mean squared error\n",
    "pred = model.predict(X_train_tfr[selected_feat])\n",
    "print('linear train mse: {}'.format(mean_squared_error(y_train, pred)))\n",
    "print('linear train rmse: {}'.format(sqrt(mean_squared_error(y_train, pred))))\n",
    "print()\n",
    "pred = model.predict(X_test_tfr[selected_feat])\n",
    "print('linear test mse: {}'.format(mean_squared_error(y_test, pred)))\n",
    "print('linear test rmse: {}'.format(sqrt(mean_squared_error(y_test, pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cH8uAMlNYgCq"
   },
   "source": [
    "### Evaluating predictions with respect to the original price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-vxoss6OYfkd"
   },
   "outputs": [],
   "source": [
    "# Evaluating predictions with respect to the original price\n",
    "plt.scatter(y_test, model.predict(X_test_tfr[selected_feat]))\n",
    "plt.xlabel('True House Price')\n",
    "plt.ylabel('Predicted House Price')\n",
    "plt.title('Evaluation of XGBoost Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ru62nz22uWPg"
   },
   "source": [
    "## Training XGBoost without Pre-Processing\n",
    "\n",
    "XGBoost can handle categorical variable( [Ref.](https://xgboost.readthedocs.io/en/stable/tutorials/categorical.html) )and supports missing values by default. In tree algorithms, branch directions for missing values are learned during training. Note that the gblinear booster treats missing values as zeros. When the missing parameter is specifed, values in the input predictor that is equal to missing will be treated as missing and removed. By default itâs set to NaN.Considering the same we are going to train without pre-processing and compare the result.([Ref.](https://xgboost.readthedocs.io/en/stable/faq.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8BAJ4MVufQS"
   },
   "outputs": [],
   "source": [
    "data_no_pro = pd.read_csv('housing_dataset.csv')\n",
    "print(data.shape)\n",
    "data_no_pro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HlzXDAUwulTz"
   },
   "outputs": [],
   "source": [
    "# Create a list of categorical variables (those with data type 'object') from the training DataFrame.\n",
    "# This is done by iterating over all columns in X_train and checking their data types.\n",
    "\n",
    "vars_cat = [var for var in X_train.columns if X_train[var].dtypes == 'O']\n",
    "\n",
    "# 'vars_cat' will contain the names of all categorical variables, which can be useful for further preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1asePBpumS7"
   },
   "outputs": [],
   "source": [
    "# Convert all categorical variables in the DataFrame 'data_no_pro' to the 'category' data type.\n",
    "# This can help reduce memory usage and improve performance when dealing with categorical data.\n",
    "\n",
    "data_no_pro[vars_cat] = data_no_pro[vars_cat].apply(lambda x: x.astype('category'))\n",
    "\n",
    "# The 'apply' function is used to apply the lambda function to each column specified in 'vars_cat'.\n",
    "# The lambda function converts each column to the 'category' type, which is more efficient for categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cp66KF1uqF8"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_no_pro.drop('SalePrice', axis=1), # predictors\n",
    "                                                    data_no_pro.SalePrice, # target\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=0)  # for reproducibility\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GloaGcj0utsL"
   },
   "source": [
    "### Temporal features\n",
    "Extracting information from the data to capture the difference in years between the year in which the house was built, and the year in which the house was sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lnx4D40AuyHM"
   },
   "outputs": [],
   "source": [
    "# Create new temporal features from date variables\n",
    "def elapsed_years(df, var):\n",
    "    # capture difference between year variable and year the house was sold\n",
    "    df[var] = df['YrSold'] - df[var]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IrNgY3HQu2Q2"
   },
   "outputs": [],
   "source": [
    "# Apply it to both train and test set\n",
    "for var in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:\n",
    "    X_train = elapsed_years(X_train, var)\n",
    "    X_test = elapsed_years(X_test, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0vBsbmqxUh1"
   },
   "outputs": [],
   "source": [
    "X_train=X_train.drop(columns=['YearBuilt', 'YearRemodAdd', 'GarageYrBlt'])\n",
    "X_test=X_test.drop(columns=['YearBuilt', 'YearRemodAdd', 'GarageYrBlt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38f1h3Abu6P5"
   },
   "source": [
    "###Modelling with unprocessed features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQuZNIteu2Wu"
   },
   "outputs": [],
   "source": [
    "# Create an xgboost regression model\n",
    "model_no_pro = xgb.XGBRegressor(n_estimators=100,max_depth=6, eta=0.1, subsample=0.7, colsample_bytree=0.8, objective='reg:squarederror', random_state=0, enable_categorical=True, tree_method='approx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GryNYUnBu_Ca"
   },
   "outputs": [],
   "source": [
    "# Train on training set\n",
    "model_no_pro.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIu0tDfWyaQ5"
   },
   "source": [
    "**Note - Hyperparameters:**   **enable_categorical** - [Ref.](https://xgboost.readthedocs.io/en/stable/tutorials/categorical.html), **tree_method** - [Ref.](https://xgboost.readthedocs.io/en/stable/parameter.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_cBP0JkfvEFz"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model:\n",
    "# Evaluate performance using the mean squared error and the root of the mean squared error\n",
    "pred = model_no_pro.predict(X_train)\n",
    "print('linear train mse: {}'.format(mean_squared_error(y_train, pred)))\n",
    "print('linear train rmse: {}'.format(sqrt(mean_squared_error(y_train, pred))))\n",
    "print()\n",
    "pred = model_no_pro.predict(X_test)\n",
    "print('linear test mse: {}'.format(mean_squared_error(y_test, pred)))\n",
    "print('linear test rmse: {}'.format(sqrt(mean_squared_error(y_test, pred))))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "583px",
    "left": "0px",
    "right": "1324px",
    "top": "107px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
