{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tdtrlAhvIHY"
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "At the end of the experiment, you will be able to:\n",
    "\n",
    "* understand the big picture of transformers\n",
    "* understand and work with the TextVectorization layer\n",
    "* understand and work with the Embedding layer\n",
    "* learn word embeddings during model training\n",
    "* perform visualization of word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tv8TQrtfwaNd"
   },
   "source": [
    "### The Big Picture of Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ni6YCBwE7d_D"
   },
   "source": [
    "TextVectorization and Embedding Layers are used in Encoder-Decoder Transformer.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST%205%20Big%20Picture.png\" width=800px/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxzUg64Jdv-s"
   },
   "source": [
    "Above is the entire architecture of transformer. A TextVectorization layer, Embedding layer, an Encoder and a Decoder.\n",
    "\n",
    "Transformer architecture follows an encoder-decoder structure. The encoder, on the left-hand side, is tasked with mapping an input sequence to a sequence of continuous representations; the decoder, on the right-hand side, receives the output of the encoder together with the decoder output at the previous time step to generate an output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgVPmcTjnORs"
   },
   "source": [
    "The Transformer architecture was originally designed for translation. In the encoder, the attention layers can use all the words in a sentence (since, as we just saw, the translation of a given word can be dependent on what is after as well as before it in the sentence). The decoder, however, works sequentially and can only pay attention to the words in the sentence that it has already translated (so, only the words before the word currently being generated). For example, when we have predicted the first three words of the translated target, we give them to the decoder which then uses all the inputs of the encoder to try to predict the fourth word.\n",
    "\n",
    "To speed things up during training (when the model has access to target sentences), the decoder is fed the whole target, but it is not allowed to use future words (if it had access to the word at position 2 when trying to predict the word at position 2, the problem would not be very hard!). For instance, when trying to predict the fourth word, the attention layer will only have access to the words in positions 1 to 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtUillZR7eEN"
   },
   "source": [
    "TextVectorization and Embedding Layers are also required in Encoder-only Transformer.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5_AST_05_Image1_Transformer.png\" width=900px/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geRywC9LeHjR"
   },
   "source": [
    "In this assignment encoder & decoder will not form the topic of discussion, the main focus will be on the TextVectorization and Embedding Layers.\n",
    "This has been discussed in detail in the later sections of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECysu18IoMaW"
   },
   "source": [
    "## Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLrl_aUaoPam"
   },
   "source": [
    "The **IMDb Movie Reviews dataset** is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as *positive* or *negative*. The dataset contains an even number of positive and negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzZ2UaHyoS6z"
   },
   "source": [
    "This dataset is processed and used in the later sections of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34uyN3Lm_ghZ"
   },
   "source": [
    "### Setup Steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RH8Ecq9sbYU"
   },
   "source": [
    "### Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnUhfGmx9cup"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, pathlib, shutil, random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, Dense\n",
    "from tensorflow.keras.utils import text_dataset_from_directory\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtDuXYqa7cbA"
   },
   "source": [
    "## TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIv-Yv3EnQ4_"
   },
   "source": [
    "It invloves preparing the text data:\n",
    "  * Text standardization\n",
    "  * Text splitting into tokens\n",
    "  * Vocabulary indexing\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o590tI2BldJt"
   },
   "source": [
    "A flowchart depicting the procedure or sequence of steps followed in a TextVectorization layer.\n",
    "* 'Standardization' is taking care of basic preprocessing of text data such as removing the punctuation and converting the text to lower case.\n",
    "* 'Tokenization' is giving the list of words from the sentence.\n",
    "* Later, these words are represented with indices and with the help of embedding to get the vector encoding of indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-m7SHDzoHHz"
   },
   "source": [
    "<center>\n",
    "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5_AST_05_Transformer_Encoder_Text_data_prep.png\" width=650px/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RGm-7ySooG0"
   },
   "source": [
    "All these steps are performed in a TextVectorization Layer.\n",
    "\n",
    "\n",
    "*   Keras provides a TextVectorization layer which can be dropped directly into\n",
    "      - a tf.data pipeline **or**\n",
    "      - a Keras model\n",
    "\n",
    "*  MOREOVER, TextVectorization also handles both approaches of representing groups of words:\n",
    "      - Words as a set or Bag-of-words\n",
    "      - Words as a sequence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QdHTVk5K_wR"
   },
   "source": [
    "### Define a dummy dataset and a test sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njuS3JqcS-RQ"
   },
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "# dataset_t = [\"I write, rewrite, and still rewrite again\"]\n",
    "#Q: Is the word 'still' in the dataset (vocabulary)? Is it there in the test_sentence?\n",
    "#Q: How many words in test_sentence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9sp6faVpYYU"
   },
   "source": [
    "### Create a TextVectorization layer and adapt to dummy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pB0aHkyhr3D-"
   },
   "source": [
    "Create and demonstrate the working of a TextVectorization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0AS4G6RqMEU"
   },
   "outputs": [],
   "source": [
    "#Q: What 3 things does a TextVec layer do?\n",
    "\n",
    "# Instantiating a TextVectorization layer/object with output mode as integer\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",              # int is default. There are different kinds of modes available\n",
    "    max_tokens=15,                  # Vocabulary size\n",
    "    output_sequence_length=10,      # Maximum length of output sequence\n",
    "    # We can use custom functions also for standardizing and splitting the text - see the Book by Chollet\n",
    "    # standardize=custom_standardization_fn,\n",
    "    # split=custom_split_fn,\n",
    ")\n",
    "\n",
    "# Adapt to data\n",
    "text_vectorization.adapt(dataset)      # Computes a vocabulary of string terms from tokens in a dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eqL6sCaWTnTH"
   },
   "outputs": [],
   "source": [
    "# To see the working of TextVectorization\n",
    "\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "print(f\"vocabulary = {vocabulary}\")\n",
    "print(f\"len(vocabulary) = {len(vocabulary)}\")\n",
    "\n",
    "# To see how the the text_vec layer transforms/vectorizes the raw text\n",
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "# YOUR CODE HERE to show the 'encoded_sentence' and its length\n",
    "\n",
    "\n",
    "# decode back for comparison with test_sentence\n",
    "inverse_vocab = dict(enumerate(vocabulary)) # making a dictionary to decode embeddings\n",
    "print(f\"inverse_vocab = {inverse_vocab}\")\n",
    "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    "print(f\"decoded sentence = {decoded_sentence}\")\n",
    "\n",
    "print(f\"test_sentence = {test_sentence}\")\n",
    "\n",
    "# Q: What is a vocabulary?\n",
    "# Q: No. of tokens in vocabulary?\n",
    "# Q: Length of encoded_sentence (output of TextVec layer)?\n",
    "# Q: Type of elements in encoded_sentence (embedding)?\n",
    "# Q: Is decoded sentence the same as the test_sentence? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flOZCU1JQNYR"
   },
   "source": [
    "## Processing the dataset using TextVectorization layer of keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkoDTKmd900K"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XIAoz7enlj6"
   },
   "source": [
    "A pre-processed version of the IMDB dataset provided by Keras was used in the previous assignments.\n",
    "\n",
    "Originally IMDB dataset contains the *train* and the *test* folders.\n",
    "Here, the original dataset will be used and pre-processing related to it will be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46rYfHyZrqbs"
   },
   "outputs": [],
   "source": [
    "# List subdirectories\n",
    "!cd aclImdb && ls -d */"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFgcefQNLh-q"
   },
   "outputs": [],
   "source": [
    "# Remove unnecessary folder\n",
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-DWQXmaYLuha"
   },
   "outputs": [],
   "source": [
    "# Visualise a sample\n",
    "!cat aclImdb/train/pos/4077_10.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Za7BufC_2q_g"
   },
   "source": [
    "### Create a validation directory and move 20% of the train data to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BseO3_sLukB"
   },
   "outputs": [],
   "source": [
    "# move 20% of the training data to the validation folder\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    # random.Random(1337).shuffle(files) # We should shuffle. Only commenting for demonstration\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBeeGi5S8pbs"
   },
   "source": [
    "### Create batches of data using `text_dataset_from_directory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDLoEh2CLwB6"
   },
   "outputs": [],
   "source": [
    "# Create dataset using utility\n",
    "batch_size = 32\n",
    "\n",
    "# Q: Name other such utilities seen earlier ?\n",
    "train_ds = text_dataset_from_directory(\"aclImdb/train\", batch_size=batch_size)\n",
    "\n",
    "val_ds = # YOUR CODE HERE to apply text_dataset_from_directory() with path \"aclImdb/val\"\n",
    "\n",
    "test_ds = # YOUR CODE HERE to apply text_dataset_from_directory() with path \"aclImdb/test\"\n",
    "\n",
    "# Extracting only the review text(not labels); to be used later to adapt the TextVec layer\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)             # lambda x, y: x  --> replace x,y with x. That is remove labels, just keep text data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htOQeKY98_3n"
   },
   "source": [
    "There are 20000, 5000, and 25000 records in train, validation, and test directories with two class as positive and negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_03-Oj9LwD5"
   },
   "outputs": [],
   "source": [
    "# Check shapes\n",
    "\n",
    "for inputs, targets in train_ds:\n",
    "\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "\n",
    "    # YOUR CODE HERE to show shape and datatype of 'targets'\n",
    "\n",
    "    print(\"inputs[2]:\", inputs[2])\n",
    "    print(\"targets[2]:\", targets[2])\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sR63E5Ea9fDw"
   },
   "source": [
    "### Create TextVectorization layer and adapt to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bqi5Z24gMK9j"
   },
   "outputs": [],
   "source": [
    "# Vectorizing the data\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = # YOUR CODE HERE to create a TextVectorization layer using max_tokens as vocabular size, \"int\" as output type for a token, 600 as maximum length of review\n",
    "\n",
    "# YOUR CODE HERE to adapt text_vectorization() layer on 'text_only_train_ds'\n",
    "\n",
    "\n",
    "# Apply TextVec to train, val, test set\n",
    "\n",
    "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), tf.reshape(y, (-1,1))),\n",
    "                            num_parallel_calls=4)\n",
    "\n",
    "int_val_ds = # YOUR CODE HERE to apply text_vectorization() on val_ds\n",
    "\n",
    "int_test_ds = # YOUR CODE HERE to apply text_vectorization() on test_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7sj-tcc9v2w"
   },
   "source": [
    "### Visualize and compare the raw and processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMUYGUqBx3K1"
   },
   "outputs": [],
   "source": [
    "# Let's visualize the raw text and the vectorized (to int) text\n",
    "for text, label in train_ds:\n",
    "  print(text[0])\n",
    "  print(label[0])\n",
    "  break\n",
    "\n",
    "# YOUR CODE HERE to create a for loop to the sample text and label from 'int_train_ds'\n",
    "\n",
    "\n",
    "# Q: How can you verify whether the index of movie is 18?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P01-UwZIAuzH"
   },
   "source": [
    "Vector representation of the word 'movie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jl8iSY55z1A0"
   },
   "outputs": [],
   "source": [
    "text_vectorization(\"movie\")\n",
    "# Q: What is the shape of the TV output?\n",
    "# Q: Why so many 0s?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oAL6gzUAzvl"
   },
   "source": [
    "Vector representation of \"great movie\" and \"a fine story\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCDXUf8bDRIz"
   },
   "outputs": [],
   "source": [
    "text_vectorization([\"great movie\", \"a fine story\"])\n",
    "#Q: shape?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BL3w3DRqVy5B"
   },
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ex3lQf00YyM-"
   },
   "source": [
    "**Why do we need Word Embeddings?**\n",
    "\n",
    "To deal with textual data, we need to convert it into numbers before feeding it into any machine learning model. For simplicity, words can be compared to categorical variables. We use one-hot encoding to convert categorical features into numbers. To do so, we create dummy features for each of the category and populate them with 0's and 1's.\n",
    "\n",
    "Similarly, if we use one-hot encoding on words in textual data, we will have a dummy feature for each word, which means 10,000 features for a vocabulary of 10,000 words. This is not a feasible embedding approach as it demands large storage space for the word vectors and reduces model efficiency and no relation is captured between words.\n",
    "\n",
    "**Word embeddings** are vector representations of words that achieve exactly this: they map human language into a structured geometric space.\n",
    "\n",
    "* dense (floats)\n",
    "* low-dimensional (1024 dims for large vocabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMKg5tOpcgXF"
   },
   "source": [
    "There are two ways to obtain word embeddings:\n",
    "\n",
    "* Learn word embeddings jointly with the main task you care about (such as document classification or sentiment prediction). In this setup, you start with random word vectors and then learn word vectors, in the same way you learn the weights of a neural network. **Move away from manual feature engineering.**\n",
    "* Load into your model word embeddings that were precomputed using a different machine learning task than the one you’re trying to solve. These are called pretrained word embeddings.\n",
    "\n",
    "**Q: Do two ways remind you of something we studied in CNNs ?**\n",
    "\n",
    "In this assignment the main agenda is to explore the Learning of word embeddings.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5OVct9NckRD"
   },
   "source": [
    "### Embedding Layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdLRIpXwkQAS"
   },
   "source": [
    "The procedure if as follows:\n",
    "\n",
    "*   Like a dictionary that **maps integer indices** (which stand for specific words) **to dense vectors**\n",
    "\n",
    "*   Input: a rank-2 tensor of integers, of shape (batch_size, sequence_length)\n",
    "*   Output: 3D floating-point tensor of shape (batch_size, sequence_length, embedding_dimensionality)\n",
    "*   WORD INDEX ⭢ EMBEDDING LAYER ⭢ CORRESPONDING WORD VEC\n",
    "\n",
    "*   Initial weights are random\n",
    "*   Learns specialized structure upon training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4O2LtP7V_jWe"
   },
   "source": [
    "### Visualization of Word Embeddings\n",
    "\n",
    "Apply dimensionality reduction to the word embeddings to convert it into 2D. Later, plot this 2D vector.\n",
    "\n",
    "<br><br>\n",
    "<center>\n",
    "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Word_Embedding.png\" width=\"650\" height=\"450\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G91_UEMy-gpX"
   },
   "source": [
    "Visualization in 3D:\n",
    "\n",
    "<center>\n",
    "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST5%20Embedding%20Layer.png\" width=750px/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4mowLS1YiVC"
   },
   "source": [
    "### Define a NN architecture with a TextVectorization layer, an Embedding layer, and Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7tm5OHz0Ha1s"
   },
   "outputs": [],
   "source": [
    "max_tokens = 20000\n",
    "inputs = keras.Input(shape=(1,), dtype=tf.string)           # shape=(None,), dtype=\"int64\"\n",
    "\n",
    "# The Text Vectoritation layer\n",
    "txt_vec_out = # YOUR CODE HERE to add text_vectorization() layer                # Note that this TextVec layer is already apadted on the train dataset\n",
    "\n",
    "# The Embedding layer\n",
    "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256, name='embedding')(txt_vec_out)    # the largest integer (i.e. word index) in the input\n",
    "                                                                                                    # should be no larger than 19999 (vocabulary size).\n",
    "# Q: What is the input to the Embedding layer?\n",
    "# Q: What is the dimension of the output embeddings?\n",
    "# Q: In embedding layer shape, what are None and None?\n",
    "\n",
    "x = # YOUR CODE HERE to add a GlobalAveragePooling1D layer\n",
    "x = # YOUR CODE HERE to add a Dense layer with 64 neurons\n",
    "x = # YOUR CODE HERE to add a Dense layer with 32 neurons\n",
    "x = # YOUR CODE HERE to add a Dropout layer\n",
    "x = # YOUR CODE HERE to add a Dense layer with 16 neurons\n",
    "x = # YOUR CODE HERE to add a Dropout layer\n",
    "outputs = # YOUR CODE HERE to add a final Dense layer with 1 neuron, use the appropriate activation function\n",
    "\n",
    "model = # YOUR CODE HERE to create the keras model with (inputs, outputs)\n",
    "\n",
    "# YOUR CODE HERE to compile the model use \"rmsprop\" optimizer, \"binary_crossentropy\" loss, \"accuracy\" as performance metric)\n",
    "\n",
    "model.summary()\n",
    "#Q: Weights in the embedding layer?\n",
    "#Hint: Dict; 1 input word => embedding of size ___ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPgn8y_bDH5g"
   },
   "source": [
    "### Visualize the words in 2D-plane by reducing the dimensions using PCA\n",
    "\n",
    "Use the word embeddings from before and after model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sr0WUiNtIFUa"
   },
   "outputs": [],
   "source": [
    "# Get the embedding layer\n",
    "embedding_layer = model.get_layer('embedding')\n",
    "\n",
    "# Get the embeddings\n",
    "embeddings = embedding_layer.get_weights()[0]\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WcbbZGCNRXgU"
   },
   "outputs": [],
   "source": [
    "# Get the vocabulary from the TextVectorization layer\n",
    "vocab = text_vectorization.get_vocabulary()\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3RCOsGSRXgW"
   },
   "outputs": [],
   "source": [
    "# Sample words to visualize word embeddings for\n",
    "test_words = ['good', 'bad', 'nice', 'poor', 'terrible', 'terrific', 'awesome', 'awful', 'best', 'worst']\n",
    "\n",
    "print(f\"{'Word':<15} {'Index'}\")\n",
    "print(\"=\"*30)\n",
    "for word in test_words:\n",
    "    print(f\"{word:<15} {vocab.index(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0pgCNGOIFUd"
   },
   "outputs": [],
   "source": [
    "# Embedding dimension\n",
    "embeddings[vocab.index('good')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pjxK8A-o6Cdu"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a 2-dimensional PCA model of the word vectors using the scikit-learn PCA class\n",
    "# n_components in PCA specifies the no. of dimensions\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "\n",
    "# Fit and transform the vectors using PCA model\n",
    "reduced_untrained_emb = pca.fit_transform(embeddings)\n",
    "reduced_untrained_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SJ4bmMPIFUf"
   },
   "outputs": [],
   "source": [
    "# Reduced embedding for word 'good'\n",
    "reduced_untrained_emb[vocab.index('good')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mOYsvNeNbjcl"
   },
   "outputs": [],
   "source": [
    "# Visualize the embeddings\n",
    "plt.figure(figsize=(8, 6))\n",
    "for word in test_words:\n",
    "    if word != '':  # Skip the empty string token\n",
    "        x, y = reduced_untrained_emb[vocab.index(word)]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(word, (x, y), xytext=(5, 2), textcoords='offset points')\n",
    "\n",
    "plt.title(\"Word Embeddings Visualization (Before training)\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DL5Gb3tIFUh"
   },
   "source": [
    "### Train the model *(Switch to GPU runtime if needed)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGtVcVbnIFUi"
   },
   "outputs": [],
   "source": [
    "# Fit the model on train set\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\"one_hot_dense.keras\", save_best_only=True)]\n",
    "\n",
    "# Change target shape from (None,) to (None, 1)\n",
    "train_dataset = train_ds.map(lambda x, y: (x, tf.reshape(y, (-1,1))))                # Note that we are using 'train_ds' and not 'int_train_ds'\n",
    "val_dataset = # YOUR CODE HERE to updated val_ds target\n",
    "\n",
    "# YOUR CODE HERE to train the model on 'train_dataset', use 'val_dataset' for validation, train it for 20 epochs, specify the callbacks list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qyC4SWhR3k1X"
   },
   "outputs": [],
   "source": [
    "## Load saved model\n",
    "# model = keras.models.load_model(\"one_hot_dense.keras\")\n",
    "\n",
    "# Check model performance\n",
    "test_dataset = test_ds.map(lambda x, y: (x, tf.reshape(y, (-1,1))))\n",
    "\n",
    "# YOUR CODE HERE to evaluate model on 'test_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRLocsUHLV8W"
   },
   "source": [
    "From the above test accuracy, it can be seen that the model perfomance is not that well. It is expected as we are using only Dense layers.\n",
    "\n",
    "Let's see if the embeddings learned during training were able to capture the semantic relationships between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8G_TRTfIFUj"
   },
   "outputs": [],
   "source": [
    "# Get the embedding layer\n",
    "trained_embedding_layer = # YOUR CODE HERE to get 'embedding' layer\n",
    "\n",
    "# Get the embeddings\n",
    "trained_embeddings = # YOUR CODE HERE to get the embedding_layer weights\n",
    "trained_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrsYQ-5Y58nb"
   },
   "outputs": [],
   "source": [
    "# Create a 2-dimensional PCA model of the word vectors using the scikit-learn PCA class\n",
    "# n_components in PCA specifies the no.of dimensions\n",
    "pca = # YOUR CODE HERE to instantiate PCA()\n",
    "\n",
    "# Fit and transform the vectors using PCA model\n",
    "reduced_trained_emb = # YOUR CODE HERE to apply pca and transform the 'trained_embeddings'\n",
    "reduced_trained_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-iiGLD_RIFUk"
   },
   "outputs": [],
   "source": [
    "# Visualize the embeddings after model training\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhxPUb2WF-Nj"
   },
   "source": [
    "From the above plot, it can be seen that good nice are more related, bad poor are more related, and so on."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
