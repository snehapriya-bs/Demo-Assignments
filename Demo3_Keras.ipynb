{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Y-w8Vn7pVP2"
   },
   "source": [
    "## Learning Objectives:\n",
    "\n",
    "In this tutorial, we will learn to implement the following:\n",
    "\n",
    "> Section-1\n",
    "* Building a neural network from scratch with tensorflow operations\n",
    "\n",
    "> Section-2\n",
    "* Keras Sequential API\n",
    "* Keras Functional API\n",
    "* Keras Model subclassing\n",
    "* Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMFDPJD1zT5x"
   },
   "source": [
    "# **Section - 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75f6R38aUnJn"
   },
   "source": [
    "## Introduction to Keras\n",
    "\n",
    "Keras is an open-source neural network library written in Python that allows you to build and train deep learning models. It provides a user-friendly and modular API for creating and configuring deep neural networks with high-level abstractions. Keras is built on top of other popular deep learning frameworks such as TensorFlow, Theano, and CNTK. It has a wide range of applications in areas such as computer vision, natural language processing, and time-series forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVF-O0dYjUZu"
   },
   "source": [
    "### Setup Steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eatd_MK1leSr"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ycvvjf4Iw2nc"
   },
   "outputs": [],
   "source": [
    "# Import TensorFlow and Keras (a high-level API within TensorFlow)\n",
    "import tensorflow as tf  # Core TensorFlow library\n",
    "from tensorflow import keras  # Keras module within TensorFlow for building models\n",
    "\n",
    "# Import additional libraries for mathematical operations and data handling\n",
    "import math  # Provides mathematical functions (like sqrt, sin, etc.)\n",
    "import numpy as np  # For numerical computing, including arrays and matrix operations\n",
    "import pandas as pd  # For handling and analyzing structured data (e.g., DataFrames)\n",
    "import matplotlib.pyplot as plt  # For plotting and visualization of data\n",
    "\n",
    "# Import specific Keras layers and tools for building neural networks\n",
    "from keras.layers import Dense, Flatten  # Dense: fully connected layer, Flatten: reshapes input data\n",
    "from keras import Input  # Used to define the input layer of a model\n",
    "from tensorflow.keras.utils import plot_model  # To visualize the architecture of a neural network\n",
    "\n",
    "# Import the MNIST dataset (handwritten digit dataset) for training and testing\n",
    "from tensorflow.keras.datasets import mnist  # Provides training and test data for the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "517QIYKpfVuw"
   },
   "source": [
    "## Basic Sequential Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Tdm4MV4T7ws"
   },
   "source": [
    "We want to build a sequential model. This means that the layers of our neural network are stacked sequentially. The approach is as follows:\n",
    "1.  First implement a class to build a dense layer. We call it \"NaiveDense\"\n",
    "2.  Implement a class (\"NaiveSequential\") to stack the layers sequentially and build a sequential model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpdyi8kVw-zz"
   },
   "outputs": [],
   "source": [
    "# Implementing a custom dense (fully connected) layer class\n",
    "class NaiveDense:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        \"\"\"\n",
    "        Initializes the layer with the given input size, output size, and activation function.\n",
    "        Args:\n",
    "            input_size (int): Number of input features.\n",
    "            output_size (int): Number of neurons in this layer.\n",
    "            activation (function): Activation function (e.g., ReLU, Sigmoid) applied to the output.\n",
    "        \"\"\"\n",
    "        self.activation = activation  # Store the activation function for later use\n",
    "\n",
    "        # Define the shape of the weights matrix (input_size x output_size)\n",
    "        w_shape = (input_size, output_size)\n",
    "\n",
    "        # Initialize the weights with small random values (between 0 and 0.1)\n",
    "        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
    "        # Store the weights as a TensorFlow variable to allow for updates during training\n",
    "        self.w = tf.Variable(w_initial_value)\n",
    "\n",
    "        # Define the shape of the bias vector (one bias per output neuron)\n",
    "        b_shape = (output_size,)\n",
    "\n",
    "        # Initialize the biases to zero\n",
    "        b_initial_value = tf.zeros(b_shape)\n",
    "        # Store the biases as a TensorFlow variable to enable updates during training\n",
    "        self.b = tf.Variable(b_initial_value)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        \"\"\"\n",
    "        This method makes the class instance callable like a function.\n",
    "        It computes the layer's output by performing a matrix multiplication and adding the bias.\n",
    "        Args:\n",
    "            inputs (Tensor): Input data (batch of features).\n",
    "        Returns:\n",
    "            Tensor: Activated output after applying the activation function.\n",
    "        \"\"\"\n",
    "        # Perform matrix multiplication between inputs and weights, add bias, and apply activation\n",
    "        return self.activation(tf.matmul(inputs, self.w) + self.b)  # Bias is broadcasted to match dimensions\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        \"\"\"\n",
    "        A property to access the weights and biases of the layer.\n",
    "        Returns:\n",
    "            tuple: (weights, biases)\n",
    "        \"\"\"\n",
    "        return (self.w, self.b)  # Returns weights and biases as a tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zggIWLcPbj-2"
   },
   "source": [
    "We implemented a dense layer. Now we will stack them together sequentially in our NaiveSequential class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kA-qPdezucM"
   },
   "outputs": [],
   "source": [
    "class NaiveSequential:\n",
    "    def __init__(self,layers):       # Layers: list of layer objects\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:    # Ouptut of the prev layer is the input to the next layer\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        weights = []\n",
    "        for layer in self.layers:     # Save weights of each layer to a list\n",
    "            weights += layer.weights  # Q: What does layer.weights return?\n",
    "        return weights                # A: layer.weights calls the function layer.weights() since it decorated with @property. It returns (w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WNsmMS1dlKn"
   },
   "source": [
    "Sequential stacking of dense layer is implemented.\n",
    "\n",
    "Further, instantiate NaiveSequential class and make our first NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6D514nbI_-h"
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = NaiveSequential([\n",
    "        NaiveDense(input_size=28*28, output_size=512, activation=tf.nn.relu),\n",
    "        NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax)\n",
    "])\n",
    "# Q: What input argument does NaiveSequential take? A: list of layer objects\n",
    "# Q: What is the input and output dimension of the overall model? A: input dim = 784, output dim = 10\n",
    "# Q: Can the output_size of 1 layer be different from the input_size of the next layer? A: No, they have to be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ud2-Cqli9IMV"
   },
   "source": [
    "'model' is the object of 'NaiveSequential' class. This class has 'weights' as one of the methods of the class which is accessed using 'model.weights'. When 'NaiveDense' class is used as a function it will return the initial values of weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQ-ZcN9-z4Po"
   },
   "outputs": [],
   "source": [
    "print(model.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elt6Stitd89N"
   },
   "source": [
    "The sequential model is untrained and currently not useful.\n",
    "\n",
    "We must train the model to make it learn useful representaions but first we need data.\n",
    "\n",
    "So, we will solve a calssification problem by using above sequential model using [MNIST dataset](https://keras.io/api/datasets/mnist/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVTPXHcfUFA6"
   },
   "source": [
    "### MNIST Dataset\n",
    "\n",
    "The **MNIST dataset** contains images of handwritten digits. It has a training set of 60,000 images, and a test set of 10,000 images.\n",
    "\n",
    "<center><img src='https://storage.googleapis.com/tfds-data/visualization/fig/mnist-3.0.1.png' width=400px></center>\n",
    "\n",
    "Now, let us do the following:\n",
    "1. Load the data\n",
    "2. Reshape the data according to the input shape of the model\n",
    "3. Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VI9typezHEjU"
   },
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "# The MNIST dataset contains 28x28 grayscale images of handwritten digits (0-9).\n",
    "\n",
    "# Display the shape of the training images and labels\n",
    "print(f\"train_images.shape = {train_images.shape}\")  # Output: (60000, 28, 28)\n",
    "print(f\"train_labels.shape = {train_labels.shape}\")  # Output: (60000,)\n",
    "\n",
    "# Reshape and normalize the data for use in neural networks\n",
    "train_images = train_images.reshape((len(train_images), 28 * 28)).astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((len(test_images), 28 * 28)).astype(\"float32\") / 255\n",
    "\n",
    "# Explanation:\n",
    "# - Reshape the images from (60000, 28, 28) to (60000, 784) to flatten them into 1D vectors.\n",
    "# - Convert pixel values from integers (0 to 255) to floats (0.0 to 1.0) by dividing by 255.\n",
    "# - Normalization ensures that the input values are scaled, improving model convergence.\n",
    "\n",
    "# Display the training labels\n",
    "print(train_labels)\n",
    "# Output: [5 0 4 ... 5 6 8]\n",
    "# The labels represent the digits (0 to 9) corresponding to each image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jzc2HM4d0xu4"
   },
   "source": [
    "### Visualize Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VR186wpO0nwO"
   },
   "outputs": [],
   "source": [
    "# Read image\n",
    "img =train_images[2].reshape(28,28)\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.grid(True)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkUUYF4Ye4_q"
   },
   "source": [
    "We divide the data into batches. For this operation, we implement a class for Batch Generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5GXARTD3uDNr"
   },
   "outputs": [],
   "source": [
    "# A class to generate batches of data for training or testing\n",
    "class BatchGenerator:\n",
    "\n",
    "    def __init__(self, images, labels, batch_size=128):\n",
    "        # Ensure the number of images and labels are the same\n",
    "        assert len(images) == len(labels), \"Images and labels must have the same length\"\n",
    "\n",
    "        # Initialize starting index for batch generation\n",
    "        self.index = 0\n",
    "\n",
    "        # Store the provided images and labels\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "        # Set the batch size (default: 128 samples per batch)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Calculate the total number of batches needed (rounding up if necessary)\n",
    "        self.num_batches = math.ceil(len(images) / batch_size)\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Fetch the next batch of images and labels.\"\"\"\n",
    "        # Get the current batch of images and labels based on the current index\n",
    "        images = self.images[self.index : self.index + self.batch_size]\n",
    "        labels = self.labels[self.index : self.index + self.batch_size]\n",
    "\n",
    "        # Increment the index to point to the next batch\n",
    "        self.index += self.batch_size\n",
    "\n",
    "        # Return the batch of images and labels\n",
    "        return images, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlhLK02pBLd5"
   },
   "source": [
    "`batch_generator.num_batches` is an attribute of the 'BatchGenerator' class which represents the total number of batches that can be generated from the given dataset of images and labels, based on the specified batch size. It is calculated as the total number of images divided by the batch size, rounded up to the nearest integer using the `math.ceil()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZ44-4yk1QfU"
   },
   "outputs": [],
   "source": [
    "batch_generator = BatchGenerator(train_images, train_labels)  # Initialize the batch generator\n",
    "batch_generator.num_batches  # Access the number of batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yq-nyLJWBixN"
   },
   "source": [
    "`batch_generator.next()` is a method of the BatchGenerator class which generates the next batch of images and labels from the dataset. Each time next() is called, it returns a tuple of images and labels corresponding to the next batch of size batch_size, and updates the internal index pointer to point to the start of the next batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxbB6T_V1ndM"
   },
   "outputs": [],
   "source": [
    "batch_generator.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgMQxpCtDyA-"
   },
   "source": [
    "Once we have defined the model all we have to do for train the model is:\n",
    "\n",
    "1.   model.compile()\n",
    "2.   model.fit()\n",
    "\n",
    "We should know what goes on behind the scenes. The steps involved in training a model are as follows:\n",
    "\n",
    "*Training steps:*\n",
    "\n",
    "1. Compute the predictions using current weights (Forward Pass).\n",
    "2. Compute the loss value for these predictions.\n",
    "3. Compute the gradient with regard to model weights.\n",
    "4. Update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qa-TTuNNFbjX"
   },
   "outputs": [],
   "source": [
    "# One_training_step() function gives the idea of how loss is computed and\n",
    "# Layer parameters (weights and biases) are updated\n",
    "\n",
    "def one_training_step(model, images_batch, labels_batch):\n",
    "    with tf.GradientTape() as tape:                   # GradientTape() is the computational graph\n",
    "        predictions = model(images_batch)               # Forward pass.\n",
    "        per_sample_losses = keras.losses.sparse_categorical_crossentropy(labels_batch, predictions)       # Define loss\n",
    "\n",
    "        average_loss = tf.reduce_mean(per_sample_losses)\n",
    "    gradients = tape.gradient(average_loss, model.weights)      # Compute gradients\n",
    "    update_weights(gradients, model.weights)                    # Update the weights\n",
    "    return average_loss\n",
    "\n",
    "learning_rate = 1e-3\n",
    "def update_weights(gradients, weights):\n",
    "    for g,w in zip(gradients, weights):\n",
    "        w.assign_sub(g*learning_rate)             # w -= g*lr ; w = w - lr*g\n",
    "\n",
    "loss_observed = []\n",
    "\n",
    "# Full training loop\n",
    "def fit(model, images, labels, epochs, batch_size=128):\n",
    "    for epoch in range(epochs):                       # Repeat for epochs\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        batch_generator = BatchGenerator(images, labels)\n",
    "        for batch_counter in range(batch_generator.num_batches):      # Go through all mini-batches in the data\n",
    "            images_batch, labels_batch = batch_generator.next()\n",
    "            loss = one_training_step(model, images_batch, labels_batch)\n",
    "            if batch_counter%100 == 0:\n",
    "                print(f\"    loss at batch {batch_counter:<3} : {loss:.2f}\")\n",
    "        loss_observed.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fi3VZPlPg1KQ"
   },
   "source": [
    "Train the model on MNIST data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_NZfSlyI3BK"
   },
   "outputs": [],
   "source": [
    "fit(model, train_images, train_labels, epochs=10, batch_size=128)\n",
    "\n",
    "# Q: We didn't do a compile step.... or did we?\n",
    "# A: In model.compile(), we pass information about the loss function, optimizer, and evaluation metric.\n",
    "#     In our naive implementation, instead of defining a separate compile() function, we have defined the loss\n",
    "#     inside one_training_step; implemented the optimizer 'mini-batch gradient descent' in update_weights();\n",
    "#     and we are doing the evaluation (accuracy) separately in a later cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrUYaWqILrzd"
   },
   "outputs": [],
   "source": [
    "# Plot the loss observed during training\n",
    "\n",
    "plt.plot(range(1,11), loss_observed, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xr-wTuwUwntW"
   },
   "source": [
    "After 10 epochs the loss has come down.\n",
    "\n",
    "The model has definitely learned something. Lets evaluate how accurately it can predict labels for images **it has not seen before**. These are the **images in the test set**.\n",
    "\n",
    "Use the \"accuracy\" metric. Here we simply find the fraction of times the model succeeded in predicting the correct label.\n",
    "\n",
    "When using Keras, we would mention this metric in `model.compile()`. (More on Keras later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0r_RULJJazg"
   },
   "outputs": [],
   "source": [
    "#  Evaluation step\n",
    "predictions = model(test_images).numpy()                # Gives probabilities of class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)       # Selecting class label which has highest probability value\n",
    "\n",
    "matches = predicted_labels == test_labels               # Check how many are correctly predicted\n",
    "\n",
    "print(f\"accuracy: {matches.mean():.2f}\")\n",
    "print(predictions.shape)\n",
    "print(predicted_labels)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dswgtU2CzI7S"
   },
   "source": [
    "## Section - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cslrsAKOJNQz"
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "## **Different APIs**\n",
    "1. Sequential Model\n",
    "2. Functional API\n",
    "3. Model subclassing\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aE0TBr1TeyAz"
   },
   "source": [
    "### Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9gpkdSyhHpS"
   },
   "source": [
    "Whatever has been implemented so far can be done alternatively using the Sequential class in keras. In the following approach, layers are passed as a list.\n",
    "\n",
    "Defining the same old model by subclassing the Model class [[Reference](https://keras.io/api/models/model/)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruJTcQTYJMdc"
   },
   "outputs": [],
   "source": [
    "# from keras.layers import Dense, Flatten\n",
    "# from keras import Input\n",
    "\n",
    "seq_model = keras.Sequential([\n",
    "                         Dense(64, activation=\"relu\"),\n",
    "                         Dense(10, activation=\"softmax\")\n",
    "])\n",
    "# Q: Do you notice a difference in arguments of the Dense layers, compared to our implementation?\n",
    "# A: We did not have write input_shape explicitly for each layer. It automatically inferred by Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQATPhMCbH7m"
   },
   "source": [
    "Alternatively, instead of passing layers as list, we can build a sequential model by adding layers incrementally to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUvX7v8za9wp"
   },
   "outputs": [],
   "source": [
    "# Initialize a Sequential model - a linear stack of layers\n",
    "seq_model_inc = keras.Sequential()\n",
    "\n",
    "# Add a dense layer with 64 neurons and ReLU activation function\n",
    "# This hidden layer learns patterns from the input data\n",
    "seq_model_inc.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "# Add a dense output layer with 10 neurons and softmax activation function\n",
    "# This layer will classify the input data into one of 10 classes (for multi-class classification)\n",
    "seq_model_inc.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMRVr06003Wc"
   },
   "source": [
    "Notice that we have not yet provided information of input dimensions.\n",
    "\n",
    "These layers are referred to as symbolic layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BI55yU8yiyC4"
   },
   "source": [
    "The model's layer weights are not created until the model is built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnx4mD2Caz3g"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    seq_model_inc.weights\n",
    "except:\n",
    "    print(\"seq_model_inc.weights did not work because model was not built and weights were not initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LA3izeoUjhPM"
   },
   "source": [
    "To create a weights you need to call on some data or call its build method with input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ptmky2Mtce-b"
   },
   "outputs": [],
   "source": [
    "seq_model_inc.build(input_shape=(None, 3))  # None means it can take any batch size; 3 is the number of features in your input\n",
    "seq_model.build(input_shape=(None, 3))\n",
    "\n",
    "# seq_model_inc.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkiK58yWc8C0"
   },
   "outputs": [],
   "source": [
    "seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wThaAytu1-Od"
   },
   "source": [
    "Q: Verify the number of parameters by a quick calculation?\n",
    "\n",
    "A: 650 = 10*64 + 10\n",
    "\n",
    "weight matrix has 64*10 weights and 10 biases for the 10 neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxvhrpKMdhN-"
   },
   "source": [
    "**Specifying input shape in advance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgLTvjUOczAr"
   },
   "outputs": [],
   "source": [
    "model_seq = keras.Sequential(name=\"sequential_model\")\n",
    "model_seq.add(keras.Input(shape=(3, )))   #specifying the input here\n",
    "model_seq.add(keras.layers.Dense(64, activation=tf.nn.relu, name=\"first_layer\"))\n",
    "model_seq.add(keras.layers.Dense(10, activation=tf.nn.softmax, name=\"second_layer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IY6_BFbzeYl-"
   },
   "outputs": [],
   "source": [
    "model_seq.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9C_bajdesXG"
   },
   "source": [
    "### 2. Functional API\n",
    "\n",
    "We will use the Keras functional API to create the same model. Keras functional API can create more flexible models than Sequential API. It can handle models with non-linear topology, shared layers, and even multiple inputs or outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76zkbaPmeglO"
   },
   "source": [
    "Key Idea- Expresses each layer as a function of the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWdrmokoqly3"
   },
   "source": [
    "<center>(input: 3-dimensional vectors)</center>\n",
    "<center>  ↧ </center>\n",
    "<center>[Dense (64 units, relu activation)]</center>\n",
    "<center>   ↧ </center>\n",
    "<center>(output: 10 units, softmax activation)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJMD5LIDP6SG"
   },
   "source": [
    "Defining the same old model by subclassing the Model class [[Reference](https://keras.io/api/models/model/)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7Rh4xmEedE_"
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(3,), name=\"input_layer\")\n",
    "features = Dense(64, activation=\"relu\",name=\"first_layer\")(inputs)          #f(inputs)\n",
    "outputs = Dense(10, activation=\"softmax\", name=\"output_layer\")(features)    #f(features)\n",
    "fun_model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GHUzMms-fgzL"
   },
   "outputs": [],
   "source": [
    "fun_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNwLmg_93mFq"
   },
   "source": [
    "We get effectively the same summary becuase we have implemented the same model using the functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjHyawfqfi-2"
   },
   "outputs": [],
   "source": [
    "plot_model(fun_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPzngzNM4epF"
   },
   "source": [
    "A deeper network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-xXU4rbSGqn"
   },
   "outputs": [],
   "source": [
    "# from keras.layers import Dense\n",
    "# import keras\n",
    "# node = Layer(nodes, extra_params)(prev_node)\n",
    "inputs = keras.Input(shape=(64,))\n",
    "dense1 = Dense(32, activation='relu')(inputs)\n",
    "dense2 = Dense(32, activation='relu')(dense1) # Defining dense2 node whose parent is dense1\n",
    "outputs = Dense(4, activation='softmax')(dense2) # Defining output node where parent is dense2\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"linear_topology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5fzZssHSsjY"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVAyv-0Tfp96"
   },
   "outputs": [],
   "source": [
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GE-NpwSNlzp"
   },
   "source": [
    "An example where the Sequential API would not be sufficeint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbFGMtx2gGcX"
   },
   "source": [
    "**Multi-Input and Multi-output:** Consider an example of building a system to rank customer tickets by priority and route them to the appropriate departments.\n",
    "\n",
    "Outputs: model need to give two outputs\n",
    "1. First task of the model is to classify the tickets into priority and non priority (Binary classification)\n",
    "\n",
    "2. Second task is to route the ticket to appropriate department (Multi-class classification based on the number of departments)\n",
    "\n",
    "These two task are to be done simultaneously\n",
    "\n",
    "Inputs:\n",
    "1. Title of the ticket (text input)\n",
    "2. The text body of the ticket (text input)\n",
    "3. Any tags added by the user\n",
    "\n",
    "Q. Is it possible to build the model sequentially?\n",
    "\n",
    "A: No, we cannot build a multi-input , multi-output model through the sequential API, because, by definition itself, the required model is not sequential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRnAPB7Ff2tV"
   },
   "outputs": [],
   "source": [
    "# Define the size of the input features and the number of classes for outputs\n",
    "vocabulary_size = 10000   # Number of unique words/tokens in the vocabulary\n",
    "num_tags = 100            # Number of unique tags\n",
    "num_departments = 4       # Number of departments (classification output)\n",
    "\n",
    "# Inputs\n",
    "title = keras.Input(shape=(vocabulary_size,), name=\"title\")  # Input for the title with a shape based on vocabulary size\n",
    "text_body = keras.Input(shape=(vocabulary_size,), name=\"text_body\")  # Input for the text body\n",
    "tags = keras.Input(shape=(num_tags,), name=\"tags\")  # Input for the tags (binary vector with num_tags size)\n",
    "\n",
    "# Concatenate the inputs into a single layer to combine the features\n",
    "features = keras.layers.Concatenate()([title, text_body, tags])  # Merging title, text body, and tags into one feature layer\n",
    "features = keras.layers.Dense(64, activation=\"relu\")(features)   # Applying a Dense layer with 64 units and ReLU activation\n",
    "\n",
    "# Outputs\n",
    "priority = keras.layers.Dense(1, activation=\"sigmoid\", name=\"priority\")(features)  # Output layer for binary classification (priority)\n",
    "\n",
    "department = keras.layers.Dense(num_departments, activation=\"softmax\", name=\"department\")(features)  # Output layer for department classification (softmax for multi-class)\n",
    "\n",
    "# Define the model with multiple inputs and multiple outputs\n",
    "model = keras.Model(inputs=[title, text_body, tags],\n",
    "                    outputs=[priority, department])  # Model with inputs as title, text body, and tags; outputs as priority and department\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNVaw-romnCB"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gzos2b6Nm1sg"
   },
   "outputs": [],
   "source": [
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhkyvvNkki2Q"
   },
   "source": [
    "Reusing the model by training intermediate layer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vm0jZItWk6m9"
   },
   "outputs": [],
   "source": [
    "# Extract the output of the 4th layer of the model (the features from a previous layer in the model)\n",
    "features = model.layers[4].output  # The output from the 4th layer of the model, which will be used as input for the next layer\n",
    "\n",
    "# Add a Dense layer to predict 'difficulty' with 3 possible classes (softmax for multi-class classification)\n",
    "difficulty = keras.layers.Dense(3, activation=\"softmax\", name=\"difficulty\")(features)  # A Dense layer with 3 units and softmax activation for difficulty classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r5B9Zh8XlnXA"
   },
   "outputs": [],
   "source": [
    "# Create a new model by specifying the inputs and outputs\n",
    "\n",
    "new_model = keras.Model(inputs=[title, text_body, tags],      # The inputs for the new model: title, text_body, and tags\n",
    "                        outputs=[priority, department, difficulty])  # The outputs for the new model: priority, department, and difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lxvxqOEl7sf"
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-bQlSKbDmGQn"
   },
   "source": [
    "### 3. Subclassing the Model class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZH5CS5eIKZWF"
   },
   "source": [
    "We saw how the functional API enabled us to make more complex models compared to the sequential API. We moved up the ladder of progressive disclosure of complexity.\n",
    "\n",
    "\n",
    "Defining the same old model by subclassing the Model class [[Reference](https://keras.io/api/models/model/)].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ks_pwLIQmHCG"
   },
   "outputs": [],
   "source": [
    "class CustomerTicketModel(keras.Model):\n",
    "\n",
    "# Define the layers in the __init__ method\n",
    "    def __init__(self, num_departments):\n",
    "        super().__init__()\n",
    "        self.concat_layer = keras.layers.Concatenate()\n",
    "        self.mixing_layer = keras.layers.Dense(64, activation=\"relu\")\n",
    "        self.priority_scorer = keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "        self.department_classifier = keras.layers.Dense(num_departments, activation=\"softmax\")\n",
    "\n",
    "\n",
    "# Define the relationship between layers in the call method\n",
    "# See Section 7.2.3 in Francois chollet for more details\n",
    "# You implement custom layers by writing a call method.\n",
    "    def call(self, inputs):\n",
    "        # input should be dictionary type\n",
    "        title = inputs[\"title\"]\n",
    "        text_body = inputs[\"text_body\"]\n",
    "        tags = inputs[\"tags\"]\n",
    "\n",
    "        features = self.concat_layer([title, text_body, tags])\n",
    "        features = self.mixing_layer(features)\n",
    "\n",
    "        priority = self.priority_scorer(features)\n",
    "        department = self.department_classifier(features)\n",
    "\n",
    "        return priority, department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jlsAUwhEmBVH"
   },
   "outputs": [],
   "source": [
    "sub_class_model = CustomerTicketModel(num_departments=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEBHPhccsVHz"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sub_class_model.summary()\n",
    "except:\n",
    "    print(\"summary() did not work because we have not built the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_6VNQK89erZ"
   },
   "outputs": [],
   "source": [
    "# here model is built by calling the data since build() method is not\n",
    "# defined in model subclass\n",
    "# generate random data\n",
    "title_data = np.random.randint(0, 2, size=(1000,vocabulary_size))\n",
    "text_body_data = np.random.randint(0, 2, size=(1000,vocabulary_size))\n",
    "tags_data = np.random.randint(0, 2, size=(1000,num_tags))\n",
    "\n",
    "priority, department = sub_class_model({\"title\":title_data,\n",
    "                                        \"text_body\":text_body_data,\n",
    "                                        \"tags\":tags_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggm2YttTTXIq"
   },
   "outputs": [],
   "source": [
    "sub_class_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zacePgu0KSv"
   },
   "source": [
    "## Building the model using custom dense layer and functional API\n",
    "\n",
    "Building custom layer [[Reference](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hfijADSlWyd"
   },
   "outputs": [],
   "source": [
    "from keras.initializers import RandomNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_eC0PSoOhJF"
   },
   "outputs": [],
   "source": [
    "class Custom_Dense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None):\n",
    "        super() .__init__()\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "    # Subclassing gives us the flexibility here to initialize weights on our own\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        std_dev = np.sqrt(2/(input_dim + self.units))\n",
    "        self.W = self.add_weight(shape=(input_dim, self.units),\n",
    "                                initializer=RandomNormal(stddev=std_dev))\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                    initializer=\"zeros\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y = tf.matmul(inputs, self.W) + self.b\n",
    "        if self.activation is not None:\n",
    "            y = self.activation(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfOyUTR8hxWr"
   },
   "source": [
    "We can even define custom metrics and custom loss functions using the subclassing API. Refer to Section 7.3.1 of Chollet for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yniIioTy3kGx"
   },
   "source": [
    "### Using custom dense layer with functional API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAuxXHxh1CNt"
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(28*28,))\n",
    "features = Custom_Dense(512, activation=tf.nn.relu)(inputs)\n",
    "features = Custom_Dense(128, activation=tf.nn.relu)(features)\n",
    "outputs = Custom_Dense(10, activation=tf.nn.softmax)(features)\n",
    "\n",
    "model = keras.Model(inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUKJ4z3A1sJY"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SP_wZ5iU-pBx"
   },
   "outputs": [],
   "source": [
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DFEbVhX1zJa"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer =keras.optimizers.RMSprop(),\n",
    "              loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhVlMHxH4GxT"
   },
   "outputs": [],
   "source": [
    "train_x = train_images[10000:]\n",
    "train_y = train_labels[10000:]\n",
    "val_x = train_images[:10000]\n",
    "val_y = train_labels[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWqYVq4LhnAa"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=train_x, y=train_y, epochs=10,\n",
    "                    validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6QIIAEVBlA_"
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(history.history)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgwaEhddCId2"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1,11),data['loss'], label=\"Training Loss\")\n",
    "plt.plot(range(1,11),data['val_loss'],label=\"validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oeyc7A--4tK"
   },
   "source": [
    "Q. What is the difference between evaluate() and predict()\n",
    "\n",
    "A: evaluate() returns the loss score and evaluation score. predict() runs a forward pass for the given input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4FdxPG4a3Zn"
   },
   "source": [
    "\"predict\" is used to make predictions on new data using a trained model. Given an input tensor, the \"predict\" function outputs the corresponding predictions generated by the model.\n",
    "\n",
    "\"evaluate\" is used to evaluate the performance of a trained model on a given dataset. Given an input dataset, the \"evaluate\" function computes the model's performance metrics, such as accuracy, loss, or any other metrics defined during model compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEY25GH4CgsE"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41qJqZfgGCaS"
   },
   "outputs": [],
   "source": [
    "class_predicted = np.argmax(model.predict(test_images), axis=1)\n",
    "accuracy = np.sum(class_predicted == test_labels)/len(test_labels)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKb-DqiGDfdT"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mogsqOiDoxt"
   },
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, class_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lmqB_9W864r"
   },
   "outputs": [],
   "source": [
    "pd.Series(test_labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ReQMaCbZ-CRI"
   },
   "outputs": [],
   "source": [
    "# Given values for label 0\n",
    "TP = 974  # True Positives\n",
    "FP = 15   # False Positives\n",
    "FN = 6    # False Negatives\n",
    "\n",
    "# Calculating precision, recall, and F1 score\n",
    "precision = TP / (TP + FP)  # Precision calculation\n",
    "recall = TP / (TP + FN)     # Recall calculation\n",
    "f1 = 2 * precision * recall / (precision + recall)  # F1 score calculation\n",
    "\n",
    "precision, recall, f1  # Output the precision, recall, and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-aAMoqq9DyiW"
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(test_labels, class_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7APwypLtAJq0"
   },
   "outputs": [],
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0P6lYTZeaqH"
   },
   "source": [
    "## Using Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ollrAr-MxRnW"
   },
   "source": [
    "A callback is an object that can perform actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch, etc).\n",
    "\n",
    "You can use callbacks to:\n",
    "\n",
    "* Write TensorBoard logs after every batch of training to monitor your metrics\n",
    "* Periodically save your model to disk\n",
    "* Do early stopping\n",
    "* Get a view on internal states and statistics of a model during training\n",
    "\n",
    "Access Keras callbacks [here](https://keras.io/api/callbacks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFIa20V_QsG8"
   },
   "outputs": [],
   "source": [
    "# Build model using functional API\n",
    "inputs = Input(shape=(28*28,))\n",
    "features = Dense(512,activation=\"relu\")(inputs)\n",
    "features = keras.layers.Dropout(0.5)(features)\n",
    "outputs = Dense(10,activation=\"softmax\")(features)\n",
    "\n",
    "mnist_model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9Eky3r5ewYC"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5ZeDDy-84iH"
   },
   "outputs": [],
   "source": [
    "callbacks_list = [EarlyStopping(monitor=\"val_loss\", patience=2),\n",
    "                  # Added .keras extension to the filepath for saving the entire model\n",
    "                  ModelCheckpoint(\"mnist_model_checkpoint.keras\", save_best_only=True),\n",
    "                  TensorBoard(log_dir=\"./tensorboard_files\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKEhf3ZxQsLn"
   },
   "outputs": [],
   "source": [
    "mnist_model.compile(optimizer =keras.optimizers.Adam(),\n",
    "                    loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "                    metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgQoB39dQsNy"
   },
   "outputs": [],
   "source": [
    "mnist_model.fit(x= train_x, y= train_y,\n",
    "                epochs= 5,\n",
    "                validation_data= (val_x, val_y),\n",
    "                callbacks= callbacks_list,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JakR-KExx7RW"
   },
   "source": [
    "###TensorBoard\n",
    "\n",
    "It is a visualization tool provided with TensorFlow.\n",
    "\n",
    "This callback logs events for TensorBoard, including:\n",
    "\n",
    "* Metrics summary plots\n",
    "* Training graph visualization\n",
    "* Weight histograms\n",
    "* Sampled profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sug8-ZUzHDNX"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMMokLDMHgMR"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=tensorboard_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V92U6cInMvDz"
   },
   "source": [
    "**Reference**\n",
    "\n",
    "\n",
    "*   Chollet, F. (2021). Deep learning with python. Manning Publications.\n",
    "*   Geron,Aurelien(2022): Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, O'Reilly Media, Inc. Publications\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
