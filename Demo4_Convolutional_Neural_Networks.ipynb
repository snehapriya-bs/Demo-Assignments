{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-1SXdIFtYAY"
   },
   "source": [
    "## Learning Objectives:\n",
    "\n",
    "At the end of the experiment, you will be able to:\n",
    "\n",
    "1. Understand Conv2D and MaxPooling layers used in ConVNet\n",
    "2. Build a simple ConvNet for image classification using the digit MNIST dataset\n",
    "3. Build another  ConvNet for  image Classification using the cats-and-dogs dataset\n",
    "4. Apply Data Augmentation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUgk-iFZtjTA"
   },
   "source": [
    "## 1. Building a simple CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyPJgDl1mFWD"
   },
   "source": [
    "Let's start with a simple example. We will\n",
    "1. Build a CNN with convolution and pooling layers\n",
    "2. Train it on the MNIST dataset\n",
    "\n",
    "The figure below is a typical ConvNet (LeNet) architecture that we are going to build but with different numbers and sizes of filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFRlXH4XvD28"
   },
   "source": [
    "![picture](https://drive.google.com/uc?export=view&id=1vT8e59AYTFRlrrI3C-iUHTctxyhfBiJJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNLA8HiKxQhc"
   },
   "source": [
    "### Setup Steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IqJs6tAUX10"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6eMODYdFtKeR"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDEevleAzCi4"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCr4POjiAI4L"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "print(f\"train_images.shape = {train_images.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XPQ7HdyzJTq"
   },
   "source": [
    "## Reshape and convert to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vWWJnvaAPOY"
   },
   "outputs": [],
   "source": [
    "# Reshape and convert to float\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "print(f\"train_images.shape = {train_images.shape}\")\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRqPDv_UvQHl"
   },
   "source": [
    "### Building the architecture\n",
    "LeNet-5: Example of an early ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCD1Mta9twxU"
   },
   "outputs": [],
   "source": [
    "# Input layer: Defines the input shape for the model. It expects a 28x28 pixel image with 1 channel (grayscale).\n",
    "inputs = keras.Input(shape=(28, 28, 1))\n",
    "\n",
    "# First convolutional layer: Applies 32 filters of size 3x3 with ReLU activation function to extract features from the input.\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
    "\n",
    "# First max-pooling layer: Reduces the spatial dimensions of the feature map by taking the maximum value over a 2x2 window.\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "# Second convolutional layer: Applies 64 filters of size 3x3 with ReLU activation to further extract features.\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "\n",
    "# Second max-pooling layer: Reduces the spatial dimensions further with a 2x2 window.\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "# Third convolutional layer: Applies 128 filters of size 3x3 with ReLU activation to extract deeper features.\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "\n",
    "# Flatten layer: Flattens the 3D output of the convolutional layers into a 1D vector for the fully connected layer.\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Output layer: A dense layer with 10 units (for 10 classes) and softmax activation function to output probability distribution over the 10 classes.\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "# Define the model: Specifies the input and output layers for the model.\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KEJymG4Wty8d"
   },
   "outputs": [],
   "source": [
    "model.summary()\n",
    "# Q: Verify no. of params in 1st conv layer # A: (3X3X1 + 1) X 32 ; 32 filters, 9 weights per kernel, 1 bias ber kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yrrx-MY0ASp6"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",     # Q: Why sparse_cat_crossent?  A: labels are not one-hot-encoded\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAY7NwrZM81q"
   },
   "source": [
    "#### Call Back Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRjznrJmvYwE"
   },
   "outputs": [],
   "source": [
    "# Define a function to return a commonly used list of Keras callbacks\n",
    "def def_callbacks(filepath, mod_chk_mon=\"val_loss\", earlystop=0):\n",
    "    # Initialize an empty list to store the callbacks\n",
    "    callback_list = []\n",
    "\n",
    "    # Default callback: ModelCheckpoint\n",
    "    # This callback saves the model's weights at the specified `filepath` whenever there is an improvement\n",
    "    # in the monitored metric (by default, \"val_loss\").\n",
    "    # - `save_best_only=True` ensures that only the model with the best performance is saved.\n",
    "    # - `monitor=mod_chk_mon` specifies the metric to monitor (e.g., validation loss).\n",
    "    callback_list.append(keras.callbacks.ModelCheckpoint(\n",
    "        filepath + \".keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=mod_chk_mon\n",
    "    ))\n",
    "\n",
    "    # Optional callback: EarlyStopping\n",
    "    # If `earlystop` is greater than 0, add an EarlyStopping callback to the list.\n",
    "    # This stops training when the monitored metric stops improving for a specified number of epochs (patience).\n",
    "    # - `patience=earlystop` sets the number of epochs to wait before stopping once improvement stalls.\n",
    "    if earlystop > 0:\n",
    "        callback_list.append(keras.callbacks.EarlyStopping(patience=earlystop))\n",
    "\n",
    "    # Return the list of callbacks\n",
    "    return callback_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PCMqHhuNSTL"
   },
   "source": [
    "### Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ai9XQpACty-j"
   },
   "outputs": [],
   "source": [
    "# Set PARTIAL_RUN flag to indicate a shorter training run for testing purposes\n",
    "PARTIAL_RUN = False\n",
    "\n",
    "# Set the default number of epochs for full training\n",
    "epochs = 10\n",
    "\n",
    "# If PARTIAL_RUN is True, reduce the number of epochs for a quick run\n",
    "if PARTIAL_RUN:\n",
    "    epochs = 2  # Run a shorter training session (2 epochs)\n",
    "\n",
    "# Train the model\n",
    "# - `train_images` and `train_labels` are the training data and labels.\n",
    "# - `epochs=epochs` uses the determined number of epochs (2 if PARTIAL_RUN is True, otherwise 10).\n",
    "# - `validation_split=0.2` sets aside 20% of the training data for validation.\n",
    "# - `batch_size=64` defines the batch size for gradient updates during training.\n",
    "# - `callbacks=def_callbacks(\"prob1\")` uses the `def_callbacks` function to set up model checkpoints and (optionally) early stopping.\n",
    "model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    batch_size=64,\n",
    "    callbacks=def_callbacks(\"prob1\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mGU00hTzSwi"
   },
   "source": [
    "### Prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlntfyEptgt7"
   },
   "outputs": [],
   "source": [
    "# Evaluate test accuracy\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)  # Q: Which state is this model at? A: Slightly overfit, trained till 10th epoch\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhYHKrIdp6uV"
   },
   "source": [
    "Nearly **99%** accurate! This is much better than what we achieved with our feedforward network with only dense layers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5yJg1wnqgKQ"
   },
   "source": [
    "### Now, let's see the importance of pooling layers.\n",
    "\n",
    "We will make a new model called 'model_no_max_pool' without any pooling layers and compare it with the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6z0UF_TvBSr"
   },
   "outputs": [],
   "source": [
    "# Define the input layer with a shape of 28x28 pixels and 1 color channel (grayscale image)\n",
    "inputs = keras.Input(shape=(28, 28, 1))\n",
    "\n",
    "# First convolutional layer\n",
    "# - 32 filters, each of size 3x3\n",
    "# - ReLU activation function for non-linearity\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
    "\n",
    "# Second convolutional layer\n",
    "# - 64 filters, each of size 3x3\n",
    "# - ReLU activation function\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "\n",
    "# Third convolutional layer\n",
    "# - 128 filters, each of size 3x3\n",
    "# - ReLU activation function\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "\n",
    "# Flatten the output of the last convolutional layer\n",
    "# This converts the 3D feature map into a 1D vector to be fed into the dense layer\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Output layer\n",
    "# - 10 units for 10 classes (e.g., digits 0–9 in a digit classification task)\n",
    "# - Softmax activation function for multi-class classification\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "# Create the model\n",
    "model_no_max_pool = keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GaKi96I7va-d"
   },
   "outputs": [],
   "source": [
    "model_no_max_pool.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jj1tUASrPs4"
   },
   "source": [
    "#### Observations from models with and without pooling layers\n",
    "1. The initial model with pooling layers had just 104,202 parameters but the model without pooling layers (model_no_max_pool) has 712,202 parameters i.e. increase in the number of trainable parameters. **Model with pooling layer is less prone to overfitting** due to a smaller number of parameters/weights.\n",
    "\n",
    "2. What other advantage does the pooing layer provide? **They facilitate learning a spatial hierarchy of features**.\n",
    "\n",
    "  In the CNN given below. Imagine a 1x1 patch on a C3 feature map. It contains information from 6x6 window of the input layer. On the other hand, if no pooling layers are present, then it would contain information from a 3x3 window in the input layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_O5B2701vd44"
   },
   "source": [
    "![picture](https://drive.google.com/uc?export=view&id=1vT8e59AYTFRlrrI3C-iUHTctxyhfBiJJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Cis1mDCvutX"
   },
   "source": [
    "**Optional Exercise:** Train the model_no_max_pool\n",
    " with the MNIST data set and compare its accuracy with the first model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6LzAYhovfuC"
   },
   "source": [
    "**In class exercise:** Let's try to build a LeNet-5 architecure as given in above diagram right now:\n",
    "\n",
    "\n",
    "* 1st Conv and 2nd Conv layers have a 3x3 & 5x5 kernel respectively\n",
    "* Pooling layers have 2x2 kernel\n",
    "* All activations as 'relu' except for last\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NhASzPTavccH"
   },
   "outputs": [],
   "source": [
    "# Define the input layer with a shape of 28x28 pixels and 1 color channel (for grayscale images)\n",
    "inputs = keras.Input(shape=(28, 28, 1))\n",
    "\n",
    "# First convolutional layer\n",
    "# - Applies 6 filters, each with a size of 3x3\n",
    "# - Uses 'same' padding to maintain the spatial dimensions (28x28) after convolution\n",
    "# - ReLU activation introduces non-linearity\n",
    "x = layers.Conv2D(filters=6, kernel_size=3, padding='same', activation='relu')(inputs)\n",
    "\n",
    "# First pooling layer\n",
    "# - Applies max pooling with a pool size of 2x2, reducing spatial dimensions by half (from 28x28 to 14x14)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "# Second convolutional layer\n",
    "# - Applies 16 filters, each with a size of 5x5\n",
    "# - 'Valid' padding, meaning no padding is added, so the spatial dimensions will reduce (from 14x14 to 10x10)\n",
    "# - ReLU activation for non-linearity\n",
    "x = layers.Conv2D(filters=16, kernel_size=5, padding='valid', activation='relu')(x)\n",
    "\n",
    "# Second pooling layer\n",
    "# - Max pooling with a 2x2 pool size, further reducing spatial dimensions by half (from 10x10 to 5x5)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "# Flatten layer\n",
    "# - Flattens the 3D output into a 1D vector to prepare for fully connected layers\n",
    "# - Output shape will be 16 * 5 * 5 = 400\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# First dense (fully connected) layer\n",
    "# - 120 neurons with ReLU activation\n",
    "x = layers.Dense(120, activation='relu')(x)\n",
    "\n",
    "# Second dense layer\n",
    "# - 84 neurons with ReLU activation\n",
    "x = layers.Dense(84, activation='relu')(x)\n",
    "\n",
    "# Output layer\n",
    "# - 10 neurons with softmax activation to produce a probability distribution over 10 classes (e.g., for digit classification)\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Print a summary of the model's architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syM6r3KAwLRb"
   },
   "source": [
    "## 2. Image Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrqghDRkvDAh"
   },
   "source": [
    "Now, we know how to build a simple CNN, let's build and train one to solve an image classification problem.\n",
    "\n",
    "We will work with the cats-vs-dogs dataset to classify whether a given image is that of a cat or a dog .i.e a  binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLFh-pFLUdpK"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XelqVXc1wRv8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCNRgUsjdd6U"
   },
   "source": [
    "We have already uploaded the dataset into structured folders. You simply need to download it from our repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUfhxMyOkrA2"
   },
   "outputs": [],
   "source": [
    "# Base directory where the main dataset folder is located\n",
    "data_dir = '/content/cats_vs_dogs_small'\n",
    "\n",
    "# Path to the training data directory\n",
    "train_path = data_dir + '/train'\n",
    "\n",
    "# Path to the validation data directory\n",
    "validation_path = data_dir + '/validation'\n",
    "\n",
    "# Path to the test data directory\n",
    "test_path = data_dir + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V71xYZDRlL_E"
   },
   "outputs": [],
   "source": [
    "# Define the path to a sample dog image and display its shape\n",
    "dog_image = train_path + '/dog/' + 'dog.443.jpg'\n",
    "print(\"Shape of the dog image is:\", imread(dog_image).shape)\n",
    "\n",
    "# Display the sample dog image\n",
    "plt.imshow(imread(dog_image))\n",
    "\n",
    "# Initialize lists to store the dimensions of each dog image\n",
    "dim1 = []\n",
    "dim2 = []\n",
    "\n",
    "# Loop over all dog images in the training directory\n",
    "for image_file in os.listdir(train_path + '/dog'):\n",
    "    # Read each image in the dog training folder\n",
    "    img = imread(train_path + '/dog/' + image_file)\n",
    "    # Get the height (d1), width (d2), and color channels of the image\n",
    "    d1, d2, colour_channels = img.shape\n",
    "    # Append the dimensions to respective lists\n",
    "    dim1.append(d1)\n",
    "    dim2.append(d2)\n",
    "\n",
    "# Plot the distribution of image dimensions\n",
    "sns.jointplot(x=dim1, y=dim2)\n",
    "print(\"Mean across height of all dog images in train set is:\", np.mean(dim1))\n",
    "print(\"Mean across width of all dog images in train set is:\", np.mean(dim2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3FyIB1gyqXg"
   },
   "source": [
    "### Converting the image dataset into a workable format\n",
    "\n",
    "We have the images in folders. We need to make it into a workable dataset:\n",
    "  * Which has labels\n",
    "  * All the images have the same size\n",
    "\n",
    "For this, we will use the utility [**image_dataset_from_directory**](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory).\n",
    "\n",
    "Calling image_dataset_from_directory(main_directory, labels='inferred') will return a tf.data.Dataset that yields batches of images from the subdirectories class_a and class_b, together with labels 0 and 1 (0 corresponding to class_a and 1 corresponding to class_b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVOWgUhECt5d"
   },
   "outputs": [],
   "source": [
    "# Load and prepare the training dataset\n",
    "# - `train_path`: Directory path for training images\n",
    "# - `image_size=(180, 180)`: Resize all images to 180x180 pixels to ensure uniform input dimensions\n",
    "# - `batch_size=32`: Load images in batches of 32, which helps in efficient training\n",
    "train_dataset = image_dataset_from_directory(\n",
    "               train_path,\n",
    "               image_size=(180, 180),  # Resize the images to (180,180)\n",
    "               batch_size=32)\n",
    "\n",
    "# Load and prepare the validation dataset\n",
    "# - `validation_path`: Directory path for validation images\n",
    "# - `image_size=(180, 180)`: Resize validation images to 180x180 pixels, matching the input size expected by the model\n",
    "# - `batch_size=32`: Batch size of 32 for validation to save memory and maintain consistency with training batch size\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "                      validation_path,\n",
    "                      image_size=(180, 180),\n",
    "                      batch_size=32)\n",
    "\n",
    "# Load and prepare the test dataset\n",
    "# - `test_path`: Directory path for test images\n",
    "# - `image_size=(180, 180)`: Resize test images to 180x180 pixels, same as training and validation sizes\n",
    "# - `batch_size=32`: Batch size of 32 for the test set for efficient processing\n",
    "test_dataset = image_dataset_from_directory(\n",
    "                test_path,\n",
    "                image_size=(180, 180),\n",
    "                batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_UlcX7j7c8J"
   },
   "outputs": [],
   "source": [
    "print(f\"train_dataset = {train_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISmvr0Zx2-fP"
   },
   "outputs": [],
   "source": [
    "# Iterate over the batches in the training dataset\n",
    "for data_batch, labels_batch in train_dataset:\n",
    "    # Print the shape of a single batch of data (images)\n",
    "    print(\"data batch shape:\", data_batch.shape)\n",
    "\n",
    "    # Print the shape of the corresponding labels batch\n",
    "    print(\"labels batch shape:\", labels_batch.shape)\n",
    "\n",
    "    # Break after the first batch to verify batch size and avoid unnecessary iterations\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iuo-mdPJ1nM8"
   },
   "outputs": [],
   "source": [
    "# Define covnet model\n",
    "# Define the input layer with a shape of (180, 180, 3), where:\n",
    "# 180x180 is the height and width of the input images, and\n",
    "# 3 corresponds to the three color channels (RGB).\n",
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "\n",
    "# Rescale input values to the range [0, 1],\n",
    "# which is a common preprocessing step for images before feeding them into a neural network.\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "\n",
    "# First convolutional layer with 32 filters, 3x3 kernel, and ReLU activation function.\n",
    "# This layer will learn 32 feature maps from the input image.\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "\n",
    "# Max-pooling layer with a pool size of 2, which reduces the spatial dimensions (height and width) of the input.\n",
    "# This helps reduce the computational load and the risk of overfitting.\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "# Second convolutional layer with 64 filters and ReLU activation, similar to the previous one, but now learning more complex features.\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "\n",
    "# Max-pooling layer again to downsample the feature maps.\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "# Third convolutional layer with 128 filters and ReLU activation, learning even higher-level features.\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "\n",
    "# Max-pooling layer to further reduce spatial dimensions.\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "# Fourth convolutional layer with 256 filters, continuing the trend of increasing the number of learned features.\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "\n",
    "# Max-pooling layer to reduce dimensionality.\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "# Fifth convolutional layer with 256 filters and ReLU activation, capturing the most complex features.\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "\n",
    "# Flatten layer to convert the 3D tensor into a 1D vector, which is necessary for feeding the data into a fully connected layer.\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Dense layer with a single output unit and sigmoid activation function for binary classification.\n",
    "# This layer will output a probability value between 0 and 1, representing the probability of the positive class.\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Create the final model by specifying the inputs and outputs.\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2FDkJSc1uTm"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss=\"binary_crossentropy\",   # Q: Why binary_crossentropy? A: Binary classification\n",
    "                      optimizer=\"rmsprop\",\n",
    "                      metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_vp34qM4Jlh"
   },
   "outputs": [],
   "source": [
    "# Flag to control whether to run the full training or a partial run with fewer epochs\n",
    "PARTIAL_RUN = False\n",
    "epochs = 10  # Default number of epochs for training\n",
    "\n",
    "# If PARTIAL_RUN is True, set the number of epochs to 2 for a quicker test run.\n",
    "if PARTIAL_RUN:\n",
    "  epochs = 2\n",
    "\n",
    "# Fit the model using the training dataset and validate on the validation dataset.\n",
    "# `train_dataset` and `validation_dataset` should be previously defined datasets (e.g., tf.data.Dataset objects or Keras ImageDataGenerators).\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,  # The training dataset used to train the model\n",
    "    epochs=epochs,  # Number of times to loop through the entire dataset\n",
    "    validation_data=validation_dataset,  # Dataset used for validating the model's performance during training\n",
    "    callbacks=def_callbacks(\"convnet_from_scratch_keras_model\")  # List of callbacks to be applied during training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Z29iTD0imOa"
   },
   "outputs": [],
   "source": [
    "# Load the previously trained model from the specified file path\n",
    "test_model = keras.models.load_model(\"convnet_from_scratch_keras_model\")\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "# The evaluate() function returns the loss and metrics specified during model compilation (e.g., accuracy).\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "\n",
    "# Print the test accuracy with a precision of 3 decimal places\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFwCvzcJ1JK6"
   },
   "source": [
    "Great! From just **2000** images, our network has learnt to classify images of cats and dogs with an accuracy of apx **70%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQqCNE1H8IRf"
   },
   "source": [
    "## 3. Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMU298TB2XiY"
   },
   "source": [
    "The small dataset can cause a high variance estimation of model performance\n",
    "\n",
    "Q: How to overcome this and get a more robust model?\n",
    "\n",
    "Now, we want to avoid this problem altogether by artificially (and cleverly) producing new data from the already available data.\n",
    "\n",
    "For this, we perform **data augmentation**.\n",
    "\n",
    "Data augmentation is another regularization method. What other methods did we see in the last tutorial?\n",
    "\n",
    "Data augmentation takes the approach of generating more training data from existing training samples by augmenting the samples via a number of random transformations that yield a believable-looking image. Common transformations include:\n",
    "  * Flipping the image\n",
    "  * Rotating the image\n",
    "  * Zooming in/out of the image\n",
    "\n",
    "See some sample images below after augmentation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OANl64VG5vCI"
   },
   "source": [
    "![picture](https://drive.google.com/uc?export=view&id=1HRhsHEHtcVptNVMF1EbCGiZX5XuTdrs5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gRMWTEGhNQk"
   },
   "outputs": [],
   "source": [
    "# Define a function to apply data augmentation transformations\n",
    "def get_data_augmented(flip=\"horizontal\", rotation=0.1, zoom=0.2):\n",
    "    # Create a Sequential model that applies a series of transformations to the input images\n",
    "    data_augmentation = keras.Sequential([\n",
    "        keras.layers.RandomFlip(flip),       # Randomly flip the image horizontally or vertically (default is horizontal)\n",
    "        keras.layers.RandomRotation(rotation),  # Randomly rotate the image by a given factor (0.1 means ±10% of 360 degrees)\n",
    "        keras.layers.RandomZoom(zoom)        # Randomly zoom in or out the image (zoom factor of 0.2 means 20%)\n",
    "    ])\n",
    "    return data_augmentation\n",
    "\n",
    "# Apply the data augmentation function to get the augmentations\n",
    "data_augmentation = get_data_augmented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fftU_EPahV48"
   },
   "outputs": [],
   "source": [
    "# Define input layer: images of shape (180, 180, 3), where 180x180 is the image size, and 3 corresponds to RGB channels\n",
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "\n",
    "# Augmenting data - Apply random transformations to the images\n",
    "# This ensures the network never sees the same data twice during training\n",
    "x = data_augmentation(inputs)\n",
    "\n",
    "# Rescaling: Normalize pixel values to the range [0, 1]\n",
    "x = layers.Rescaling(1./255)(x)\n",
    "\n",
    "# Convolutional Layer 1: 32 filters, 3x3 kernel, ReLU activation function\n",
    "# This layer learns 32 feature maps from the input image.\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)  # Max pooling to downsample the feature map by a factor of 2\n",
    "\n",
    "# Convolutional Layer 2: 64 filters, 3x3 kernel, ReLU activation\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)  # Max pooling to reduce spatial dimensions\n",
    "\n",
    "# Convolutional Layer 3: 128 filters, 3x3 kernel, ReLU activation\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)  # Max pooling again\n",
    "\n",
    "# Convolutional Layer 4: 256 filters, 3x3 kernel, ReLU activation\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)  # Max pooling\n",
    "\n",
    "# Convolutional Layer 5: 256 filters, 3x3 kernel, ReLU activation\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "\n",
    "# Flatten the output from the convolutional layers into a 1D vector\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Dropout layer: This is a **regularization method** that helps prevent overfitting\n",
    "# During training, it randomly drops 50% of the neurons (with probability of 0.5) in this layer\n",
    "# This helps the network generalize better by forcing it to learn more robust features.\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Output layer: A dense layer with a single neuron, sigmoid activation for binary classification\n",
    "# The output is a probability value between 0 and 1\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Define the complete model\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model with binary crossentropy loss (for binary classification),\n",
    "# RMSprop optimizer, and accuracy as a metric\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHYpxJ5IhdFj"
   },
   "outputs": [],
   "source": [
    "# Flag to control whether to run the full training or a partial run with fewer epochs\n",
    "PARTIAL_RUN = False\n",
    "epochs = 80  # Default number of epochs for training\n",
    "\n",
    "# If PARTIAL_RUN is True, set the number of epochs to 2 for a quicker test run.\n",
    "if PARTIAL_RUN:\n",
    "    epochs = 2\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,  # The dataset used to train the model\n",
    "    epochs=epochs,  # Number of epochs to run the training\n",
    "    validation_data=validation_dataset,  # Dataset used to validate the model after each epoch\n",
    "    callbacks=def_callbacks(\"convnet_from_scratch_with_augmentation_keras\")  # List of callbacks to be applied during training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LAK18JWiwNO"
   },
   "outputs": [],
   "source": [
    "# Load the previously trained model from the specified file path\n",
    "test_model = keras.models.load_model(\"convnet_from_scratch_with_augmentation_keras.keras\")\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "# The evaluate() function returns two values:\n",
    "# - test_loss: The value of the loss function used during model training (e.g., binary crossentropy).\n",
    "# - test_acc: The accuracy of the model on the test dataset.\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "\n",
    "# Print the test accuracy with 3 decimal places\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HU64Kt9J7Ha4"
   },
   "source": [
    "With data augmentation, we roughly get **82-85%** accuracy. This is a big improvement over the previous approach, where we got roughly 70% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHfHdGCP_n6Y"
   },
   "source": [
    "### Please answer the questions below to complete the experiment:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VgSwVENIPcM6"
   },
   "outputs": [],
   "source": [
    "#@title  We are applying 2 convolution filters of size 3X3 on an image of a size 6X6 Pixel having 3 channels. What is the shape of the output after the convolution operation and what is the number of parameters including bias? Given, no padding and stride is one. {run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
    "Answer = \"(3X3X2); 20\" #@param [\"\", \"(4X4X2); 56\", \"(3X3X2); 20\", \"(3X3X2); 56\", \"(4X4X2); 20\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMzKSbLIgFzQ"
   },
   "outputs": [],
   "source": [
    "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
    "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjcH1VWSFI2l"
   },
   "outputs": [],
   "source": [
    "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
    "Additional = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VBk_4VTAxCM"
   },
   "outputs": [],
   "source": [
    "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
    "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XH91cL1JWH7m"
   },
   "outputs": [],
   "source": [
    "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
    "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8xLqj7VWIKW"
   },
   "outputs": [],
   "source": [
    "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
    "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "FzAZHt1zw-Y-"
   },
   "outputs": [],
   "source": [
    "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
    "try:\n",
    "  if submission_id:\n",
    "      return_id = submit_notebook()\n",
    "      if return_id : submission_id = return_id\n",
    "  else:\n",
    "      print(\"Please complete the setup first.\")\n",
    "except NameError:\n",
    "  print (\"Please complete the setup first.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
