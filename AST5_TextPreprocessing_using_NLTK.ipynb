{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tdtrlAhvIHY"
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "At the end of the experiment, you will be able to:\n",
    "\n",
    "* understand the use of NLTK library\n",
    "* perform text pre-processing using NLTK such as removing html strips and noise text, removing special characters, lemmatization, stemming, tokenization, removing stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okovJktL_Ea7"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "**NLTK** (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries.\n",
    "\n",
    "It is a free, open source, community-driven project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25xeb09mMs0B"
   },
   "source": [
    "## Dataset Description\n",
    "\n",
    "The IMDB movie review dataset can be downloaded from [here](http://ai.stanford.edu/~amaas/data/sentiment/). This dataset for binary sentiment classification contains around 50k movie reviews with the following attributes:\n",
    "\n",
    "* **review:** text based review of each movie\n",
    "* **sentiment:** positive or negative sentiment value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdLwhJWI9xmZ"
   },
   "source": [
    "### Setup Steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RH8Ecq9sbYU"
   },
   "source": [
    "### Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFl76_ngsasw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk                                                                        # platform for building Python programs to process natural language\n",
    "nltk.download('stopwords')                                                         # to download the stop words\n",
    "nltk.download('punkt')                                                             # tokenizer divides a text into a list of sentences, by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences\n",
    "nltk.download('wordnet')                                                           # to lemmatize word using WordNet's built-in function\n",
    "nltk.download('averaged_perceptron_tagger')                                        # for part-of-speech tagger\n",
    "from nltk.corpus import stopwords                                                  # importing the NTLK stopwords to remove articles, preposition and other words that are not actionable\n",
    "from nltk.stem.porter import PorterStemmer                                         # process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words\n",
    "from nltk.tokenize import word_tokenize                                            # allows to create individual objects from a bag of words\n",
    "from bs4 import BeautifulSoup                                                      # Python library for pulling data from HTML and XML files\n",
    "import re                                                                          # regular expression (or RE) specifies a set of strings that matches it\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7bG2m775ba6"
   },
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_fAB1U0fqJ7"
   },
   "outputs": [],
   "source": [
    "# read the dataset\n",
    "df = # YOUR CODE HERE to read 'IMDB_Dataset.csv'\n",
    "print(df.shape)\n",
    "df.head(10)      # first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sH2F7KdgLXMY"
   },
   "outputs": [],
   "source": [
    "# Let us view one of the reviews\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wy2WsjbRDCJr"
   },
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8mLwZe5CWSN"
   },
   "outputs": [],
   "source": [
    "# summary of the dataset\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twI5ntekDNrs"
   },
   "source": [
    "Now, we will look at the sentiment count by category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6izwMrUVDQ6f"
   },
   "outputs": [],
   "source": [
    "# sentiment count category-wise\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6HG5xcMCxJN"
   },
   "outputs": [],
   "source": [
    "# Visualize the postive and negative sentiments in a bar plot\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyeDUn36DVRn"
   },
   "source": [
    "We can see that the dataset is balanced.\n",
    "\n",
    "Now, we will do the text cleaning of the reviews.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "on2XCsxtI0-b"
   },
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "The data scraped from the website is mostly in the raw text form. This data needs to be cleaned before analyzing it or fitting a model to it. Cleaning up the text data is necessary to highlight the attributes that we are going to want our machine learning system to pick up on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tupdRoUSGYrw"
   },
   "source": [
    "**Removing noisy text**\n",
    "\n",
    "Sample noise removal tasks could include:\n",
    "\n",
    "* removing text file headers, footers\n",
    "* removing HTML, XML, etc. markup and metadata\n",
    "* extracting valuable data from other formats, such as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzsW7umy6Sea"
   },
   "outputs": [],
   "source": [
    "# removing the html strips\n",
    "def strip_html(text):\n",
    "    # BeautifulSoup is a useful library for extracting data from HTML and XML documents\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    text = text.replace('[', '')\n",
    "    # YOUR CODE HERE to remove ']'\n",
    "    return text\n",
    "    #return re.sub('\\[[^]]*\\]', '', text)          # to remove the text also present within square brackets\n",
    "\n",
    "# removing the noisy text\n",
    "def denoise_text(text):\n",
    "    # YOUR CODE HERE to update the text by applying strip_html() function\n",
    "    # YOUR CODE HERE to update the text by applying remove_between_square_brackets() function\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Mt02JQqEqzl"
   },
   "outputs": [],
   "source": [
    "txt = \"\"\"<body>\n",
    "<h1>This is a test heading.</h1>\n",
    "<p>Starting a paragraph here...</p>\n",
    "</body>\"\"\"\n",
    "\n",
    "# YOUR CODE HERE to call strip_html() on txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEYyDYVGFmiD"
   },
   "outputs": [],
   "source": [
    "txt = \"I bought [apple, banana, orange] yesterday.\"\n",
    "\n",
    "# YOUR CODE HERE to call remove_between_square_brackets() on txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x56E-c6KFvZH"
   },
   "outputs": [],
   "source": [
    "# apply denoise_text() function on review column\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQzlTJ-VGwVu"
   },
   "source": [
    "**Removing special characters**\n",
    "\n",
    "Special characters typically include any character that is not a letter or number, such as punctuation. Removing special characters from a string results in a string containing only letters and numbers.\n",
    "\n",
    "We can use the `re` python library for Regular expression operations.\n",
    "\n",
    "To know more about Regular expressions, click [here](https://realpython.com/regex-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hsi1MZ9X6Xqm"
   },
   "outputs": [],
   "source": [
    "# define function for removing special characters\n",
    "def remove_special_characters(text):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acd8t0kuIDOZ"
   },
   "outputs": [],
   "source": [
    "txt = \"Hi There! How are you?\"\n",
    "\n",
    "# YOUR CODE HERE to call remove_special_characters() on txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBWV2c7oGpVL"
   },
   "outputs": [],
   "source": [
    "# apply remove_special_characters() function on review column\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKHDo92_nm-H"
   },
   "source": [
    "**Lemmatization**\n",
    "\n",
    "Lemmatization is a text pre-processing technique used to break a word down to its root meaning or word (called lemma) to identify similarities.\n",
    "\n",
    "For example, a lemmatization algorithm would reduce the word ***better*** to its root word, or lemme, ***good***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tjqMtkJ9ake5"
   },
   "outputs": [],
   "source": [
    "# Lemmatize word using WordNet's built-in function\n",
    "# pos: The Part Of Speech tag.\n",
    "#      Valid options are \"n\" for nouns, \"v\" for verbs, \"a\" for adjectives,\n",
    "#                        \"r\" for adverbs and \"s\" for satellite adjectives.\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\", ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGsYJGmLG9bM"
   },
   "source": [
    "**Text Stemming**\n",
    "\n",
    "Stemming, also called suffix stripping, is a technique used to reduce text dimensionality. Stemming is also a type of text normalization that enables you to standardize some words into specific expressions also called stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8R8eAA_b6bOS"
   },
   "outputs": [],
   "source": [
    "# stemming the text\n",
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63MTPhVdIw45"
   },
   "outputs": [],
   "source": [
    "txt = \"He likes lemons bananas and oranges. He goes to market.\"\n",
    "\n",
    "# YOUR CODE HERE to call simple_stemmer() on txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bmBWpKUhG5F7"
   },
   "outputs": [],
   "source": [
    "# apply function on review column\n",
    "# YOUR CODE HERE to apply 'simple_stemmer' function on reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HA2RsIUenbGe"
   },
   "source": [
    "**Tokenization**\n",
    "\n",
    "Tokenization is the process of splitting paragraphs and sentences into smaller understandable parts (words).\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbCqVxg_F6jw"
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize('I enjoy playing football in the rain.')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvcVK-EfOnGk"
   },
   "source": [
    "**Part-of-speech tagging**\n",
    "\n",
    "Part-of-speech (POS) tagging is the task of determining the word class of a token. This is crucial for *disambiguation*, because different parts of speech may have similar forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vuT3NelVN44l"
   },
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omeXdqsFHm1M"
   },
   "source": [
    "**Removing stop words**\n",
    "\n",
    "Stop words are English words that do not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like the, he, have etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mgIjbfZgEgNH"
   },
   "outputs": [],
   "source": [
    "# setting english stopwords\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "print(stopword_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jacr7HAjfILU"
   },
   "source": [
    "The above list of stopwords also contains the word \"not\", and its other forms such as don't, didn't, etc. We need them for correct sentiment classification.\n",
    "\n",
    "For example, consider a negative review \"*not a good movie*\", and if we remove 'not' from it then it becomes a positive review \"*a good movie*\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1uSxBepgdsE"
   },
   "outputs": [],
   "source": [
    "# Exclude 'not' and its other forms from the stopwords list\n",
    "\n",
    "updated_stopword_list = []\n",
    "\n",
    "for word in stopword_list:\n",
    "    if word=='not' or word.endswith(\"n't\"):\n",
    "        pass\n",
    "    else:\n",
    "        updated_stopword_list.append(word)\n",
    "\n",
    "print(updated_stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fH5HXfk8OJj"
   },
   "outputs": [],
   "source": [
    "# removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    # splitting strings into tokens (list of words)\n",
    "\n",
    "    # YOUR CODE HERE to create 'tokens' variable by tokenizing 'text'\n",
    "\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        # filtering out the stop words\n",
    "        filtered_tokens = [token for token in tokens if token not in updated_stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in updated_stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVGniBcAKyvA"
   },
   "outputs": [],
   "source": [
    "txt = \"The movie was not that great\"\n",
    "\n",
    "# YOUR CODE HERE to call remove_stopwords() on txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H_OBq2crHXU5"
   },
   "outputs": [],
   "source": [
    "# apply remove_stopwords function on review column\n",
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
