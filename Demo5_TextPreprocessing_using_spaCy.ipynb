{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akVZjBjD2PtD"
   },
   "source": [
    "## Learning Objectives:\n",
    "\n",
    "At the end of the experiment, you will be able to:\n",
    "\n",
    "* understand the spaCy library\n",
    "* perform simple natural language processing tasks using the spaCy library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGel018GxQGn"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TI_iPJtzxSEB"
   },
   "source": [
    "**spaCy** is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
    "\n",
    "It is designed specifically for production use and helps you build applications that process and “understand” large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning.\n",
    "\n",
    "spaCy's features and capabilities include:\n",
    "\n",
    "- ***Tokenization***:\tSegmenting text into words, punctuations marks etc.\n",
    "- ***Part-of-speech (POS) Tagging***: Assigning word types to tokens, like verb or noun.\n",
    "- ***Dependency Parsing***: Assigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.\n",
    "- ***Lemmatization***: Assigning the base forms of words. For example, the lemma of “was” is “be”, and the lemma of “rats” is “rat”.\n",
    "- ***Sentence Boundary Detection (SBD)***: Finding and segmenting individual sentences.\n",
    "- ***Named Entity Recognition (NER)***: Labelling named “real-world” objects, like persons, companies or locations.\n",
    "- ***Entity Linking (EL)***: Disambiguating textual entities to unique identifiers in a knowledge base.\n",
    "- ***Similarity***: Comparing words, text spans and documents and how similar they are to each other.\n",
    "- ***Text Classification***: Assigning categories or labels to a whole document, or parts of a document.\n",
    "- ***Rule-based Matching***: Finding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions.\n",
    "- ***Training***: Updating and improving a statistical model's predictions.\n",
    "- ***Serialization***: Saving objects to files or byte strings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9SWBYOydg-e"
   },
   "source": [
    "### Statistical models\n",
    "\n",
    "While some of spaCy's features work independently, others require ***trained pipelines*** to be loaded, which enable spaCy to predict linguistic annotations - for example, whether a word is a verb or a noun.\n",
    "\n",
    "A trained pipeline can consist of multiple components that use a statistical model trained on labeled data.\n",
    "\n",
    "spaCy currently offers trained pipelines for a variety of languages, which can be installed as individual Python modules. Pipeline packages can differ in size, speed, memory usage, accuracy and the data they include.\n",
    "\n",
    "For English language, available trained pipelines include:\n",
    "- `en_core_web_sm`\n",
    "- `en_core_web_md`\n",
    "- `en_core_web_lg`\n",
    "- `en_core_web_trf` - English transformer pipeline\n",
    "\n",
    "To know more about trained pipelines for English, refer [here](https://spacy.io/models/en).\n",
    "\n",
    "Let's perform basic NLP tasks with spaCy using an English trained pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vYQQZ4j8duG"
   },
   "source": [
    "### Setup Steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysivBdlXMxoe"
   },
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnmSjKExJ-AK"
   },
   "outputs": [],
   "source": [
    "# Install the specific version 3.7.4 of the SpaCy library quietly (without showing unnecessary logs).\n",
    "!pip -q install spacy==3.7.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlHYHxmhJWqC"
   },
   "outputs": [],
   "source": [
    "# Check the installation details of SpaCy, such as version, available models, and configuration, by running the SpaCy CLI command.\n",
    "!python -m spacy info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioo8J8kk270t"
   },
   "source": [
    "From the above info, we can see that by default spaCy contains the small trained pipeline for English `en_core_web_sm`.\n",
    "\n",
    "To use medium, large, and transformer trained pipelines, they need to be installed first using the `!python -m spacy download` command.\n",
    "\n",
    "For example: `!python -m spacy download en_core_web_trf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enIOTr8y6hf9"
   },
   "outputs": [],
   "source": [
    "# Install English transformer pipeline\n",
    "# Note that Runtime needs to restart after this step\n",
    "\n",
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpB5KKWSKkby"
   },
   "source": [
    "**Restart the Runtime/Session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x83z3TOfNGS-"
   },
   "outputs": [],
   "source": [
    "!python -m spacy info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6wPl_vwqL6Y"
   },
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFOiNNeP0giN"
   },
   "outputs": [],
   "source": [
    "# Import the SpaCy library to perform NLP tasks and its visualization tool, displaCy, for rendering annotations and dependency parses.\n",
    "import spacy  # The main SpaCy library for Natural Language Processing (NLP).\n",
    "from spacy import displacy  # displacy is a SpaCy module used for visualizing text annotations like entity recognition and syntactic dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoRmCXB55X3l"
   },
   "source": [
    "### Load the trained pipeline\n",
    "\n",
    "Once you've downloaded and installed a trained pipeline, you can load it via `spacy.load()`. This will return a *Language object* containing all components and data needed to process text. We usually call it `nlp`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GGlQY8Ro2hw6"
   },
   "outputs": [],
   "source": [
    "# Load transformer pipeline for English\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# This gives us a Language object\n",
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UeGDyVaDpczn"
   },
   "source": [
    "Esentially, spaCy's *Language* object is a pipeline that uses the language model to perform a number of natural language processing tasks such as *tokenization*, *part-of-speech tagging*, *syntactic parsing*, *named entity recognition*, etc.\n",
    "\n",
    "<br>\n",
    "<img src='https://cdn.iisc.talentsprint.com/AIandMLOps/Images/spacy_pipeline.png' width=800px>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grxmbI2H6H5Q"
   },
   "outputs": [],
   "source": [
    "# Pipeline names\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVztM7aSHg-N"
   },
   "source": [
    "## Performing basic NLP tasks using spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWwX8KlNK3NX"
   },
   "source": [
    "Calling the Language object, `nlp`, on a string of text will return a processed *Doc*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLEKXAxxK3NX"
   },
   "outputs": [],
   "source": [
    "# An example sentence\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "beDKSs4XK3NY"
   },
   "outputs": [],
   "source": [
    "# Feed the string object under 'text' to the Language object under 'nlp'\n",
    "# Store the result under the variable 'doc'\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-d28toejq29Y"
   },
   "outputs": [],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xDMhpziK3NX"
   },
   "source": [
    "Passing the variable `text` to the _Language_ object `nlp` returns a spaCy *Doc* object, short for document.\n",
    "\n",
    "This object contains both the input text stored under `text` and the results of natural language processing using spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "69rWTI8oK3NY"
   },
   "outputs": [],
   "source": [
    "# Call the variable to examine the object\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLU_RXJuK3NZ"
   },
   "source": [
    "Calling the variable `doc` returns the contents of the object.\n",
    "\n",
    "Although the output resembles that of a Python string, the *Doc* object contains a wealth of information about its linguistic structure, which spaCy generated by passing the text through the NLP pipeline.\n",
    "\n",
    "Let's examine the tasks that were performed under the hood after the input sentence was provided to the language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhcEc6AiK3NZ"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybAUeP7dK3Na"
   },
   "source": [
    "*Tokenization* breaks the text down into words, punctuation and so on.\n",
    "\n",
    "The diagram below outlines the tasks that spaCy can perform after a text has been tokenised, such as *part-of-speech tagging*, *syntactic parsing* and *named entity recognition*.\n",
    "\n",
    "<img src='https://cdn.iisc.talentsprint.com/AIandMLOps/Images/spacy_pipeline.png' width=800px>\n",
    "\n",
    "Each *Doc* consists of individual tokens, and we can iterate over them.\n",
    "\n",
    "Let's print out each *Token* object stored in the _Doc_ object `doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OxUEyrg8K3Nb"
   },
   "outputs": [],
   "source": [
    "# Tokens present inside the document\n",
    "\n",
    "# Print the header \"Token\" followed by a separator line, then iterate through the tokens in the processed text (doc) and print each token's text.\n",
    "print(\"Token\\n\" + '='*20)  # Display \"Token\" as a header with a line of '=' for clarity.\n",
    "\n",
    "for token in doc:  # Loop through each token in the SpaCy Doc object.\n",
    "    print(token.text)  # Print the actual text of each token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KJaYn-3K3Nc"
   },
   "source": [
    "### Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnGIsQRHK3Nd"
   },
   "source": [
    "Part-of-speech (POS) tagging is the task of determining the word class of a token. This is crucial for *disambiguation*, because different parts of speech may have similar forms.\n",
    "\n",
    ">Consider the example: *The sailor dogs the hatch*.<br>\n",
    ">The present tense of the verb *dog* (to fasten something with something) is precisely the same as the plural form of the noun *dog*: *dogs*.\n",
    "\n",
    "To identify the correct word class, we must examine the context in which the word appears.\n",
    "\n",
    "spaCy provides two types of part-of-speech tags, coarse and fine-grained, which are stored under the attributes `pos_` and `tag_`, respectively.\n",
    "\n",
    "To access the results of POS tagging, let's loop over the *Doc* object `doc` and print each *Token* and its part-of-speech tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Va5tmZcItRvH"
   },
   "outputs": [],
   "source": [
    "# Print the token, its coarse-grained (general) POS tag, and fine-grained (detailed) POS tag for each token in the processed text.\n",
    "\n",
    "# Header for the table with proper formatting for alignment\n",
    "print(f\"{' ':<30}POS tag\\n{' ':<20}{'-'*25}\")  # Display a header for \"POS tag\" section with a separator.\n",
    "print(f\"{'Token':<20}{'Coarse':<13}Fine-grained\\n{'='*45}\")  # Define columns: \"Token\", \"Coarse\" (POS), and \"Fine-grained\" tags.\n",
    "\n",
    "for token in doc:  # Iterate over each token in the SpaCy Doc object.\n",
    "    coarse = token.pos_  # Retrieve the coarse-grained POS tag (e.g., NOUN, VERB).\n",
    "    fine = token.tag_  # Retrieve the fine-grained POS tag (e.g., singular noun or past-tense verb).\n",
    "\n",
    "    # Print each token's text, coarse POS tag, and fine-grained POS tag in formatted columns.\n",
    "    print(f\"{token.text:<20}{coarse:<13}{fine}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8u4RTfgK3Ny"
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcxwD63HK3Nz"
   },
   "source": [
    "A **lemma** is the base form of a word.\n",
    "\n",
    "Unless explicitly instructed, computers cannot tell the difference between singular and plural forms of words, but treat them as distinct tokens, because their forms differ.\n",
    "\n",
    "For instance, if we want to count the occurrences of words, a process known as _lemmatization_ is needed to group together the different forms of the same token.\n",
    "\n",
    "Lemmas are available for each _Token_ under the attribute `lemma_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JtMeYKd4K3Nz"
   },
   "outputs": [],
   "source": [
    "# Print each token along with its base form (lemma) from the processed text.\n",
    "\n",
    "# Header for the table with proper formatting\n",
    "print(f\"{'Token':<20} Lemma\\n{'='*30}\")  # Define columns: \"Token\" and \"Lemma\" with a separator.\n",
    "\n",
    "for token in doc:  # Iterate over each token in the SpaCy Doc object.\n",
    "    lemma = token.lemma_  # Retrieve the lemma (base form) of the token.\n",
    "\n",
    "    # Print the token's text and its corresponding lemma in formatted columns.\n",
    "    print(f\"{token.text:<20} {lemma}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnstEWPGR_5U"
   },
   "source": [
    "### Removing punctuations, stop words, converting to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jPZjVnY-SUET"
   },
   "outputs": [],
   "source": [
    "# Access and print the stop words list provided by SpaCy for English.\n",
    "\n",
    "# Retrieve SpaCy's predefined set of English stop words.\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Print the total number of stop words in the set.\n",
    "print(len(spacy_stopwords))\n",
    "\n",
    "# Print the complete list of stop words.\n",
    "print(spacy_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PntYPVjkVOfb"
   },
   "outputs": [],
   "source": [
    "# Check and print whether each token is a stop word or a punctuation mark.\n",
    "\n",
    "# Header for the table with proper formatting\n",
    "print(f\"{'Token':<20} {'Is stopword':<15} Is Punctuation\\n{'='*55}\")\n",
    "# Define columns: \"Token\", \"Is stopword\", and \"Is Punctuation\" with a separator line.\n",
    "\n",
    "for token in doc:  # Iterate through each token in the SpaCy Doc object.\n",
    "    stop = token.is_stop  # Check if the token is a stop word (returns True/False).\n",
    "    punct = token.is_punct  # Check if the token is a punctuation mark (returns True/False).\n",
    "\n",
    "    # Print the token's text, whether it's a stop word, and whether it's a punctuation mark.\n",
    "    print(f\"{token.text:<20} {stop:<15} {punct}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11yH1FR4XAdq"
   },
   "outputs": [],
   "source": [
    "def preprocess_token(token):\n",
    "\n",
    "    \"\"\" If a token is not a stop word and not a punctuation,\n",
    "        then reduce it to its base form, remove trailing spaces, and covert to lowercase. \"\"\"\n",
    "\n",
    "    if not token.is_stop and not token.is_punct:  # Check if the token is neither a stop word nor punctuation.\n",
    "        return token.lemma_.strip().lower()  # Return the lemmatized, trimmed, and lowercase version of the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZoCOU8gSXz_h"
   },
   "outputs": [],
   "source": [
    "# Preprocess each token in the text and print both the original token and the preprocessed version.\n",
    "\n",
    "# Header for the output table with proper formatting\n",
    "print(f\"{'Token':<20} Preprocessed token\\n{'='*40}\")\n",
    "# Define columns: \"Token\" (original text) and \"Preprocessed token\" with a separator line.\n",
    "\n",
    "for token in doc:  # Iterate through each token in the SpaCy Doc object.\n",
    "    output = preprocess_token(token)  # Call the preprocess_token function to process the token.\n",
    "\n",
    "    # Print the original token text and its preprocessed version.\n",
    "    # If the token is a stop word or punctuation, the preprocessed version will be None.\n",
    "    print(f\"{token.text:<20} {output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyeCdI6iK3Nz"
   },
   "source": [
    "### Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mv2U9VEzK3N0"
   },
   "source": [
    "Named entity recognition (NER) is the task of recognising and classifying entities named in a text.\n",
    "\n",
    "spaCy can recognise the named entities such as persons, geographic locations, and products as these were annotated in the dataset its trained on (OntoNotes 5 corpus).\n",
    "\n",
    "We can use the *Doc* object's `.ents` attribute to get the named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wn523SiK3N0"
   },
   "outputs": [],
   "source": [
    "# Access and display the named entities in the processed text (doc).\n",
    "doc.ents  # This returns a list of all named entities recognized by SpaCy in the processed text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oM4MeyX2qlas"
   },
   "source": [
    "This returns a tuple with the named entities.\n",
    "\n",
    "Each item in the tuple is a spaCy *Span* object. *Span* objects can consist of multiple *Token* objects, as many named entities span multiple *Tokens*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NGs7oHNjqMGu"
   },
   "outputs": [],
   "source": [
    "# Check the type of the object that stores the named entities\n",
    "type(doc.ents[0])  # This will return the type of the first entity in the list of named entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaCzY90CK3N0"
   },
   "source": [
    "The named entities and their types are stored under the attributes `.text` and `.label_` of each *Span* object.\n",
    "\n",
    "Let's loop over the *Span* objects in the tuple and print out both attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yveeIWxXK3N0"
   },
   "outputs": [],
   "source": [
    "# Loop over the named entities in the Doc object, and print each entity with its label and explanation.\n",
    "\n",
    "# Header for the output table with proper formatting\n",
    "print(f\"{'Text':<20} {'Entity_label':<16} Explanation\\n{'='*80}\")\n",
    "# Define columns: \"Text\" (named entity), \"Entity_label\" (entity type), and \"Explanation\" (label description).\n",
    "\n",
    "for ent in doc.ents:  # Iterate over each named entity in the doc.ents list.\n",
    "    ent_text = ent.text           # Get the text of the named entity (e.g., 'Apple Inc.')\n",
    "    ent_label = ent.label_        # Get the entity label (e.g., 'ORG' for organization)\n",
    "    ent_label_val = spacy.explain(ent_label)  # Get the explanation of the entity label (e.g., 'Organization')\n",
    "\n",
    "    # Print the entity text, its label, and the explanation of the label.\n",
    "    print(f\"{ent_text:<20} {ent_label:<16} {ent_label_val}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GCgHkJ7K3N1"
   },
   "source": [
    "As you can see, named entities like '$1 billion' identified in the *Doc* consist of multiple *Tokens*, which is why they are represented as *Span* objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGjkG8m3K3N2"
   },
   "source": [
    "spaCy [*Span*](https://spacy.io/api/span) objects contain several useful arguments.\n",
    "\n",
    "Most importantly, the attributes `start` and `end` return the indices of _Tokens_, which determine where the _Span_ starts and ends in the *Doc* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPwNfX-VK3N2"
   },
   "outputs": [],
   "source": [
    "# Print the named entity and its start and end token indices in the Doc.\n",
    "print(doc.ents[2], doc.ents[2].start, doc.ents[2].end)\n",
    "# doc.ents[2] accesses the third named entity in the Doc (indexing starts at 0).\n",
    "# doc.ents[2].start gives the index of the first token of the named entity.\n",
    "# doc.ents[2].end gives the index of the token just after the last token of the named entity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z29gu0lwK3N3"
   },
   "source": [
    "The named entity starts at index 8 and ends at index 11 in the *Doc* object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ycEVv9KrNkZ"
   },
   "source": [
    "#### Visualize Named Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTKE_TnYK3N4"
   },
   "source": [
    "We can also render the named entities using *displacy*, the spaCy module we used for visualising dependency parses above.\n",
    "\n",
    "Note that we must pass the string `ent` to the `style` argument to indicate that we wish to visualise named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUN9_a2KK3N5"
   },
   "outputs": [],
   "source": [
    "# Visualize the named entities in the document using SpaCy's displacy visualizer.\n",
    "spacy.displacy.render(doc, style='ent')\n",
    "# `style='ent'` specifies that we want to visualize the named entities in the document.\n",
    "# This will highlight and display the named entities with their labels (e.g., ORG, PERSON, GPE) in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5fLOBFpxSt1"
   },
   "source": [
    "**Test another example 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZLngerFvbL7"
   },
   "outputs": [],
   "source": [
    "# Visualize named entities in a new sample text using SpaCy's displacy visualizer.\n",
    "\n",
    "text2 = \"On 3rd Feb, Ram was in Delhi.\\nLater he traveled to Mumbai via Air India flight reading a Time magazine to meet Raj.\\nAfter 10 days, he went again back to Delhi wearing a Timex watch.\"\n",
    "# This is a new sample text to analyze.\n",
    "\n",
    "doc2 = nlp(text2)  # Process the new text using the SpaCy NLP pipeline (the `nlp` object).\n",
    "spacy.displacy.render(doc2, style='ent')\n",
    "# Render the named entities in the new `doc2`, highlighting and labeling them in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogdrSb382R3J"
   },
   "source": [
    "If a particular tag used for a named entity is unfamiliar, you can check it's explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-dFic41xMda"
   },
   "outputs": [],
   "source": [
    "# Get the explanation for the 'DATE' entity label using SpaCy's explain function.\n",
    "spacy.explain('DATE')\n",
    "# This will return a human-readable explanation for the entity label 'DATE'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBp1zyMXxnJ6"
   },
   "outputs": [],
   "source": [
    "# Get the explanation for the 'PERSON' entity label using SpaCy's explain function.\n",
    "spacy.explain('PERSON')\n",
    "# This will return a human-readable explanation for the entity label 'PERSON'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAvWA3882Ues"
   },
   "source": [
    "**Test another example 3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UV6e_njh1LKX"
   },
   "outputs": [],
   "source": [
    "# Visualize named entities in another sample text using SpaCy's displacy visualizer.\n",
    "\n",
    "text3 = \"Holmes solves his another case while sitting at his home in Baker Street, without moving a single inch.\"\n",
    "# This is another sample text to analyze, mentioning a character and a location.\n",
    "\n",
    "doc3 = nlp(text3)  # Process the new text using the SpaCy NLP pipeline (the `nlp` object).\n",
    "spacy.displacy.render(doc3, style='ent')\n",
    "# Render the named entities in the new `doc3`, highlighting and labeling them in the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L76wtVpb1n6a"
   },
   "outputs": [],
   "source": [
    "# Get the explanation for the 'FAC' entity label using SpaCy's explain function.\n",
    "spacy.explain('FAC')\n",
    "# This will return a human-readable explanation for the entity label 'FAC'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFXGM68ARzhb"
   },
   "source": [
    "References:\n",
    "* https://spacy.io/usage/spacy-101"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
