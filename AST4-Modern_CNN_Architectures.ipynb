{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bpgBd9QYP0_"
   },
   "source": [
    "## Learning Objectives:\n",
    "\n",
    "At the end of the experiment, you will be able to:\n",
    "1.   Build important blocks for modern CNNs\n",
    "      - Residual connections\n",
    "      - Batch normalisation\n",
    "      - Depthwise separable convolution\n",
    "2.   Interpret what CNNs learn\n",
    "      - Visualising activations\n",
    "      - Visualising filters\n",
    "      - Visualising heatmaps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqTI0D3bYjsJ"
   },
   "source": [
    "## 1. Important building blocks for modern CNNs\n",
    "\n",
    "Here we will study about 3 important building blocks:\n",
    "* Residual connection\n",
    "* Batch normalization\n",
    "* Depthwise separable convolution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7q-wwT5Y7Oh"
   },
   "source": [
    "### Residual connections\n",
    "\n",
    "Why do we need them?\n",
    "\n",
    "* CNNs can become extremely deep.\n",
    "* Prone to the vanishing gradient problem\n",
    "\n",
    "Solution:\n",
    "* allow gradients to flow through another shortcut\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ryb9kj388SSs"
   },
   "source": [
    "![picture](https://drive.google.com/uc?export=view&id=1gOwZPYnxfCGSsevLCzc7_41apUrkJKI5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVF-O0dYjUZu"
   },
   "source": [
    "### Setup Steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EBDiTOcWlv0"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRCDcQ_VBeAc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jjbyoe08y6YU"
   },
   "source": [
    "### **Build model with residual connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYkBqRTP_TJd"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(32, 32, 3), name=\"input\")\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\", name=\"C1\")(inputs)  # Q: No. of filters and kernel size? A: 32, 3x3\n",
    "residual = x\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\", name=\"C2a\")(x)\n",
    "# ADD a residual layer\n",
    "# YOUR CODE HERE\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "be3LFj5gDg3k"
   },
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "De25vNGd-0Ae"
   },
   "source": [
    "Residual branch may contain 1 layer to make sure addition is possible, i.e. accomodate sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rjv_A_9PY6A3"
   },
   "outputs": [],
   "source": [
    "# Build model with residual connection - layer in the residual branch\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9M3gPZRmksDR"
   },
   "outputs": [],
   "source": [
    "# Plot_model\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzTDlBrsDrws"
   },
   "outputs": [],
   "source": [
    "# Model Summary\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkL86M3HB7Gs"
   },
   "source": [
    "**Important**: Add layers of **same** shape !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyQydIMuB8Vw"
   },
   "source": [
    "Q: Can we add layers of shapes (30,30,64) and (30,30,32)?\n",
    "\n",
    "A: No\n",
    "\n",
    "Q: But what happens if you have a pooling layer in between?\n",
    "\n",
    "A: Spatial Dimension reduction due to stride\n",
    "\n",
    "Solution: Use Strides in the  Conv layer in the skip connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMfQcikJ8tqh"
   },
   "source": [
    "### Model with residual connections and a max pool layer in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yG1V6Zrml5L"
   },
   "outputs": [],
   "source": [
    "# model with residual connections and a max pool layer in between\n",
    "inputs = keras.Input(shape=(32, 32, 3))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
    "residual = x\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D(2, padding=\"same\")(x)  # Max pooling layer reduces the dimsion. #Q: By how much ? A: half\n",
    "residual = layers.Conv2D(64, 1, strides=2)(residual) #Need a stride=2 to accomodate for the maxpool downsampling in the other branch\n",
    "x = layers.add([x, residual])\n",
    "model3 = keras.Model(inputs=inputs, outputs=x)\n",
    "plot_model(model3, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OV4MhFP1Dy5l"
   },
   "outputs": [],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rycg6y3ehvl"
   },
   "source": [
    "With residual connections, you can build networks of arbitrary depth, without having to worry about vanishing gradients. We will see an example later.\n",
    "\n",
    "**Intuitions on why residual blocks work:**\n",
    "*   Shorter path for gradients\n",
    "*   Easy to learn the identity matrix\n",
    "*   Ensemble of shallow networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9dpxDKLtbkG"
   },
   "source": [
    "### **Batch Normalization**\n",
    "*   Adaptively normalize data even as the mean and variance change over time during training\n",
    "*   During training, it uses the mean and variance of the current batch of data to normalize samples\n",
    "*    During inference (when a big enough batch of representative data may not be available), it uses an exponential moving average of the batch-wise mean and variance of the data seen during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTucnUKguvcW"
   },
   "source": [
    "<div>\n",
    "<img src=\"https://miro.medium.com/max/1153/1*xQhPvRh08oKFC63swgWr_w.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbssJsObuxz0"
   },
   "source": [
    "Now let's try to calculate the no. of params introduced because of batch normalization in following example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TClF7208pG7"
   },
   "source": [
    "### Model with a batch normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5jAYBOip2so"
   },
   "outputs": [],
   "source": [
    "# Model with a batch normalization layer\n",
    "inputs = keras.Input(shape=(32, 32, 3))\n",
    "# Because the output of the Conv2D layer gets normalized, the layer doesn’t need its own bias vector\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuFhr6H8xa47"
   },
   "source": [
    "Q: How many paramerters does the BN layer introduce in the above model?\n",
    "\n",
    "A: 128\n",
    "\n",
    "\n",
    "Q: HOW?\n",
    "\n",
    "A: Batch Normalization layer introduces **four parameters** per channel. However, only **two parameters**, γ and β, are **learnable/trainable parameters** used to apply scaling and shifting to the transformation. The remaining **two parameters**, moving_mean and moving_variance, are **non-trainable** and are directly calculated from the mean across the batch and saved as part of the state of the Batch Normalization layer.\n",
    "\n",
    "Here, the number of channels in the preceding layer is 32. Hence, the total number of parameters is equal to 32 * 4 = 128, but out of this, only 128/2 = 64 are trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtKeg1J9tVk9"
   },
   "outputs": [],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YLehpMpxJMN"
   },
   "source": [
    "**Intuitions:**\n",
    "*  Batch Normalization is also a (weak) regularization method.\n",
    "    - increases no. of params\n",
    "    - but also adds noise ~ data augmentation ~ dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AqwncgPvQju"
   },
   "source": [
    "### **Depthwise separable convolutions**\n",
    "\n",
    "* This layer performs a **spatial convolution [Depthwise Conv.]** on each channel of\n",
    "its input, independently, before mixing output channels via a **pointwise convolution**.\n",
    "\n",
    "* **Depthwise separable convolution = Depthwise Conv. +  Pointwise Conv.**\n",
    "\n",
    "* This makes your model smaller and  acts as a strong prior. We impose a strong prior by assuming that spatial patterns and cross-channel patterns can be modeled separately.This is equivalent to separating the learning of spatial features and the learning of channel-wise features.\n",
    "* Depthwise separable convolution relies on the assumption that spatial locations in intermediate activations\n",
    "are highly correlated, but different channels are highly independent. So we never use depthwise separable convolution after the input layer. Because RGB channels are **highly correlated**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezUdJ7lJ4f-p"
   },
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/56/RGB_channels_separation.png\" width=\"400\"/><img src=\"https://drive.google.com/uc?export=view&id=1e4h4NdbHRCxB1Oe_eoNhQQ6ZbAERf22K\" width=\"500\"/> <figcaption>## RGB channels are highly correlated. ##________________## Depthwise separable convolutions ##</figcaption>\n",
    "\n",
    "</center>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9__AMGKUS6bb"
   },
   "source": [
    "Let's quickly look at the code first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwVyRlQ8uOLb"
   },
   "outputs": [],
   "source": [
    "# Building a model with Separable Conv layer\n",
    "inputs = keras.Input(shape=(32, 32, 3))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
    "x = layers.SeparableConv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "sep_model = keras.Model(inputs=inputs, outputs=x)\n",
    "plot_model(sep_model, show_shapes=True)\n",
    "sep_model.summary()\n",
    "# Q: Verify the no. of params in the separable_conv2D layer A: (32*(3*3)) + (32*1*1*64) + 64 = 2400\n",
    "# Not in keras 'SeparableConv2D', the bias at  Depthwise Conv. is not considered.\n",
    "# Considering all bias : (32*(3*3) + 32) + (32*1*1*64) + 64 = 2432"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCsZV3LCxYJd"
   },
   "source": [
    "Let's compoare the above with a model where we replace the SeparableConv2D with a Conv2D layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0bS1qIoywrAB"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Q: Why does sep_model have much less params? A: Depthwise and pointwise convs are done independently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfYMjxYWyOL5"
   },
   "source": [
    "### **A mini Xception-like model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQkHVp72IRkX"
   },
   "source": [
    "We'll build a model like the Xception model, but a smaller version.\n",
    "\n",
    "But first let's see what the actual Xception model looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEYS-LaWIDql"
   },
   "source": [
    "![picture](https://miro.medium.com/max/833/1*t6qfo9ucYza_lbLfg5-p_w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXmbHNJAWhMd"
   },
   "source": [
    "Q: In middle- flow blocks, what arguments do you give to the SepConv layer ?\n",
    "\n",
    "A: HW question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQYxgWbwIvLN"
   },
   "source": [
    "Let's use the cats-vs-dogs data and create datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUfhxMyOkrA2"
   },
   "outputs": [],
   "source": [
    "# defining path names for futur use\n",
    "data_dir = '/content/cats_vs_dogs_small'\n",
    "\n",
    "train_path = data_dir + '/train'\n",
    "validation_path = data_dir + '/validation'\n",
    "test_path = data_dir + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVOWgUhECt5d"
   },
   "outputs": [],
   "source": [
    "# creating datasets using utility\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnZGLjDhycgf"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "# x = data_augmentation(inputs)\n",
    "\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x) # Q: why not use depth-wise sep conv here? A: RGB channels in input image are highly corelated\n",
    "\n",
    "for size in [32, 64, 128, 256, 512]:                # Repeated block. Very common practice\n",
    "    residual = x\n",
    "\n",
    "    x = layers.BatchNormalization()(x)              # We can also apply BN just before the activation\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    residual = layers.Conv2D(\n",
    "        size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIvwflhlc9Q-"
   },
   "outputs": [],
   "source": [
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDIn4pcAyjcC"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit( train_dataset,\n",
    "                      epochs=100,\n",
    "                      validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIhBNi09EvQ0"
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(history.history)\n",
    "plt.plot(range(1,len(data)+1),data['accuracy'],'bo',label=\"Training accuracy\")\n",
    "plt.plot(range(1,len(data)+1),data['val_accuracy'],'b',label=\"Validation accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(range(1,len(data)+1),data['loss'],'bo',label=\"Training loss\")\n",
    "plt.plot(range(1,len(data)+1),data['val_loss'],'b',label=\"Validation loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHFkjDr4eAZV"
   },
   "source": [
    "## 2. Interpreting what ConvNets learn\n",
    " * Visualizing intermediate ConvNetsoutputs (intermediate activations)\n",
    " * Visualizing ConvNets filters\n",
    " * Visualizing heatmaps of class activation in an image\n",
    "\n",
    "NOTE:\n",
    "* We will focus mostly on the concepts and key ideas\n",
    "* A lot of the code is pre-processing and post-processing. We will not spend time on these parts.\n",
    "* We will see the part of the code that implements the key ideas.\n",
    "\n",
    "Pro:\n",
    "*   Developing ideas\n",
    "*   Developing thought process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtyRbwd7eKAE"
   },
   "source": [
    "### **Visualizing intermediate activations**\n",
    "\n",
    "The output of a layer is called its 'activation'(It's the output of the activation function).\n",
    "\n",
    "These activations can be visualized by plotting the feature maps.\n",
    "\n",
    "![picture](https://drive.google.com/uc?export=view&id=1vT8e59AYTFRlrrI3C-iUHTctxyhfBiJJ)\n",
    "\n",
    "We will plot each feature map independently as a 2D image, since they encode relatively indepent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6EpNnTPtWcL"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers  # <----- Note this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrmyY_4uF8rE"
   },
   "source": [
    "**Model to be used for visualization :** We are going to use a trained model from the previous assignment -  M2_AST_01_Convolutional _Neural_Networks. In that assignment, we created one model with augmentation and saved it through a callback function with the name \"convnet_from_scratch_with_augmentation_keras\". You can download that model from there and use it by providing the proper path after loading it.\n",
    "\n",
    "For the sake of simplicity, we have already provided that model and has been downloaded along with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoNajzD7pntO"
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model('/content/convnet_from_scratch_with_augmentation_keras')\n",
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dky0dGDqoew"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset) #Q: Why don't we have 2 arguments\n",
    "print(f\"Test accuracy is:{test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSL_9iBEGlZA"
   },
   "source": [
    "**Getting the image &  Preprocessing** that will be passsed inside the model for visualization ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vvkpXx6KhZng"
   },
   "outputs": [],
   "source": [
    "img_path = keras.utils.get_file(fname=\"cat.jpg\",origin=\"https://img-datasets.s3.amazonaws.com/cat.jpg\")\n",
    "# Preprocessing a single image\n",
    "def get_img_array(img_path, target_size):\n",
    "    img = keras.utils.load_img(img_path, target_size=target_size)\n",
    "    array = keras.utils.img_to_array(img) # converts image to np array\n",
    "    # Add a dimension to transform the array intoa “batch” of a single sample.\n",
    "    array = np.expand_dims(array, axis=0)     #Its shape is now (1, 180, 180, 3)\n",
    "    return array\n",
    "\n",
    "img_tensor = get_img_array(img_path, target_size=(180, 180)) #resize image\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img_tensor[0].astype(\"uint8\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A18axaoihdg8"
   },
   "outputs": [],
   "source": [
    "# Focus here\n",
    "# Instantiating a model that returns \"layer activations\"\n",
    "layer_outputs = []\n",
    "layer_names = []\n",
    "for layer in model.layers:                                      #Q: what does model.layers return ? A: list of layers of the model\n",
    "    if isinstance(layer, (layers.Conv2D, layers.MaxPooling2D)): #Q: difference between layer and layers? A: layer is a layer object in the model. layers is the module we have imported\n",
    "        layer_outputs.append(layer.output)\n",
    "        # layer_outputs.append(layers.Dense(1)(layer.output))  # To visualise the output in plot_model()\n",
    "        layer_names.append(layer.name)\n",
    "activation_model = keras.Model(inputs=model.input, outputs=layer_outputs) #Q: what does layer_outputs contain? A: output of conv2d and maxpool layers\n",
    "plot_model(activation_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtSTumbvHGfM"
   },
   "outputs": [],
   "source": [
    "activation_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7z909nZiiTxc"
   },
   "outputs": [],
   "source": [
    "# Compute layer activations\n",
    "activations = activation_model.predict(img_tensor)\n",
    "print(f\"No. of outputs= {len(activations)}\")\n",
    "\n",
    "first_layer_feature_maps = activations[0]    # Q: why is this the first layer's activation? A: index 0\n",
    "print(f\"first_layer_activation.shape= {first_layer_feature_maps.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lML8McG4k0L9"
   },
   "outputs": [],
   "source": [
    "# Visualise activation\n",
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(first_layer_feature_maps[0, :, :, 0], cmap=\"viridis\")  # Q: which (1st/2nd/..)feature map are we visualing? A: 1st (0 in the last index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqjzaGrX0vZg"
   },
   "source": [
    "It seems that the filter has detected ______.\n",
    "\n",
    "Let's look at a feature map after each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHK7sEtx2IUo"
   },
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "  plt.matshow(activations[i][0, :, :, 2], cmap=\"viridis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXJOiFWx1Bg0"
   },
   "source": [
    "Note the dimensions on the above images. Successive feature maps are actually of smaller dimensions but scaled to be the same size during visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SC0nlq_3Lb0"
   },
   "source": [
    "Now let's visualise all the feature maps of all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sov3-WP2k3aT"
   },
   "outputs": [],
   "source": [
    "# Post-processing code - only visualizaton\n",
    "# Visualizing every channel in every intermediate activation\n",
    "images_per_row = 16\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    n_features = layer_activation.shape[-1]\n",
    "    size = layer_activation.shape[1]\n",
    "    n_cols = n_features // images_per_row\n",
    "    display_grid = np.zeros(((size + 1) * n_cols - 1,\n",
    "                             images_per_row * (size + 1) - 1))\n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            channel_index = col * images_per_row + row\n",
    "            channel_image = layer_activation[0, :, :, channel_index].copy()\n",
    "            if channel_image.sum() != 0:\n",
    "                channel_image -= channel_image.mean()\n",
    "                channel_image /= channel_image.std()\n",
    "                channel_image *= 64\n",
    "                channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype(\"uint8\")\n",
    "            display_grid[\n",
    "                col * (size + 1): (col + 1) * size + col,\n",
    "                row * (size + 1) : (row + 1) * size + row] = channel_image\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(display_grid, aspect=\"auto\", cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDL2xEBypLwG"
   },
   "source": [
    "* The first layer acts as a collection of various edge detectors.\n",
    "\n",
    "* As you go deeper, the activations become increasingly abstract and less visually interpretable. They begin to encode higher-level concepts such as “cat ear” and “cat eye.”\n",
    "\n",
    "* The sparsity of the activations increases with the depth of the layer: in the first layer, almost all filters are activated by the input image, but in the following layers, more and more filters are blank. This means the pattern encoded by the filter isn’t found in the input image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVsagdgdquAw"
   },
   "source": [
    "### **Visualising ConvNet filters**\n",
    "*   Pick a filter\n",
    "*   Ask the question: What kind of an input image will excite the filter?\n",
    "*   What should the input image be so that you see a (yellow) feature map?\n",
    "*   In other words, we want to visualize those patterns in the input image that the filter picks up and results in high (yellow) values in the feature map.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VA0-i2t6k69H"
   },
   "outputs": [],
   "source": [
    "# Instantiating the Xception convolutional base\n",
    "\n",
    "model = keras.applications.xception.Xception( weights=\"imagenet\", include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1fkAZk_fyIaq"
   },
   "outputs": [],
   "source": [
    "# Q: Printing the names of  conv and sepConv layers in Xception\n",
    "\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, (keras.layers.Conv2D, keras.layers.SeparableConv2D)):\n",
    "        print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rb_zmZoNyIc-"
   },
   "outputs": [],
   "source": [
    "# Creating a feature extractor model\n",
    "\n",
    "layer_name = \"block3_sepconv1\"\n",
    "layer = model.get_layer(name=layer_name)\n",
    "feature_extractor = keras.Model(inputs=model.input, outputs=layer.output)\n",
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q33H-EUi7aeB"
   },
   "source": [
    "Q: What is the last layer ?\n",
    "\n",
    "Q: How many filters does block3_sepconv1 have?\n",
    "\n",
    "Q: Why are there so many Nones in the shapes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lB30XfKvyIff"
   },
   "outputs": [],
   "source": [
    "# Using the feature extractor\n",
    "\n",
    "activation = feature_extractor(keras.applications.xception.preprocess_input(img_tensor))\n",
    "# Image is preprocessed specific to Inception model before passing inside the feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTnndxqY6sTc"
   },
   "source": [
    "Here comes the key idea:\n",
    "*   Define an objective function: mean pixel value of feature map\n",
    "*   Use gradient \"Ascent\" on the \"input image space\" to maximize this objective\n",
    "Here's an analogy: (drawing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLxwawZfyIh0"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def compute_loss(image, filter_index):      #Q: How many indices do we have? A: 256\n",
    "    activation = feature_extractor(image)\n",
    "    filter_activation = activation[:, 2:-2, 2:-2, filter_index] # leaving out the boundaries\n",
    "    return tf.reduce_mean(filter_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_amLc3KyIj8"
   },
   "outputs": [],
   "source": [
    "# Loss maximization via stochastic gradient ascent\n",
    "# Revise tutorial 1\n",
    "@tf.function\n",
    "def gradient_ascent_step(image, filter_index, learning_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(image)\n",
    "        loss = compute_loss(image, filter_index)\n",
    "    grads = tape.gradient(loss, image)    # Q: Is the gradient a vector or scalar? A: vector\n",
    "    grads = tf.math.l2_normalize(grads)    # Normalize the gradient\n",
    "    image += learning_rate * grads          # Q: What makes this gradient \"ascent\"? A: plus sign\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHshMEN4yIml"
   },
   "outputs": [],
   "source": [
    "# Function to generate filter visualizations\n",
    "\n",
    "img_width = 200\n",
    "img_height = 200\n",
    "\n",
    "def generate_filter_pattern(filter_index):\n",
    "    iterations = 30\n",
    "    learning_rate = 10.\n",
    "    image = tf.random.uniform(\n",
    "        minval=0.4,\n",
    "        maxval=0.6,\n",
    "        shape=(1, img_width, img_height, 3))\n",
    "    for i in range(iterations):\n",
    "        image = gradient_ascent_step(image, filter_index, learning_rate)\n",
    "    return image[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSnWkDvN0EZ1"
   },
   "outputs": [],
   "source": [
    "# Utility function to convert a tensor into a valid image\n",
    "\n",
    "def deprocess_image(image):\n",
    "    image -= image.mean()\n",
    "    image /= image.std()\n",
    "    image *= 64\n",
    "    image += 128\n",
    "    image = np.clip(image, 0, 255).astype(\"uint8\")\n",
    "    image = image[25:-25, 25:-25, :]\n",
    "    return image\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(deprocess_image(generate_filter_pattern(filter_index=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rASYbJnX0HtR"
   },
   "outputs": [],
   "source": [
    "# Post processing- Just visualization\n",
    "# Generating a grid of all filter response patterns in a layer\n",
    "\n",
    "all_images = []\n",
    "for filter_index in range(64):\n",
    "    print(f\"Processing filter {filter_index}\")\n",
    "    image = deprocess_image(\n",
    "        generate_filter_pattern(filter_index)\n",
    "    )\n",
    "    all_images.append(image)\n",
    "\n",
    "margin = 5\n",
    "n = 8\n",
    "cropped_width = img_width - 25 * 2\n",
    "cropped_height = img_height - 25 * 2\n",
    "width = n * cropped_width + (n - 1) * margin\n",
    "height = n * cropped_height + (n - 1) * margin\n",
    "stitched_filters = np.zeros((width, height, 3))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        image = all_images[i * n + j]\n",
    "        stitched_filters[\n",
    "            (cropped_width + margin) * i : (cropped_width + margin) * i + cropped_width,\n",
    "            (cropped_height + margin) * j : (cropped_height + margin) * j\n",
    "            + cropped_height,\n",
    "            :,\n",
    "        ] = image\n",
    "\n",
    "keras.utils.save_img(\n",
    "    f\"filters_for_layer_{layer_name}.png\", stitched_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqQbBmvZ0Hwi"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(40,40))\n",
    "# plt.matshow(stitched_filters)\n",
    "\n",
    "for i in [0,8,16,32]:\n",
    "  plt.figure()\n",
    "  plt.imshow((all_images[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rn1Ra77e4VIK"
   },
   "source": [
    "![picture](https://drive.google.com/uc?export=view&id=1bwf3RIEp9yNTICbf1f5FWg9bX1H5BGNm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EREwT4VT8vRN"
   },
   "source": [
    "![picture](https://drive.google.com/uc?export=view&id=1VMtNw4qCs4BoN7d9Us8tNEtiKK_J4Csd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQLo9U5-8wBs"
   },
   "source": [
    "![picture](https://drive.google.com/uc?export=view&id=1eXCejZ3bZP1rBwtLzUQ9RMO9N0tuqDCP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cA6vN4Yb4ijq"
   },
   "source": [
    "###  **Visualizing heatmaps of class activation**\n",
    "\n",
    "* Visualise which parts of a given image led a ConvNet to its final classification decision\n",
    "* Such techniques are called **class activation map** (CAM) visualisation\n",
    "* Produce heatmaps of class activation over input images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edfJ7xmG-p-v"
   },
   "source": [
    "Example:\n",
    "\n",
    "![picture](https://drive.google.com/uc?export=view&id=1z1XY2GoYq_tEXhTXNMNT9qT3Yp2ic9ZR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBl--ze7_go6"
   },
   "source": [
    "First let's understand the idea behind CAM:\n",
    "\n",
    "(Remember: The CNN is already trained. Now we are just visualising aspects of the trained CNN)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![picture](https://drive.google.com/uc?export=view&id=1-Kn8BsHj1rPr61NGdRy822o1fvQuxW3l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VU9idU_-HBwr"
   },
   "source": [
    "\n",
    "Note:\n",
    "*   One set of optimal weights for one class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uRA385UHK82"
   },
   "source": [
    "Now let's look at an improved version of CAM:\n",
    "\n",
    "\n",
    "*   Grad-CAM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlKVYyd-__qi"
   },
   "source": [
    "![picture](https://drive.google.com/uc?export=view&id=10QgdjWzPejhmCXLvzoXpf3_ySYx2BVCj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wpNI2J4I9jw"
   },
   "source": [
    "Key Take-away:\n",
    "\n",
    "\n",
    "*   Need the weights. Q: What are these weights?\n",
    "    - make up a weighted sum of featurmaps to get a heat map\n",
    "*   Can learn them through a new sub-problem- CAM\n",
    "*   Can compute them directly through gradients- grad-CAM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOq9rVmh4rLK"
   },
   "outputs": [],
   "source": [
    "# Loading the Xception network with pretrained weights\n",
    "\n",
    "model = keras.applications.xception.Xception(weights=\"imagenet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CinrH57K5tdr"
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QlFBo2dE4tQh"
   },
   "outputs": [],
   "source": [
    "# Preprocessing an input image for Xception\n",
    "\n",
    "img_path = keras.utils.get_file(fname=\"cat.jpg\",\n",
    "                                origin=\"https://img-datasets.s3.amazonaws.com/cat.jpg\")\n",
    "\n",
    "def get_img_array(img_path, target_size):\n",
    "    img = keras.utils.load_img(img_path, target_size=target_size)\n",
    "    array = keras.utils.img_to_array(img)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    array = keras.applications.xception.preprocess_input(array)\n",
    "    return array\n",
    "\n",
    "img_array = get_img_array(img_path, target_size=(299, 299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVUyOcLu4vT8"
   },
   "outputs": [],
   "source": [
    "# Predicting the top three labels\n",
    "preds = model.predict(img_array)\n",
    "print(keras.applications.xception.decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXruuv1D4xLa"
   },
   "outputs": [],
   "source": [
    "# printing out the top label\n",
    "np.argmax(preds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUwDupeh5UZJ"
   },
   "outputs": [],
   "source": [
    "# Setting up a model that returns the last convolutional output\n",
    "\n",
    "last_conv_layer_name = \"block14_sepconv2_act\"\n",
    "classifier_layer_names = [ \"avg_pool\",\n",
    "                            \"predictions\",]\n",
    "last_conv_layer = model.get_layer(last_conv_layer_name)\n",
    "last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)\n",
    "# plot_model(last_conv_layer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDRbr73T6FET"
   },
   "outputs": [],
   "source": [
    "# last_conv_layer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VzruX5lB6Wwb"
   },
   "outputs": [],
   "source": [
    "# Reapplying the classifier on top of the last convolutional output\n",
    "\n",
    "\n",
    "classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n",
    "x = classifier_input\n",
    "for layer_name in classifier_layer_names:\n",
    "    x = model.get_layer(layer_name)(x)\n",
    "classifier_model = keras.Model(classifier_input, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNOfPpMk9Cwl"
   },
   "outputs": [],
   "source": [
    "# Retrieving the gradients of the top predicted class\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    last_conv_layer_output = last_conv_layer_model(img_array)\n",
    "    tape.watch(last_conv_layer_output)\n",
    "    preds = classifier_model(last_conv_layer_output)\n",
    "    top_pred_index = tf.argmax(preds[0])\n",
    "    top_class_channel = preds[:, top_pred_index]\n",
    "\n",
    "grads = tape.gradient(top_class_channel, last_conv_layer_output) # this is the grad of top class wrt output feature map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eW-IgUQmP3O9"
   },
   "source": [
    "![picture](https://drive.google.com/uc?export=view&id=10QgdjWzPejhmCXLvzoXpf3_ySYx2BVCj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7gD_e-T9HDU"
   },
   "outputs": [],
   "source": [
    "# Gradient pooling and channel-importance weighting\n",
    "\n",
    "# take an avg of the grads - formula for w_k\n",
    "pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2)).numpy()\n",
    "last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
    "\n",
    "# multiply weights with feature maps\n",
    "for i in range(pooled_grads.shape[-1]):\n",
    "    last_conv_layer_output[:, :, i] *= pooled_grads[i]\n",
    "\n",
    "# compute weighted sum\n",
    "heatmap = np.mean(last_conv_layer_output, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ff0Ii0Nq9Mmu"
   },
   "outputs": [],
   "source": [
    "# Heatmap post-processing\n",
    "\n",
    "heatmap = np.maximum(heatmap, 0)\n",
    "heatmap /= np.max(heatmap)\n",
    "plt.matshow(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "keru57IC9PeA"
   },
   "outputs": [],
   "source": [
    "# Superimposing the heatmap on the original picture\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "img = keras.utils.load_img(img_path)\n",
    "img = keras.utils.img_to_array(img)\n",
    "\n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "jet = cm.get_cmap(\"jet\")\n",
    "jet_colors = jet(np.arange(256))[:, :3]\n",
    "jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n",
    "jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n",
    "\n",
    "superimposed_img = jet_heatmap * 0.4 + img\n",
    "superimposed_img = keras.utils.array_to_img(superimposed_img)\n",
    "\n",
    "save_path = \"cat.jpg\"\n",
    "superimposed_img.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUWpJwxl9U28"
   },
   "outputs": [],
   "source": [
    "plt.matshow(superimposed_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxUW2yKcCyOF"
   },
   "source": [
    "Let's see more gradCAM results\n",
    "\n",
    "Some good readings:\n",
    "* [blog](https://towardsdatascience.com/understand-your-algorithm-with-grad-cam-d3b62fce353#:~:text=Gradient%2Dweighted%20Class%20Activation%20Mapping,regions%20in%20the%20image%20for)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVfYIE6WqEcy"
   },
   "source": [
    "## Self-Practice Problem:\n",
    "Solve an image classification problem on the cats-vs-dogs dataset by training 'mini-Exception-like' model based on the instructions given below:\n",
    "\n",
    "\n",
    "1.  Set the global random seed to 42.\n",
    "2.  We are using a **cat-vs-dogs** dataset here. You will have to download it using instruction.\n",
    "\n",
    "  Download the data through the following command in your notebook\n",
    "\n",
    "`!wget -qq https://cdn.iisc.talentsprint.com/AIandMLOps/Datasets/cats_vs_dogs_small.zip`\n",
    "\n",
    "`!unzip -qq '/content/cats_vs_dogs_small.zip'`\n",
    "\n",
    "  Use the  image_dataset_from_directory utility from tensorflow.keras.utils to make appropriate datasets. (0)\n",
    "3.  Building the Model based on this [model_summary](https://indianinstituteofscience-my.sharepoint.com/:t:/g/personal/rohitc1_iisc_ac_in/EZ36t8eQFu9MrBnPjwueKcABD-2_8AZyDpyJ3vJEqUqlLQ?e=njNCGf) and its corresponding [model_plot](https://indianinstituteofscience-my.sharepoint.com/:i:/g/personal/rohitc1_iisc_ac_in/EU2WCnpqi8BEtfzltqI2vc4B5OFx53lMwn2tv6gqMebTig?e=lSsbz6). Ensure that you follow the trailing instructions:(16)\n",
    "\n",
    "      i).   For the initial layers of model mentioned in this [summary](https://indianinstituteofscience-my.sharepoint.com/:i:/g/personal/ksumanth_iisc_ac_in/EYDi7MfYgkRGvb-NqKKuGiABon9CyOUMEiffHac1sXyEsg?e=gYNdVB), random flip (horizontal), random rotation of 0.1, random zoom of 0.2, rescaling by 1./255, and set kernel_size = 5 and use_bias=False in the convolution layer.\n",
    "\n",
    "      ii).  Define a block of following layers:\n",
    "\n",
    "          *   Batch Normalization layer\n",
    "          *   Activation layer with relu as activation function\n",
    "          *   Depth wise separable layer (kernel size = 3)\n",
    "          *   Batch Normalization layer\n",
    "          *   Activation layer with relu as activation function\n",
    "          *   Depth wise separable layer (kernel size = 5)\n",
    "          *   Batch Normalization layer\n",
    "          *   Activation layer with relu as activation function\n",
    "          *   Depth wise separable layer (kernel size = 7)\n",
    "          *   Maxpool2D layer (poolsize =3, stride=2)\n",
    "          *   Convolution layer\n",
    "          *   'Add layer' due to a residual connection. Infer connection points of the skip connection from the model summary and model plot.\n",
    "\n",
    "          **Infer unspecified arguments from the summary**\n",
    "\n",
    "      iii). The block defined in (ii) repeats 4 times. Note that in each repitition, the number of filters changes. Infer this from the model plot/summary.\n",
    "\n",
    "      iv).  The last two layers are GlobalAveragePooling and Dense layers. The dense layer is the output layer (infer the number of neurons and the activation function).\n",
    "4. Compile model with rmsprop as an optimizer with appropriate loss and metric for this respective problem.\n",
    "5. Fit the model with a batch_size of 32 for 20 epochs. (Don't use EarlyStopping callback). Use the validation dataset from the data you downloaded. We have specified a small no. of epochs because training may take time. Try running colab on GPU by going to Edit > Notebook accelerator > Hardware Accelerator > GPU.\n",
    "6. Return the history as a DataFrame. Show loss and accuracy for training and validation through appropriate plots.\n",
    "7. Evaluate the Model on test dataset from the data you downloaded.\n",
    "\n",
    "**Note**:\n",
    "\n",
    "1. If you are using any parameter values or arguments apart from the ones mentioned or the ones that you must infer, state explicitly where and why you are using them.\n",
    "\n",
    "2. Also verify that the total no. of params of your model are the same as that mentioned in model_summary txt file given to you\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
